{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: old comet version (1.0.44) detected. current: 2.0.2 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET WARNING: Failing to collect the installed os packages\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/yikeqicn/segnet-recognition-joint/714e591e05d74528bc0f4ed7aa526c51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from comet_ml import Experiment\n",
    "experiment = Experiment(api_key=\"YkPEmantOag1R1VOJmXz11hmt\", parse_args=False, project_name='SegNet_Recognition_Joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import IRSPRT#RecgArtPrintNoIntsectHVBW\n",
    "import pytesseract as pyt\n",
    "from os.path import join, basename, dirname\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import editdistance\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split#, SequentialSampler #yike: add SequentialSampler\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#from datasets import IRS #ArtPrintNoIntsectLBW,ArtPrintNoIntsectLBW_biameyd_siameyd,ArtPrintNoIntsectLBW_bpr_spr,ArtPrintNoIntsectLBW_biameyd_sprt\n",
    "############from Model_Unet_github import *\n",
    "from utils_seg import *\n",
    "import utils_recg\n",
    "\n",
    "############from recognition.Model import RecgModel, DecoderType\n",
    "#from recognition.utils import log_image\n",
    "\n",
    "home = os.environ['HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet\n",
    "from __future__ import print_function, division, absolute_import, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "from PIL import Image\n",
    "from math import ceil\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "from utils_seg import get_image_summary, log_images, _variable_with_weight_decay, _variable_on_cpu, _add_loss_summaries, \\\n",
    "    _activation_summary, print_hist_summery, get_hist, per_class_acc, writeImage\n",
    "\n",
    "# Recognition\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from os.path import join\n",
    "from Densenet4htr import Densenet4htr\n",
    "import utils_recg  # dangerous\n",
    "\n",
    "\n",
    "# model layers\n",
    "def weight_variable(shape, stddev=0.1, name=\"weight\"):\n",
    "    shape = np.array(shape)\n",
    "    # print(shape)\n",
    "    # print(stddev)\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def weight_variable_devonc(shape, stddev=0.1, name=\"weight_devonc\"):\n",
    "    shape = np.array(shape)\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=stddev), name=name)\n",
    "\n",
    "\n",
    "def bias_variable(shape, name=\"bias\"):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def conv2d(x, W, b, keep_prob_):\n",
    "    with tf.name_scope(\"conv2d\"):\n",
    "        conv_2d = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  # 'VALID'\n",
    "        conv_2d_b = tf.nn.bias_add(conv_2d, b)\n",
    "        return tf.nn.dropout(conv_2d_b, keep_prob_)\n",
    "\n",
    "\n",
    "def deconv2d(x, W, stride):\n",
    "    with tf.name_scope(\"deconv2d\"):\n",
    "        x_shape = tf.shape(x)\n",
    "        output_shape = tf.stack([x_shape[0], x_shape[1] * 2, x_shape[2] * 2, x_shape[3] // 2])\n",
    "        return tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding='SAME',\n",
    "                                      name=\"conv2d_transpose\")  # 'VALID'\n",
    "\n",
    "\n",
    "def max_pool(x, n):\n",
    "    return tf.nn.max_pool(x, ksize=[1, n, n, 1], strides=[1, n, n, 1], padding='SAME')  # 'VALID'\n",
    "\n",
    "\n",
    "def crop_and_concat(x1, x2):\n",
    "    with tf.name_scope(\"crop_and_concat\"):\n",
    "        x1_shape = tf.shape(x1)\n",
    "        x2_shape = tf.shape(x2)\n",
    "        # offsets for the top left corner of the crop\n",
    "        offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
    "        size = [-1, x2_shape[1], x2_shape[2], -1]\n",
    "        x1_crop = tf.slice(x1, offsets, size)\n",
    "        return tf.concat([x1_crop, x2], 3)\n",
    "\n",
    "\n",
    "def pixel_wise_softmax(output_map):\n",
    "    with tf.name_scope(\"pixel_wise_softmax\"):\n",
    "        max_axis = tf.reduce_max(output_map, axis=3, keepdims=True)\n",
    "        exponential_map = tf.exp(output_map - max_axis)\n",
    "        normalize = tf.reduce_sum(exponential_map, axis=3, keepdims=True)\n",
    "        return exponential_map / normalize\n",
    "\n",
    "\n",
    "def cross_entropy(y_, output_map):\n",
    "    return -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(output_map, 1e-10, 1.0)), name=\"cross_entropy\")\n",
    "\n",
    "\n",
    "# unet setting\n",
    "def create_conv_net(x, keep_prob, channels, n_class, layers=3, features_root=16, filter_size=3, pool_size=2,\n",
    "                    summaries=True):\n",
    "    \"\"\"\n",
    "    Creates a new convolutional unet for the given parametrization.\n",
    "    :param x: input tensor, shape [?,nx,ny,channels]\n",
    "    :param keep_prob: dropout probability tensor\n",
    "    :param channels: number of channels in the input image\n",
    "    :param n_class: number of output labels\n",
    "    :param layers: number of layers in the net\n",
    "    :param features_root: number of features in the first layer\n",
    "    :param filter_size: size of the convolution filter\n",
    "    :param pool_size: size of the max pooling operation\n",
    "    :param summaries: Flag if summaries should be created\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\n",
    "        \"Layers {layers}, features {features}, filter size {filter_size}x{filter_size}, pool size: {pool_size}x{pool_size}\".format(\n",
    "            layers=layers,\n",
    "            features=features_root,\n",
    "            filter_size=filter_size,\n",
    "            pool_size=pool_size))\n",
    "\n",
    "    # Placeholder for the input image\n",
    "    with tf.name_scope(\"preprocessing\"):\n",
    "        nx = tf.shape(x)[1]\n",
    "        ny = tf.shape(x)[2]\n",
    "        # nx=32\n",
    "        # ny=128\n",
    "        # channels=1\n",
    "        x_image = tf.reshape(x, tf.stack([-1, nx, ny, channels]))\n",
    "        in_node = x_image\n",
    "        batch_size = tf.shape(x_image)[0]\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "    convs = []\n",
    "    pools = OrderedDict()\n",
    "    deconv = OrderedDict()\n",
    "    dw_h_convs = OrderedDict()\n",
    "    up_h_convs = OrderedDict()\n",
    "\n",
    "    in_size = 1000  # ?????????????????????\n",
    "    size = in_size\n",
    "    # down layers\n",
    "    for layer in range(0, layers):\n",
    "        with tf.name_scope(\"down_conv_{}\".format(str(layer))):\n",
    "            features = 2 ** layer * features_root\n",
    "            stddev = np.sqrt(2 / (filter_size ** 2 * features))\n",
    "            if layer == 0:\n",
    "                w1 = weight_variable([filter_size, filter_size, channels, features], stddev, name=\"w1\")\n",
    "            else:\n",
    "                w1 = weight_variable([filter_size, filter_size, features // 2, features], stddev, name=\"w1\")\n",
    "\n",
    "            w2 = weight_variable([filter_size, filter_size, features, features], stddev, name=\"w2\")\n",
    "            b1 = bias_variable([features], name=\"b1\")\n",
    "            b2 = bias_variable([features], name=\"b2\")\n",
    "\n",
    "            conv1 = conv2d(in_node, w1, b1, keep_prob)\n",
    "            print(str(layer) + ' conv1: ' + str(conv1.get_shape()))\n",
    "            tmp_h_conv = tf.nn.relu(conv1)\n",
    "            conv2 = conv2d(tmp_h_conv, w2, b2, keep_prob)\n",
    "            print(str(layer) + ' conv2: ' + str(conv2.get_shape()))\n",
    "            dw_h_convs[layer] = tf.nn.relu(conv2)\n",
    "\n",
    "            weights.append((w1, w2))\n",
    "            biases.append((b1, b2))\n",
    "            convs.append((conv1, conv2))\n",
    "\n",
    "            size -= 2 * 2 * (filter_size // 2)  # valid conv\n",
    "            if layer < layers - 1:\n",
    "                pools[layer] = max_pool(dw_h_convs[layer], pool_size)\n",
    "                in_node = pools[layer]\n",
    "                size /= pool_size\n",
    "\n",
    "    in_node = dw_h_convs[layers - 1]\n",
    "\n",
    "    # up layers\n",
    "    for layer in range(layers - 2, -1, -1):\n",
    "        with tf.name_scope(\"up_conv_{}\".format(str(layer))):\n",
    "            features = 2 ** (layer + 1) * features_root\n",
    "            stddev = np.sqrt(2 / (filter_size ** 2 * features))\n",
    "\n",
    "            wd = weight_variable_devonc([pool_size, pool_size, features // 2, features], stddev, name=\"wd\")\n",
    "            bd = bias_variable([features // 2], name=\"bd\")\n",
    "            h_deconv = tf.nn.relu(deconv2d(in_node, wd, pool_size) + bd)\n",
    "            print(str(layer) + ' h_deconv: ' + str(h_deconv.get_shape()))\n",
    "            h_deconv_concat = crop_and_concat(dw_h_convs[layer], h_deconv)\n",
    "            print(str(layer) + ' h_deconv_concat: ' + str(h_deconv_concat.get_shape()))\n",
    "            deconv[layer] = h_deconv_concat\n",
    "\n",
    "            w1 = weight_variable([filter_size, filter_size, features, features // 2], stddev, name=\"w1\")\n",
    "            w2 = weight_variable([filter_size, filter_size, features // 2, features // 2], stddev, name=\"w2\")\n",
    "            b1 = bias_variable([features // 2], name=\"b1\")\n",
    "            b2 = bias_variable([features // 2], name=\"b2\")\n",
    "\n",
    "            conv1 = conv2d(h_deconv_concat, w1, b1, keep_prob)\n",
    "            h_conv = tf.nn.relu(conv1)\n",
    "            print(str(layer) + ' h_conv1_post_deconv: ' + str(h_conv.get_shape()))\n",
    "            conv2 = conv2d(h_conv, w2, b2, keep_prob)\n",
    "            in_node = tf.nn.relu(conv2)\n",
    "            up_h_convs[layer] = in_node\n",
    "            print(str(layer) + ' h_conv2_post_deconv: ' + str(in_node.get_shape()))\n",
    "\n",
    "            weights.append((w1, w2))\n",
    "            biases.append((b1, b2))\n",
    "            convs.append((conv1, conv2))\n",
    "\n",
    "            size *= pool_size\n",
    "            size -= 2 * 2 * (filter_size // 2)  # valid conv\n",
    "\n",
    "    # Output Map\n",
    "    with tf.name_scope(\"output_map\"):\n",
    "        weight = weight_variable([1, 1, features_root, n_class], stddev)\n",
    "        bias = bias_variable([n_class], name=\"bias\")\n",
    "        conv = conv2d(in_node, weight, bias, tf.constant(1.0))\n",
    "        print(str(layer) + ' outmap: ' + str(conv.get_shape()))\n",
    "\n",
    "        # output_map = tf.nn.relu(conv)\n",
    "        output_map = conv  # no activation, to be consistant with other models and leverage previous loss/prediction structures yike !!!!\n",
    "        up_h_convs[\"out\"] = output_map\n",
    "\n",
    "    if summaries:\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            for i, (c1, c2) in enumerate(convs):\n",
    "                tf.summary.image('summary_conv_%02d_01' % i, get_image_summary(c1))\n",
    "                tf.summary.image('summary_conv_%02d_02' % i, get_image_summary(c2))\n",
    "\n",
    "            for k in pools.keys():\n",
    "                tf.summary.image('summary_pool_%02d' % k, get_image_summary(pools[k]))\n",
    "\n",
    "            for k in deconv.keys():\n",
    "                tf.summary.image('summary_deconv_concat_%02d' % k, get_image_summary(deconv[k]))\n",
    "\n",
    "            for k in dw_h_convs.keys():\n",
    "                tf.summary.histogram(\"dw_convolution_%02d\" % k + '/activations', dw_h_convs[k])\n",
    "\n",
    "            for k in up_h_convs.keys():\n",
    "                tf.summary.histogram(\"up_convolution_%s\" % k + '/activations', up_h_convs[k])\n",
    "\n",
    "    variables = []\n",
    "    for w1, w2 in weights:\n",
    "        variables.append(w1)\n",
    "        variables.append(w2)\n",
    "\n",
    "    for b1, b2 in biases:\n",
    "        variables.append(b1)\n",
    "        variables.append(b2)\n",
    "\n",
    "    return output_map, variables, int(in_size - size)\n",
    "\n",
    "\n",
    "class DecoderType:\n",
    "    BestPath = 0\n",
    "    BeamSearch = 1\n",
    "    WordBeamSearch = 2\n",
    "\n",
    "\n",
    "class Model:\n",
    "    # model constants\n",
    "    # batchSize = 50 #qyk\n",
    "    # imgSize = (128, 32)\n",
    "    # imgSize = (192, 48) #qyk\n",
    "    maxTextLen = 32  # qyk?\n",
    "    MOVING_AVERAGE_DECAY = 0.9999  # The decay to use for the moving average.\n",
    "    NUM_EPOCHS_PER_DECAY = 350.0  # Epochs after which learning rate decays.\n",
    "    LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "\n",
    "    def __init__(self, args, charList, loss_beta, loss_weight, decoderType=DecoderType.BestPath, experiment=None,\n",
    "                 mustRestore_seg=False, mustRestore_recg=False, joint=False):  # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        '''\n",
    "        loss_betaxsegloss+(1-loss_beta)xrecgloss\n",
    "        loss_weight: used in segnet training\n",
    "        joint: False -> train recognition only, True -> train segmentation with recognition frozen\n",
    "        '''\n",
    "        self.loss_beta = loss_beta  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.args = args\n",
    "        self.experiment = experiment\n",
    "        self.lrInit = args.lrInit\n",
    "        # self.mustRestore_recg= mustRestore_recg\n",
    "        ###################################\n",
    "        \"init segnet model parameters:\"\n",
    "        ###################################\n",
    "        self.mustRestore_seg = mustRestore_seg\n",
    "        ###model hyperparameters###\n",
    "        self.num_classes = args.num_class\n",
    "        # self.FilePaths = FilePaths\n",
    "        self.batch_size_seg = args.batch_size_seg  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "        ###########################################################################\n",
    "        \"init recognition model parameters: add CNN, RNN and CTC and initialize TF\"\n",
    "        ###########################################################################\n",
    "        self.charList = charList\n",
    "        self.decoderType = decoderType\n",
    "        self.mustRestore_recg = mustRestore_recg\n",
    "        # self.FilePaths = FilePaths\n",
    "        self.batchsize_recg = args.batchsize_recg  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # self.lrInit = args.lrInit #!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # self.args = args #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        ############################################################################\n",
    "        \"\"\"Graph Set Up\"\"\"\n",
    "        ############################################################################\n",
    "        tf.reset_default_graph()  # yike reset default graph  #!!!!!!!!!!!!!!!!!!!!!还要吗？????????\n",
    "        with tf.name_scope('graph_segmentation'):\n",
    "            # self.loss_segmentation, output = YIKE_FUNCTION_HERE()\n",
    "\n",
    "            ###input### -- try to only set up graph once, combine train and test, by yike\n",
    "            # tf.reset_default_graph() # yike reset default graph\n",
    "            self.input_images_seg = tf.placeholder(tf.float32, shape=[None, self.args.image_h, self.args.image_w,\n",
    "                                                                      self.args.image_c])  # try my best to make runtime batch_size flexible #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            self.input_labels_seg = tf.placeholder(tf.int64, shape=[None, self.args.image_h, self.args.image_w,\n",
    "                                                                    1])  # !!!!!!!!!!!!!!!!!!!!!\n",
    "            self.phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "            ###graph### -- combine\n",
    "            self.logit_seg = self.setup_graph(self.input_images_seg,\n",
    "                                              self.phase_train)  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            self.loss_seg = self.cal_loss(self.logit_seg, self.input_labels_seg)*500 # make it to same level as recg loss\n",
    "            self.pred_seg = tf.argmax(self.logit_seg, axis=3)\n",
    "\n",
    "            input_images_2d_seg = tf.squeeze(self.input_images_seg, [3])  # to 2d images, channel=1\n",
    "            self.output_clean_seg = tf.to_float(self.pred_seg) * (255 - input_images_2d_seg) + input_images_2d_seg\n",
    "\n",
    "            print('clean output from seg: ' + str(self.output_clean_seg.get_shape()))\n",
    "\n",
    "        with tf.name_scope('graph_recognition'):\n",
    "            # assume the input has been resized to 32x128\n",
    "            if joint:\n",
    "                self.input_images_recg = tf.transpose(self.output_clean_seg, perm=(0, 2, 1))\n",
    "            else:\n",
    "                self.input_images_recg = tf.placeholder(tf.float32, shape=(None, args.image_w, args.image_h))\n",
    "            print('recg input: ' + str(self.input_images_recg.get_shape()))\n",
    "            # CNN\n",
    "            if args.nondensenet:\n",
    "                cnnOut4d = self.setupCNN(self.input_images_recg)\n",
    "            else:  # use densenet by default\n",
    "                cnnOut4d = self.setupCNNdensenet(self.input_images_recg, args)\n",
    "\n",
    "            # RNN\n",
    "            rnnOut3d = self.setupRNN(cnnOut4d)\n",
    "\n",
    "            # CTC\n",
    "            (self.ctcloss, self.decoder) = self.setupCTC(rnnOut3d)\n",
    "\n",
    "            # Explicit regularizers\n",
    "            self.loss_recg = self.ctcloss + args.wdec * self.setupWdec(args)\n",
    "\n",
    "        # combine losses\n",
    "        self.loss_total = (1 - loss_beta) * self.loss_recg + loss_beta * self.loss_seg\n",
    "        print(self.loss_total)\n",
    "        # optimizer for NN parameters\n",
    "        self.batchesTrained = args.batchesTrained  # only for recognition training\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[])  # for recognition and segmentation\n",
    "        self.global_step = tf.Variable(0, trainable=False)  # for segmentation training\n",
    "\n",
    "        self.var_list_train_seg = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"graph_segmentation\") #TRAINABLE_\n",
    "        self.obj_list_savable_seg= tf.get_collection(tf.GraphKeys.VARIABLES, \"graph_segmentation\")\n",
    "        self.train_op_seg = self.train_op_seg_prepare(total_loss=self.loss_total, lr=self.learning_rate,\n",
    "                                                      global_step=self.global_step, var_list=self.var_list_train_seg)\n",
    "        # self.learning_rate self.loss_total\n",
    "        ## optimizer for recognition only\n",
    "\n",
    "        self.var_list_train_recg = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"graph_recognition\")#TRAINABLE_\n",
    "        self.obj_list_savable_recg= tf.get_collection(tf.GraphKeys.VARIABLES, \"graph_recognition\")\n",
    "        if args.optimizer == 'rmsprop':\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss_recg,\n",
    "                                                                                    var_list=self.var_list_train_recg)\n",
    "        elif args.optimizer == 'adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss_recg,\n",
    "                                                                                 var_list=self.var_list_train_recg)\n",
    "        elif args.optimizer == 'momentum':\n",
    "            self.optimizer = tf.train.MomentumOptimizer(self.learning_rate, .9).minimize(self.loss_recg,\n",
    "                                                                                         var_list=self.var_list_train_recg)\n",
    "\n",
    "        # self.global_step,var_list=self.var_list_train_seg)  # !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # above: loss need to change to total loss\n",
    "        ###session and saver###\n",
    "        (self.sess, self.saver_seg, self.saver_recg) = self.initTF()  # tobe changed!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    ############################################################\n",
    "    #####               Segnet Functions             ###########\n",
    "    ############################################################ Not Adjusted !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    ### 1. loss factory ###\n",
    "\n",
    "    def weighted_loss(self, logits, labels):  # num_classes, head=None):\n",
    "        \"\"\" median-frequency re-weighting \"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            # print('w_llll')\n",
    "            logits = tf.reshape(logits, (-1, self.num_classes))\n",
    "            # print(logits.get_shape())\n",
    "            epsilon = tf.constant(value=1e-10)\n",
    "\n",
    "            logits = logits + epsilon\n",
    "\n",
    "            # consturct one-hot label array\n",
    "            label_flat = tf.reshape(labels, (-1, 1))\n",
    "            # print(label_flat.get_shape())\n",
    "\n",
    "            # should be [batch ,num_classes]\n",
    "            labels = tf.reshape(tf.one_hot(label_flat, depth=self.num_classes), (-1, self.num_classes))\n",
    "            # print(labels.get_shape())\n",
    "\n",
    "            softmax = tf.nn.softmax(logits)\n",
    "            # print(softmax.get_shape())\n",
    "            #        print(epsilon.get_shape())\n",
    "\n",
    "            #        print((labels * tf.log(softmax + epsilon)).get_shape())\n",
    "            #        print(head.shape)\n",
    "            #        print(tf.multiply(labels * tf.log(softmax + epsilon), head))\n",
    "\n",
    "            cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax + epsilon), self.loss_weight),\n",
    "                                           axis=[1])\n",
    "            #        print(cross_entropy.get_shape()) # yike head -> self.loss_weight\n",
    "\n",
    "            cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "            #        print(cross_entropy_mean.get_shape())\n",
    "            tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "            loss = tf.add_n(tf.get_collection('losses'), name='total_loss_seg')\n",
    "            print('loss_seg: ' + str(loss.get_shape()))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cal_loss(self, logits, labels):\n",
    "        labels = tf.cast(labels, tf.int32)\n",
    "        return self.weighted_loss(logits, labels)\n",
    "\n",
    "        # self.weighted_loss(logits, labels, num_classes=NUM_CLASSES, head=loss_weight)\n",
    "\n",
    "        ###2. train optimizer factory ##\n",
    "\n",
    "    def train_op_seg_prepare(self, total_loss, lr, global_step, var_list):\n",
    "        # all of them are tensor\n",
    "        # total_sample = 274 yike: ok to comment out?\n",
    "        # num_batches_per_epoch = 274/1 yike: ok to comment out?\n",
    "\n",
    "        loss_averages_op = _add_loss_summaries(total_loss)\n",
    "        # Compute gradients.\n",
    "        with tf.control_dependencies([loss_averages_op]):\n",
    "            # print('try...')\n",
    "            opt = tf.train.AdamOptimizer(lr)\n",
    "            print('toto_loss_shape: ' + str(total_loss))\n",
    "            opt.compute_gradients(total_loss, var_list=var_list)  # add list of variables\n",
    "            grads = opt.compute_gradients(total_loss, var_list=var_list)  # !!!!!!!\n",
    "            # print(grads)\n",
    "            apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "            # Add histograms for trainable variables.\n",
    "            #######for var in tf.trainable_variables():\n",
    "            ######tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "            # Add histograms for gradients.\n",
    "            #####for grad, var in grads:\n",
    "            #####if grad is not None:\n",
    "            #####tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "            # Track the moving averages of all trainable variables.\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(Model.MOVING_AVERAGE_DECAY, global_step)\n",
    "            variables_averages_op = variable_averages.apply(var_list=var_list)  # tf.trainable_variables()\n",
    "\n",
    "            with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "                train_op_seg = tf.no_op(name='train_op_seg_prepare')\n",
    "\n",
    "        return train_op_seg\n",
    "\n",
    "        ###3. graph factory ###\n",
    "\n",
    "    def setup_graph(self, images, phase_train):\n",
    "        # previous inference() labels,inference, batch_size -- in order to get batch_size at running time\n",
    "        # rather than using fixed batch_size in graph set up, revise it in inference:\n",
    "        # batchsize=tf.shape(images)[0] # yike !!!\n",
    "        print('GGG')\n",
    "        input_shape = images.get_shape().as_list()\n",
    "        print(input_shape)\n",
    "\n",
    "        #       create_conv_net(x, keep_prob, channels, n_class, layers=3, features_root=16, filter_size=3, pool_size=2,\n",
    "        #                    summaries=True)\n",
    "\n",
    "        logit, _, __ = create_conv_net(x=images, keep_prob=0.8, channels=input_shape[3], n_class=self.num_classes,\n",
    "                                       layers=3, features_root=32, filter_size=3)\n",
    "        print(logit.get_shape())\n",
    "        \"\"\"\n",
    "         Start Classify \n",
    "\n",
    "        # output predicted class number (6)\n",
    "        with tf.variable_scope('conv_classifier') as scope:\n",
    "          kernel = _variable_with_weight_decay('weights',\n",
    "                                            shape=[1, 1, 64, self.num_classes],\n",
    "                                            initializer=msra_initializer(1, 64),\n",
    "                                            wd=0.0005)\n",
    "          conv = tf.nn.conv2d(conv_decode1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "          print('cv')\n",
    "          print(conv.get_shape())\n",
    "          biases = _variable_on_cpu('biases', [self.num_classes], tf.constant_initializer(0.0))\n",
    "          print(biases.get_shape())\n",
    "          logit= tf.nn.bias_add(conv, biases, name=scope.name)\n",
    "          #conv_classifier = tf.nn.bias_add(conv, biases, name=scope.name)\n",
    "          #print(conv_classifier.get_shape())\n",
    "          #logit = conv_classifier\n",
    "          #print('LLL')\n",
    "          #print(labels)\n",
    "          #print(conv_classifier)\n",
    "\n",
    "          #loss = cal_loss(conv_classifier, labels)\n",
    "          print(logit.get_shape())\n",
    "          \"\"\"\n",
    "        return logit  # loss\n",
    "\n",
    "    ############################################################################\n",
    "    ###                 Recognition Functions                                ###\n",
    "    ############################################################################\n",
    "    def setupCNN(self, cnnIn3d):\n",
    "        \"vanilla cnn from original github repo\"\n",
    "        cnnIn4d = tf.expand_dims(input=cnnIn3d, axis=3)\n",
    "\n",
    "        # list of parameters for the layers\n",
    "        kernelVals = [5, 5, 3, 3, 3]\n",
    "        featureVals = [1, 32, 64, 128, 128, 256]\n",
    "        strideVals = poolVals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
    "        numLayers = len(strideVals)\n",
    "\n",
    "        # create layers\n",
    "        pool = cnnIn4d  # input to first CNN layer\n",
    "        for i in range(numLayers):\n",
    "            kernel = tf.Variable(\n",
    "                tf.truncated_normal([kernelVals[i], kernelVals[i], featureVals[i], featureVals[i + 1]], stddev=0.1))\n",
    "            conv = tf.nn.conv2d(pool, kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
    "            relu = tf.nn.relu(conv)\n",
    "            pool = tf.nn.max_pool(relu, (1, poolVals[i][0], poolVals[i][1], 1),\n",
    "                                  (1, strideVals[i][0], strideVals[i][1], 1),\n",
    "                                  'VALID')\n",
    "\n",
    "        self.is_training = tf.placeholder(tf.bool, shape=[])  # dummy placeholder to prevent error, no effect\n",
    "        return pool\n",
    "\n",
    "    def setupCNNdensenet(self, cnnIn3d, args):\n",
    "        \"ADDED BY RONNY: densenet cnn\"\n",
    "        print('shape of cnn input: ' + str(cnnIn3d.get_shape().as_list()))\n",
    "        cnnIn4d = tf.expand_dims(input=cnnIn3d, axis=3)\n",
    "        net = Densenet4htr(cnnIn4d, **vars(args))\n",
    "        self.is_training = net.is_training\n",
    "        print('shape of cnn output: ' + str(net.output.get_shape().as_list()))\n",
    "        return net.output\n",
    "\n",
    "    def setupRNN(self, rnnIn4d):\n",
    "        \"create RNN layers and return output of these layers\"\n",
    "        rnnIn3d = tf.squeeze(rnnIn4d, axis=[2])\n",
    "\n",
    "        # basic cells which is used to build RNN\n",
    "        numHidden = self.args.rnndim\n",
    "        cells = [tf.contrib.rnn.LSTMCell(num_units=numHidden, state_is_tuple=True) for _ in range(2)]  # 2 layers\n",
    "\n",
    "        # stack basic cells\n",
    "        stacked = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # bidirectional RNN\n",
    "        # BxTxF -> BxTx2H\n",
    "        ((fw, bw), _) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnnIn3d,\n",
    "                                                        dtype=rnnIn3d.dtype, scope=\"graph_recognition/bidirectional_rnn\")\n",
    "\n",
    "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
    "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
    "\n",
    "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
    "        kernel = tf.Variable(tf.truncated_normal([1, 1, numHidden * 2, len(self.charList) + 1], stddev=0.1))\n",
    "        logits = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'), axis=[2])\n",
    "        # with tf.variable_scope('logits'):\n",
    "        #   logits = tf.squeeze(tf.layers.conv2d(concat, len(self.charList)+1, 1, use_bias=True), axis=[2]) # FIXED BY RONNY\n",
    "        return logits\n",
    "\n",
    "    def setupCTC(self, ctcIn3d):\n",
    "        \"create CTC loss and decoder and return them\"\n",
    "        # BxTxC -> TxBxC\n",
    "        ctcIn3dTBC = tf.transpose(ctcIn3d, [1, 0, 2])\n",
    "        # ground truth text as sparse tensor\n",
    "        self.gtTexts = tf.SparseTensor(tf.placeholder(tf.int64, shape=[None, 2]),\n",
    "                                       tf.placeholder(tf.int32, [None]),\n",
    "                                       tf.placeholder(tf.int64, [2]))\n",
    "        # calc loss for batch\n",
    "        self.seqLen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        loss = tf.nn.ctc_loss(labels=self.gtTexts, inputs=ctcIn3dTBC, sequence_length=self.seqLen,\n",
    "                              ctc_merge_repeated=True)  # , ignore_longer_outputs_than_inputs=True) #qyk\n",
    "\n",
    "        # decoder: either best path decoding or beam search decoding\n",
    "        if self.decoderType == DecoderType.BestPath:\n",
    "            decoder = tf.nn.ctc_greedy_decoder(inputs=ctcIn3dTBC, sequence_length=self.seqLen)\n",
    "        elif self.decoderType == DecoderType.BeamSearch:\n",
    "            decoder = tf.nn.ctc_beam_search_decoder(inputs=ctcIn3dTBC, sequence_length=self.seqLen, beam_width=50,\n",
    "                                                    merge_repeated=False)\n",
    "        elif self.decoderType == DecoderType.WordBeamSearch:\n",
    "            # import compiled word beam search operation (see https://github.com/githubharald/CTCWordBeamSearch)\n",
    "            word_beam_search_module = tf.load_op_library('TFWordBeamSearch.so')\n",
    "\n",
    "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
    "            chars = str().join(self.charList)\n",
    "            wordChars = open('wordCharList.txt').read().splitlines()[0]\n",
    "            corpus = open(self.FilePaths.fnCorpus).read()\n",
    "\n",
    "            # decode using the \"Words\" mode of word beam search\n",
    "            decoder = word_beam_search_module.word_beam_search(tf.nn.softmax(ctcIn3dTBC, dim=2), 50, 'Words', 0.0,\n",
    "                                                               corpus.encode('utf8'), chars.encode('utf8'),\n",
    "                                                               wordChars.encode('utf8'))\n",
    "\n",
    "        # return a CTC operation to compute the loss and a CTC operation to decode the RNN output\n",
    "        return (tf.reduce_mean(loss), decoder)\n",
    "\n",
    "    def setupWdec(self, args):\n",
    "        \"\"\"L2 weight decay loss.\"\"\"\n",
    "        costs = []\n",
    "        for var in tf.trainable_variables():  # all weights count toward weight decay except batchnorm and biases\n",
    "            if var.op.name.find(r'BatchNorm') == -1 & var.op.name.find(r'bias:0') == -1:\n",
    "                costs.append(tf.nn.l2_loss(var))\n",
    "        return tf.add_n(costs)\n",
    "\n",
    "    def toSparse(self, texts):\n",
    "        \"put ground truth texts into sparse tensor for ctc_loss\"\n",
    "        indices = []\n",
    "        values = []\n",
    "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
    "\n",
    "        # go over all texts\n",
    "        for (batchElement, text) in enumerate(texts):\n",
    "            # convert to string of label (i.e. class-ids)\n",
    "            labelStr = [self.charList.index(c) for c in text]\n",
    "            # sparse tensor must have size of max. label-string\n",
    "            if len(labelStr) > shape[1]:\n",
    "                shape[1] = len(labelStr)\n",
    "            # put each label into sparse tensor\n",
    "            for (i, label) in enumerate(labelStr):\n",
    "                indices.append([batchElement, i])\n",
    "                values.append(label)\n",
    "\n",
    "        return (indices, values, shape)\n",
    "\n",
    "    def decoderOutputToText(self, ctcOutput):\n",
    "        \"extract texts from output of CTC decoder\"\n",
    "        bt_size = ctcOutput[1].shape[0]  # yike !!!!!!\n",
    "        # contains string of labels for each batch element\n",
    "        encodedLabelStrs = [[] for i in range(bt_size)]  # yike self.batchsize !!!!!!!\n",
    "\n",
    "        # word beam search: label strings terminated by blank\n",
    "        if self.decoderType == DecoderType.WordBeamSearch:\n",
    "            blank = len(self.charList)\n",
    "            for b in range(bt_size):  # yike self.batchsize !!!!!!!\n",
    "                for label in ctcOutput[b]:\n",
    "                    if label == blank:\n",
    "                        break\n",
    "                    encodedLabelStrs[b].append(label)\n",
    "\n",
    "        # TF decoders: label strings are contained in sparse tensor\n",
    "        else:\n",
    "            # ctc returns tuple, first element is SparseTensor\n",
    "            decoded = ctcOutput[0][0]\n",
    "\n",
    "            # go over all indices and save mapping: batch -> values\n",
    "            idxDict = {b: [] for b in range(bt_size)}  # yike self.batchsize !!!!!\n",
    "            for (idx, idx2d) in enumerate(decoded.indices):\n",
    "                label = decoded.values[idx]\n",
    "                batchElement = idx2d[0]  # index according to [b,t]\n",
    "                encodedLabelStrs[batchElement].append(label)\n",
    "\n",
    "        # map labels to chars for all batch elements\n",
    "        return [str().join([self.charList[c] for c in labelStr]) for labelStr in encodedLabelStrs]\n",
    "\n",
    "    #########################################################\n",
    "    ####             Initialize TF (Both)                ####\n",
    "    #########################################################\n",
    "\n",
    "    def initTF(self):\n",
    "        \"initialize TF\"\n",
    "        print('Python: ' + sys.version)\n",
    "        print('Tensorflow: ' + tf.__version__)\n",
    "\n",
    "        sess = tf.Session(\n",
    "            config=tf.ConfigProto(allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True)))\n",
    "\n",
    "        #####################################\n",
    "        ##        SegNet Initiation        ##\n",
    "        #####################################\n",
    "\n",
    "        saver_seg = tf.train.Saver(var_list=self.obj_list_savable_seg, max_to_keep=1)  # saver saves model to file\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Ran global_variables_initializer first')\n",
    "        # Restore from saved model in current checkpoint directory\n",
    "        latestSnapshot_seg = tf.train.latest_checkpoint(self.args.ckptpath_seg)  # is there a saved model?\n",
    "        if self.mustRestore_seg and not latestSnapshot_seg:  # if model must be restored (for inference), there must be a snapshot\n",
    "            raise Exception('No saved model found in: ' + self.args.ckptpath_seg)\n",
    "\n",
    "        if latestSnapshot_seg:  # load saved model if available\n",
    "            saver_seg.restore(sess, latestSnapshot_seg)\n",
    "            print('Init with stored values from ' + latestSnapshot_seg)\n",
    "        else:\n",
    "            # sess.run(tf.global_variables_initializer())\n",
    "            # print('Ran global_variables_initializer')\n",
    "            sess.run(tf.initializers.variables(var_list=self.var_list_train_seg, name='init_seg'))\n",
    "            print('Ran initializers.variables on segnet trainable variables')\n",
    "\n",
    "        '''\n",
    "            # initialize params from other model (transfer learning)\n",
    "        if self.args.transfer:\n",
    "            utils.maybe_download(source_url=self.args.urlTransferFrom,\n",
    "                                 filename=join(self.args.ckptpath_seg, 'transferFrom'),\n",
    "                                 target_directory=None,\n",
    "                                 filetype='folder',\n",
    "                                 force=True)\n",
    "            saverTransfer = tf.train.Saver(\n",
    "                tf.trainable_variables()[:-1])  # load all variables except from logit (classification) layer\n",
    "            saverTransfer.restore(sess, glob(join(self.args.ckptpath_seg, 'transferFrom', 'model*'))[0].split('.')[0])\n",
    "            print('Loaded variable values (except logit layer) from ' + self.args.urlTransferFrom)\n",
    "        '''\n",
    "        #############################################\n",
    "        ###         Recognition Initialization    ###\n",
    "        #############################################\n",
    "        saver_recg = tf.train.Saver(var_list=self.obj_list_savable_recg, max_to_keep=1)\n",
    "\n",
    "        latestSnapshot_recg = tf.train.latest_checkpoint(self.args.ckptpath_recg)  # is there a saved model?\n",
    "        if self.mustRestore_recg and not latestSnapshot_recg:  # if model must be restored (for inference), there must be a snapshot\n",
    "            raise Exception('No saved model found in: ' + self.args.ckptpath_recg)\n",
    "\n",
    "        if latestSnapshot_recg:  # load saved model if available\n",
    "            saver_recg.restore(sess, latestSnapshot_recg)\n",
    "            print('Init with stored values from ' + latestSnapshot_recg)\n",
    "        else:\n",
    "            sess.run(tf.initializers.variables(var_list=self.var_list_train_recg, name='init_recg'))\n",
    "            print('Ran initializers.variables on recognition trainable variables')\n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # initialize params from other model (transfer learning)\n",
    "        \"\"\"\n",
    "        if self.args.transfer:\n",
    "            utils.maybe_download(source_url=self.args.urlTransferFrom,\n",
    "                                 filename=join(self.args.ckptpath_recg, 'transferFrom'),\n",
    "                                 target_directory=None,\n",
    "                                 filetype='folder',\n",
    "                                 force=True)\n",
    "            saverTransfer = tf.train.Saver(\n",
    "                tf.trainable_variables()[:-1])  # load all variables except from logit (classification) layer\n",
    "            saverTransfer.restore(sess, glob(join(self.args.ckptpath_recg, 'transferFrom', 'model*'))[0].split('.')[0])\n",
    "            print('Loaded variable values (except logit layer) from ' + self.args.urlTransferFrom)\n",
    "        \"\"\"\n",
    "\n",
    "        return (sess, saver_seg, saver_recg)\n",
    "    #######################################################\n",
    "    #####         Training, Inference and Save        #####\n",
    "    #######################################################\n",
    "\n",
    "    #######################SegNet##########################\n",
    "    def saveSeg(self, epoch):\n",
    "       \"save model to file\"\n",
    "       self.saver_seg.save(self.sess, join(self.args.ckptpath_seg, 'model'), global_step=epoch)\n",
    "    def trainBatchSeg(self, images, labels, labels_recg): # added labels_recg!!!!!!!!!!!!!!!!\n",
    "        \"feed a batch into the NN to train it\"\n",
    "\n",
    "        # sparse = self.toSparse(labels)\n",
    "        # lrnrate = self.lrInit if self.batchesTrained < self.args.lrDrop1 else (\n",
    "        # self.lrInit*1e-1 if self.batchesTrained < self.args.lrDrop2 else self.lrInit*1e-2)  # decay learning rate\n",
    "        bt_size=len(images)\n",
    "        train_step = self.global_step.eval(session=self.sess)\n",
    "        sparse = self.toSparse(labels_recg) # added !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        \"\"\" fix lr \"\"\"  ## To Ronny, change the schedule?\n",
    "        # lr = self.lrInit\n",
    "        lr = self.lrInit if train_step < self.args.lrDrop1 else (\n",
    "            self.lrInit * 1e-1 if train_step < self.args.lrDrop2 else self.lrInit * 1e-2)  # yike\n",
    "        (_, lossValTotal,lossValSeg) = self.sess.run([self.train_op_seg, self.loss_total,self.loss_seg],\n",
    "                                     {self.input_images_seg: images,\n",
    "                                      #self.input_images_recg: images, # added !!!!!!!!!!!!!!!\n",
    "                                      self.input_labels_seg: labels,\n",
    "                                      self.gtTexts:sparse, # added !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                                      self.seqLen: [Model.maxTextLen]*bt_size, #* self.batchsize_seg, #added!!!!!!!!!!!!!!!!!!!!!!\n",
    "                                      self.learning_rate: lr,\n",
    "                                      self.phase_train: True,\n",
    "                                      self.is_training: False})\n",
    "        # self.batchesTrained += 1\n",
    "        return lossValTotal,lossValSeg\n",
    "\n",
    "    def inferBatchSeg(self, imgs):  # modify to compatible to torch. previous def inferBatch(self, batch)\n",
    "        \"feed a batch into the NN to recngnize the texts\"\n",
    "\n",
    "        bt_size = len(imgs)  # yike !!!!!!!!\n",
    "\n",
    "        pred = self.sess.run(self.pred_seg, feed_dict=\n",
    "        {self.input_images_seg: imgs,  # check in, comment out in formal run\n",
    "         # self.input_labels: labels,\n",
    "         self.phase_train: False,\n",
    "         self.is_training: False})  # yike self.batchsize!!!!!!!!!\n",
    "        return pred\n",
    "    def imageCleanSeg(self, imgs):\n",
    "        bt_size = len(imgs)\n",
    "        cleaneds=self.sess.run(self.output_clean_seg, feed_dict={self.input_images_seg: imgs, self.phase_train: False, self.is_training:False})\n",
    "        return cleaneds\n",
    "    def trainSeg(self, loader, validateloader=None, testloader=None):\n",
    "        \"train NN\"\n",
    "        epoch = 0  # number of training epochs since start\n",
    "        best_accuracy = 0.0\n",
    "        step = 0\n",
    "        while True:\n",
    "            epoch += 1;\n",
    "            print('Epoch:', epoch, ' Training...')\n",
    "            # train\n",
    "            counter = 0\n",
    "            # step = 0\n",
    "            for idx, (images, labels, labels_recg) in enumerate(loader):\n",
    "                images = images.numpy()\n",
    "                labels = labels.numpy()\n",
    "                #labels_recg=labels_recg.numpy()\n",
    "                loss_value_total,loss_value_seg = self.trainBatchSeg(images, labels, labels_recg)\n",
    "                assert not np.isnan(loss_value_total), 'Model diverged with loss = NaN'\n",
    "                step += 1\n",
    "\n",
    "                if idx % 100 == 0:\n",
    "                    print('TRAIN: Batch:', idx / len(loader), 'Loss_Total:', loss_value_total)\n",
    "                    print('TRAIN: Batch:', idx / len(loader), 'Loss_Seg:', loss_value_seg)\n",
    "                    self.experiment.log_metric('train/loss_total', loss_value_total, step)\n",
    "                    self.experiment.log_metric('train/loss_seg', loss_value_seg, step)#!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    \n",
    "                    logits = self.sess.run(self.logit_seg,\n",
    "                                           feed_dict={self.input_images_seg: images,  # check in, comment out in formal run\n",
    "                                                      # self.input_labels: labels,\n",
    "                                                      # self.learning_rate: lr,\n",
    "                                                      self.phase_train: False,\n",
    "                                                      self.is_training:False})\n",
    "                    train_acc, train_acc_classes = per_class_acc(logits, labels)  # check in, comment out in formal run\n",
    "\n",
    "            # train log:\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metric('train/acc', train_acc, step)\n",
    "                self.experiment.log_metric('train/cap_0', train_acc_classes[0], step)\n",
    "                self.experiment.log_metric('train/cap_1', train_acc_classes[1], step)\n",
    "\n",
    "            # validate:\n",
    "            if validateloader != None:\n",
    "                avg_batch_loss_seg,avg_batch_loss_total, acc_total, cap_0, cap_1 = self.validateSeg(validateloader, epoch)\n",
    "            else:\n",
    "                avg_batch_loss_seg,avg_batch_loss_total, acc_total, cap_0, cap_1 = self.validateSeg(loader, epoch)\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metric('valid/acc', acc_total, step)\n",
    "                self.experiment.log_metric('valid/cap_0', cap_0, step)\n",
    "                self.experiment.log_metric('valid/cap_1', cap_1, step)\n",
    "                self.experiment.log_metric('valid/loss_seg', avg_batch_loss_seg, step)\n",
    "                self.experiment.log_metric('valid/loss_total', avg_batch_loss_total, step)\n",
    "                \n",
    "            # test:\n",
    "            if testloader != None:\n",
    "                acc_total, cap_0, cap_1 = self.validateSeg(testloader, epoch, is_testing=True)\n",
    "                if self.experiment is not None:\n",
    "                    self.experiment.log_metric('test/acc', acc_total, step)\n",
    "                    self.experiment.log_metric('test/cap_0', cap_0, step)\n",
    "                    self.experiment.log_metric('test/cap_1', cap_1, step)\n",
    "\n",
    "            # log best metrics\n",
    "            if acc_total > best_accuracy:  # if best validation accuracy so far, save model parameters\n",
    "                print('Character error rate improved, save model')\n",
    "                best_accuracy = acc_total\n",
    "                noImprovementSince = 0\n",
    "                self.saveSeg(epoch)\n",
    "                open(join(self.args.ckptpath_seg, 'accuracy.txt'), 'w').write(\n",
    "                    'Validation accuracy, class 0, class 1 capture rates of saved model: %f%%, %f%% and %f%% ' % (\n",
    "                    acc_total * 100.0, cap_0 * 100.0, cap_1 * 100.0))\n",
    "                if self.experiment is not None:\n",
    "                    self.experiment.log_metric('best/acc', acc_total, step)\n",
    "                    self.experiment.log_metric('best/cap_0', cap_0, step)\n",
    "                    self.experiment.log_metric('best/cap_1', cap_1, step)\n",
    "            else:\n",
    "                print('Character error rate not improved')\n",
    "                noImprovementSince += 1\n",
    "\n",
    "            # stop training\n",
    "            if epoch >= self.args.max_epoch: print('Done with training at epoch', epoch,'sigoptObservation=' + str(best_accuracy)); break\n",
    "\n",
    "    def validateSeg(self, loader, epoch, is_testing=False):\n",
    "        \"validate NN\"\n",
    "        if not is_testing:\n",
    "            print('Validating NN')\n",
    "        else:\n",
    "            print('Testing NN')\n",
    "        total_val_loss_seg = 0.0\n",
    "        total_val_loss_total= 0.0\n",
    "        # num_batches=len(loader)\n",
    "        hist = np.zeros((self.num_classes, self.num_classes))\n",
    "\n",
    "        image_upload_count = 0\n",
    "        for idx, (images, labels, labels_recg) in enumerate(loader):\n",
    "            \n",
    "            images = images.numpy()\n",
    "            labels = labels.numpy()\n",
    "            bt_size=len(images)            \n",
    "            sparse=self.toSparse(labels_recg) #added!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            #labels_recg=labels_recg.numpy() #added!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            #sparse=self.toSparse()\n",
    "            val_loss_total, val_loss_seg, val_logit = self.sess.run([self.loss_total,self.loss_seg, self.logit_seg], feed_dict=\n",
    "            {self.input_images_seg: images,  # check in, comment out in formal run\n",
    "             self.input_labels_seg: labels,\n",
    "             self.gtTexts:sparse,\n",
    "             self.seqLen: [Model.maxTextLen] * bt_size,#self.batchsize_recg, #changed!!!!!!!!!!!!!!!!!!!\n",
    "             self.phase_train: False,\n",
    "             self.is_training:False})  # self.loss,val_loss,\n",
    "\n",
    "            total_val_loss_seg += val_loss_seg\n",
    "            total_val_loss_total += val_loss_total#!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            \n",
    "            hist += get_hist(val_logit, labels)\n",
    "            # val_loss=total_val_loss / len(validateloader)*batch_size\n",
    "\n",
    "            if epoch == self.args.max_epoch and image_upload_count < 1000 and self.experiment is not None:  # decide how many images to upload\n",
    "                pred = val_logit.argmax(3)\n",
    "                images = np.squeeze(images, axis=3)\n",
    "                image_upload_count = log_images(images, pred, image_upload_count, self.experiment,\n",
    "                                                self.args.ckptpath_seg)\n",
    "\n",
    "        avg_batch_loss_seg = total_val_loss_seg / idx\n",
    "        avg_batch_loss_total = total_val_loss_total / idx\n",
    "        \n",
    "        cls_sample_nums = hist.sum(1).astype(float)\n",
    "        capture_array = np.diag(hist)\n",
    "        acc_total = capture_array.sum() / hist.sum()\n",
    "        capture_rate_ls = []\n",
    "        for cls in range(self.num_classes):\n",
    "            if cls_sample_nums[cls] == 0:\n",
    "                capture_rate = 0.0\n",
    "            else:\n",
    "                capture_rate = capture_array[cls] / cls_sample_nums[cls]\n",
    "            capture_rate_ls.append(capture_rate)\n",
    "        # iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n",
    "        # mean_iu=np.nanmean(iu)\n",
    "        print('VALID: Total accuracy: %f%%. Class 0 capture: %f%%. Class 1 capture: %f%%' % (\n",
    "            acc_total * 100.0, capture_rate_ls[0] * 100.0, capture_rate_ls[1] * 100.0))\n",
    "        return avg_batch_loss_seg,avg_batch_loss_total, acc_total, capture_rate_ls[0], capture_rate_ls[1]\n",
    "\n",
    "            ##################Recognition##########################\n",
    "\n",
    "    def trainBatchRecg(self, images, labels): #!!!!!!!!!!!!!!!!\n",
    "\n",
    "        \"feed a batch into the NN to train it\"\n",
    "        sparse = self.toSparse(labels)\n",
    "        bt_size=len(images)#!!!!!!!!!!!!!!!!!!!!\n",
    "        lrnrate = self.lrInit if self.batchesTrained < self.args.lrDrop1 else (self.lrInit * 1e-1 if self.batchesTrained < self.args.lrDrop2 else self.lrInit * 1e-2)  # decay learning rate\n",
    "        (_, lossVal) = self.sess.run([self.optimizer, self.loss_recg],\n",
    "                                    {self.input_images_recg: images,\n",
    "                                     self.gtTexts: sparse,\n",
    "                                     self.seqLen: [Model.maxTextLen]*bt_size,# * self.batchsize_recg,\n",
    "                                     self.learning_rate: lrnrate,\n",
    "                                     self.is_training: True})\n",
    "        self.batchesTrained += 1\n",
    "        return lossVal\n",
    "\n",
    "    def inferBatchRecg(self, imgs):  # modify to compatible to torch. previous def inferBatch(self, batch)\n",
    "        \"feed a batch into the NN to recngnize the texts\"\n",
    "        '''if batch to infer less than args.batchsize, error'''\n",
    "\n",
    "        bt_size = len(imgs)  # yike !!!!!!!!\n",
    "\n",
    "        decoded = self.sess.run(self.decoder,\n",
    "                                {self.input_images_recg: imgs, self.seqLen: [Model.maxTextLen] * bt_size,\n",
    "                                 self.is_training: False})  # yike self.batchsize!!!!!!!!!\n",
    "        return self.decoderOutputToText(decoded)  # previous batch.imgs\n",
    "\n",
    "    def saveRecg(self, epoch):\n",
    "        \"save model to file\"\n",
    "        self.saver_recg.save(self.sess, join(self.args.ckptpath_recg, 'model'), global_step=epoch)\n",
    "\n",
    "    def trainRecg(self, loader, validateloader=None, testloader=None):  # model\n",
    "\n",
    "        \"train NN\"\n",
    "        epoch = 0  # number of training epochs since start\n",
    "        bestCharErrorRate = bestWordErrorRate = float('inf')  # best valdiation character error rate\n",
    "\n",
    "        while True:\n",
    "            epoch += 1;\n",
    "            print('Epoch:', epoch, ' Training...')\n",
    "\n",
    "            # train\n",
    "            counter = 0\n",
    "            step = 0\n",
    "\n",
    "            for idx, (images, labels) in enumerate(loader):\n",
    "\n",
    "                # convert torchtensor to numpy\n",
    "                images = images.numpy()\n",
    "\n",
    "                # train batch\n",
    "                # try:\n",
    "                loss = self.trainBatchRecg(images, labels)\n",
    "                # except:\n",
    "                #  print(labels)\n",
    "                step += 1\n",
    "\n",
    "                # save training status\n",
    "                if np.mod(idx, 110) == 0:\n",
    "                    print('TRAIN: Batch:', idx / len(loader), 'Loss:', loss)\n",
    "                    if self.experiment is not None:\n",
    "                        self.experiment.log_metric('train/loss', loss, step)\n",
    "\n",
    "                # log images\n",
    "                if epoch == 1 and counter < 5:\n",
    "                    text = labels[counter]\n",
    "                    utils_recg.log_image(self.experiment, images[counter], text, 'train', self.args.ckptpath_recg, counter, epoch)\n",
    "                    counter += 1\n",
    "                # for debug\n",
    "                # if idx >2:\n",
    "                #  break\n",
    "\n",
    "            # validate\n",
    "            if validateloader != None:\n",
    "                charErrorRate, wordAccuracy = self.validateRecg(validateloader, epoch)  # yike !!!!!!!!!!!!\n",
    "            else:  # yike !!!!!!!!!!!!!\n",
    "                charErrorRate, wordAccuracy = self.validateRecg(loader, epoch)\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metric('valid/cer', charErrorRate, step)\n",
    "                self.experiment.log_metric('valid/wer', 1 - wordAccuracy, step)\n",
    "\n",
    "            # test\n",
    "            if testloader != None:\n",
    "                charErrorRate, wordAccuracy = self.validateRecg(testloader, epoch, is_testing=True)\n",
    "                if self.experiment is not None:\n",
    "                    self.experiment.log_metric('test/cer', charErrorRate, step)\n",
    "                    self.experiment.log_metric('test/wer', 1 - wordAccuracy, step)\n",
    "\n",
    "            # log best metrics\n",
    "            if charErrorRate < bestCharErrorRate:  # if best validation accuracy so far, save model parameters\n",
    "                print('Character error rate improved, save model')\n",
    "                bestCharErrorRate = charErrorRate\n",
    "                noImprovementSince = 0\n",
    "                self.saveRecg(epoch)\n",
    "                open(join(args.ckptpath_recg, 'accuracy.txt'), 'w').write(\n",
    "                    'Validation character error rate of saved model: %f%%' % (charErrorRate * 100.0))\n",
    "            else:\n",
    "                print('Character error rate not improved')\n",
    "                noImprovementSince += 1\n",
    "            if 1 - wordAccuracy < bestWordErrorRate:\n",
    "                bestWordErrorRate = 1 - wordAccuracy\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metric('best/cer', bestCharErrorRate, step)\n",
    "                self.experiment.log_metric('best/wer', bestWordErrorRate, step)\n",
    "\n",
    "            # stop training\n",
    "            if epoch >= args.epochEnd: print('Done with training at epoch', epoch,\n",
    "                                         'sigoptObservation=' + str(bestCharErrorRate)); break\n",
    "\n",
    "    def validateRecg(self, loader, epoch, is_testing=False):\n",
    "        \"validate NN\"\n",
    "        if not is_testing: print('Validating NN')\n",
    "        else: print('Testing NN')\n",
    "        #loader.validationSet() # comment out by yike. see row 141\n",
    "        numCharErr, numCharTotal, numWordOK, numWordTotal = 0, 0, 0, 0\n",
    "        plt.figure(figsize=(6,2))\n",
    "        counter = 0\n",
    "        '''\n",
    "        yike: convert to troch dataloader, test\n",
    "        '''\n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            if np.mod(idx,10)==0:\n",
    "                print(str(idx*50*8))\n",
    "            images=images.numpy()\n",
    "            recognized=self.inferBatchRecg(images)\n",
    "\n",
    "            for i in range(len(recognized)):\n",
    "                numWordOK += 1 if labels[i] == recognized[i] else 0 #batch.gtTexts[i]\n",
    "                numWordTotal += 1\n",
    "                dist = editdistance.eval(recognized[i], labels[i])# batch.gtTexts[i])\n",
    "                numCharErr += dist\n",
    "                numCharTotal += len(labels[i]) #batch.gtTexts[i]\n",
    "\n",
    "                if is_testing and epoch==self.args.epochEnd and self.experiment is not None: #batch.gtTexts[i]\n",
    "                    text = ' '.join(['[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + labels[i] + '\"', '->', '\"' + recognized[i] + '\"'])\n",
    "                    utils_recg.log_image(self.experiment, images[i], text, 'test-'+('ok' if dist==0 else 'err'), self.args.ckptpath_recg, counter, epoch)\n",
    "                    counter += 1 # previous batch.imgs[i]\n",
    "\n",
    "            if epoch==1 and counter<5 and not is_testing and self.experiment is not None: # log images\n",
    "                text = ' '.join(['[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + labels[i] + '\"', '->', '\"' + recognized[i] + '\"'])\n",
    "                utils_recg.log_image(self.experiment, images[i], text, 'valid', args.ckptpath_recg, counter, epoch) #batch.gtTexts[i]\n",
    "                counter += 1 #batch.imgs[i]\n",
    "\n",
    "\n",
    "        # print validation result\n",
    "        charErrorRate = numCharErr / numCharTotal\n",
    "        wordAccuracy = numWordOK / numWordTotal\n",
    "        print('VALID: Character error rate: %f%%. Word accuracy: %f%%.' % (charErrorRate * 100.0, wordAccuracy * 100.0))\n",
    "        return charErrorRate, wordAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=Model(args, charList=\"\", loss_beta=0.6,loss_weight=[.5,.5], decoderType=DecoderType.BestPath,experiment=None,mustRestore_seg=False,mustRestore_recg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-urlTranferFrom'], dest='urlTranferFrom', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help=' archived model url ', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#General Settings\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# system basics\n",
    "#parser.add_argument(\"-name\", default='segnet_unet_hvbw_all_combine_100_epoches', type=str, help=\"name of the log\") #debug model_intersect # segnet_no_intersect_1conv_64_channels_30epoch_unet_github\n",
    "parser.add_argument(\"-name_seg\", default='debug_seg_what_ever', type=str, help=\"name of the log\") #debug model_intersect # segnet_no_intersect_1conv_64_channels_30epoch_unet_github\n",
    "#segnet_binary_100epoch_unet_github\n",
    "parser.add_argument(\"-gpu\", default='1', type=str, help=\"gpu numbers\")\n",
    "\n",
    "parser.add_argument(\"-train\", default=True, help=\"train the NN\", action=\"store_true\")\n",
    "parser.add_argument(\"-validate\", help=\"validate the NN\", action=\"store_true\")\n",
    "\n",
    "parser.add_argument(\"-transfer\",default=False, help=\"test the NN\", action=\"store_true\")\n",
    "\n",
    "parser.add_argument(\"-test\",default=False, help=\"test the NN\", action=\"store_true\")\n",
    "\n",
    "# image and logistic parameters \n",
    "parser.add_argument(\"-image_h\", default=32, type=int, help='image height') #('image_h', \"360\", \"\"\" image height \"\"\") 32\n",
    "parser.add_argument(\"-image_w\", default=128, type=int, help='image width')#('image_w', \"480\", \"\"\" image width \"\"\")128\n",
    "#parser.add_argument(\"-image_h\", default=360, type=int, help='image height') \n",
    "#parser.add_argument(\"-image_w\", default=480, type=int, help='image width')\n",
    "\n",
    "parser.add_argument(\"-image_c\", default=1, type=int, help='image channel')#('image_c', \"3\", \"\"\" image channel (RGB) \"\"\")\n",
    "parser.add_argument(\"-num_class\", default=2, type=int, help='total class number')\n",
    "\n",
    "# training hyperparam\n",
    "parser.add_argument(\"-batch_size_seg\", default=10, type=int, help='batch_size')\n",
    "parser.add_argument(\"-lrInit\", default=1e-3, type=int, help='initial lr')\n",
    "parser.add_argument(\"-lrDrop1\", default=10, type=int, help='step to drop lr by 10 first time') # not sure\n",
    "parser.add_argument(\"-lrDrop2\", default=1000, type=int, help='step to drop lr by 10 sexond time') # not sure\n",
    "parser.add_argument('-max_epoch',default=100, type=int,help='max epoch numbers')\n",
    "\n",
    "\n",
    "\n",
    "# file paths\n",
    "parser.add_argument('-ckpt_root', default=\"/root/ckpt\", type=str,help= \"dir to store ckpt\") # log_dir !!!!!\n",
    "parser.add_argument('-data_root', default=\"/root/datasets\", type=str, help=\" root to any data folder \")\n",
    "parser.add_argument('-urlTranferFrom', default=\"\", type=str, help=\" archived model url \")\n",
    "\n",
    "\n",
    "#args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognition Model\n",
    "# basic operations\n",
    "parser.add_argument(\"-name_recg\", default=\"recg_new_five_datasets\", type=str, help=\"name of the log\") #'dense_128_32_noartifact_beamsearch_5_datasets'\n",
    "#parser.add_argument(\"-gpu\", default='-1', type=str, help=\"gpu numbers\")\n",
    "#parser.add_argument(\"-train\", help=\"train the NN\", action=\"store_true\")\n",
    "#parser.add_argument(\"-validate\", help=\"validate the NN\", action=\"store_true\")\n",
    "#parser.add_argument(\"-transfer\", action=\"store_true\")\n",
    "#actually not effective:\n",
    "parser.add_argument(\"-batchesTrained\", default=0, type=int, help='number of batches already trained (for lr schedule)') \n",
    "# beam search\n",
    "parser.add_argument(\"-beamsearch\", help=\"use beam search instead of best path decoding\",default=True, action=\"store_true\")\n",
    "parser.add_argument(\"-wordbeamsearch\", help=\"use word beam search instead of best path decoding\", action=\"store_true\")\n",
    "# training hyperparam\n",
    "parser.add_argument(\"-batchsize_recg\", default=10, type=int, help='batch size') # actually not effective in infrerence\n",
    "#parser.add_argument(\"-lrInit\", default=1e-2, type=float, help='initial learning rate') # actually not effective\n",
    "parser.add_argument(\"-optimizer\", default='rmsprop', help=\"adam, rmsprop, momentum\") # actually not effective\n",
    "parser.add_argument(\"-wdec\", default=1e-4, type=float, help='weight decay') # acctually not effective\n",
    "#parser.add_argument(\"-lrDrop1\", default=10, type=int, help='step to drop lr by 10 first time')\n",
    "#parser.add_argument(\"-lrDrop2\", default=1000, type=int, help='step to drop lr by 10 sexond time')\n",
    "parser.add_argument(\"-epochEnd\", default=39, type=int, help='end after this many epochs')\n",
    "# trainset hyperparam\n",
    "#parser.add_argument(\"-noncustom\", help=\"noncustom (original) augmentation technique\", action=\"store_true\")\n",
    "#parser.add_argument(\"-noartifact\", help=\"dont insert artifcats\", action=\"store_true\")\n",
    "#parser.add_argument(\"-iam\", help='use iam dataset', action='store_true')\n",
    "# densenet hyperparam\n",
    "parser.add_argument(\"-nondensenet\", help=\"use noncustom (original) vanilla cnn\", action=\"store_true\")\n",
    "parser.add_argument(\"-growth_rate\", default=12, type=int, help='growth rate (k)')\n",
    "parser.add_argument(\"-layers_per_block\", default=18, type=int, help='number of layers per block')\n",
    "parser.add_argument(\"-total_blocks\", default=5, type=int, help='nuber of densenet blocks')\n",
    "parser.add_argument(\"-keep_prob\", default=1, type=float, help='keep probability in dropout')\n",
    "parser.add_argument(\"-reduction\", default=0.4, type=float, help='reduction factor in 1x1 conv in transition layers')\n",
    "parser.add_argument(\"-bc_mode\", default=True, type=bool, help=\"bottleneck and compresssion mode\")\n",
    "# rnn,  hyperparams\n",
    "parser.add_argument(\"-rnndim\", default=256, type=int, help='rnn dimenstionality') #256\n",
    "parser.add_argument(\"-rnnsteps\", default=32, type=int, help='number of desired time steps (image slices) to feed rnn')\n",
    "# img size\n",
    "parser.add_argument(\"-imgsize\", default=[128,32], type=int, nargs='+') #qyk default 128,32 // use segnet definition\n",
    "# testset crop\n",
    "#parser.add_argument(\"-crop_r1\", default=3, type=int)\n",
    "#parser.add_argument(\"-crop_r2\", default=28, type=int)\n",
    "#parser.add_argument(\"-crop_c1\", default=10, type=int)\n",
    "#parser.add_argument(\"-crop_c2\", default=115, type=int)\n",
    "# filepaths\n",
    "parser.add_argument(\"-dataroot\", default='/root/datasets', type=str)\n",
    "#######parser.add_argument(\"-ckptroot\", default='/root/ckpt', type=str)##############\n",
    "#parser.add_argument(\"-urlTransferFrom\", default=None, type=str)\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "### SegNet\n",
    "home = os.environ['HOME']\n",
    "#name = args.name\n",
    "#ckptroot = join(home, 'ckpt')\n",
    "#args.ckptpath = join(ckptroot, name)\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "\n",
    "####args = parser.parse_known_args()[0]\n",
    "\n",
    "name_seg = args.name_seg\n",
    "name_recg=args.name_recg\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "\n",
    "ckptroot = args.ckpt_root\n",
    "args.ckptpath_seg = join(ckptroot, name_seg)\n",
    "args.ckptpath_recg = join(ckptroot, name_recg)\n",
    "if args.name_seg=='debug_seg_': shutil.rmtree(args.ckptpath_seg, ignore_errors=True)\n",
    "if args.name_recg=='debug_recg_': shutil.rmtree(args.ckptpath_recg, ignore_errors=True)\n",
    "\n",
    "os.makedirs(args.ckptpath_seg, exist_ok=True)\n",
    "os.makedirs(args.ckptpath_recg, exist_ok=True)\n",
    "\n",
    "#recg_name=args.recg_name\n",
    "#args.regckptpath=join(ckptroot,recg_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_name('recg_new_five_datasets')\n",
    "experiment.log_parameters(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets_recg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/datasets/iam_handwriting already exists, skipping download\n",
      "/root/datasets/htr_assets already exists, skipping download\n",
      "/root/datasets/img_print_single already exists, skipping download\n",
      "/root/datasets/irs_handwriting already exists, skipping download\n",
      "/root/datasets/text_recognition already exists, skipping download\n",
      "screened :815531\n",
      "1262239\n",
      "23982.54\n",
      "1262.24\n",
      "GGG\n",
      "[None, 32, 128, 1]\n",
      "0 conv1: (?, ?, ?, 32)\n",
      "0 conv2: (?, ?, ?, 32)\n",
      "1 conv1: (?, ?, ?, 64)\n",
      "1 conv2: (?, ?, ?, 64)\n",
      "2 conv1: (?, ?, ?, 128)\n",
      "2 conv2: (?, ?, ?, 128)\n",
      "1 h_deconv: (?, ?, ?, 64)\n",
      "1 h_deconv_concat: (?, ?, ?, ?)\n",
      "1 h_conv1_post_deconv: (?, ?, ?, 64)\n",
      "1 h_conv2_post_deconv: (?, ?, ?, 64)\n",
      "0 h_deconv: (?, ?, ?, 32)\n",
      "0 h_deconv_concat: (?, ?, ?, ?)\n",
      "0 h_conv1_post_deconv: (?, ?, ?, 32)\n",
      "0 h_conv2_post_deconv: (?, ?, ?, 32)\n",
      "0 outmap: (?, ?, ?, 2)\n",
      "(?, ?, ?, 2)\n",
      "loss_seg: ()\n",
      "clean output from seg: (?, 32, 128)\n",
      "recg input: (?, 128, 32)\n",
      "shape of cnn input: [None, 128, 32]\n",
      "Build Densenet4htr model with 5 blocks, 9 bottleneck layers and 9 composite layers each.\n",
      "Depth: 96\n",
      "Reduction at transition layers: 0.4\n",
      "densenet feature extractor graph built in (sec): 5.786654949188232\n",
      "Total training params: 1.0M\n",
      "shape of cnn output: [None, 32, 1, 178]\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "INFO:tensorflow:Summary name graph_segmentation/loss/cross_entropy (raw) is illegal; using graph_segmentation/loss/cross_entropy__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name graph_segmentation/loss/cross_entropy (raw) is illegal; using graph_segmentation/loss/cross_entropy__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name add (raw) is illegal; using add__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name add (raw) is illegal; using add__raw_ instead.\n",
      "COMET ERROR: Failed to extract parameters from Estimator.init()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toto_loss_shape: Tensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET ERROR: Failed to extract parameters from Estimator.init()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.5.2 (default, Nov 12 2018, 13:43:14) \n",
      "[GCC 5.4.0 20160609]\n",
      "Tensorflow: 1.12.0-rc0\n",
      "Ran global_variables_initializer first\n",
      "Ran initializers.variables on segnet trainable variables\n",
      "INFO:tensorflow:Restoring parameters from /root/ckpt/recg_new_five_datasets/model-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/ckpt/recg_new_five_datasets/model-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init with stored values from /root/ckpt/recg_new_five_datasets/model-1\n",
      "Epoch: 1  Training...\n",
      "TRAIN: Batch: 0.0 Loss: 4.435277\n",
      "TRAIN: Batch: 0.0009173393822136234 Loss: 2.64975\n",
      "TRAIN: Batch: 0.0018346787644272467 Loss: 2.8685572\n",
      "TRAIN: Batch: 0.0027520181466408698 Loss: 3.213683\n",
      "TRAIN: Batch: 0.0036693575288544934 Loss: 2.489227\n",
      "TRAIN: Batch: 0.004586696911068116 Loss: 1.7958739\n",
      "TRAIN: Batch: 0.0055040362932817395 Loss: 2.251549\n",
      "TRAIN: Batch: 0.006421375675495364 Loss: 5.5027914\n",
      "TRAIN: Batch: 0.007338715057708987 Loss: 2.992698\n",
      "TRAIN: Batch: 0.00825605443992261 Loss: 10.519098\n",
      "TRAIN: Batch: 0.009173393822136233 Loss: 1.4524218\n",
      "TRAIN: Batch: 0.010090733204349856 Loss: 4.627021\n",
      "TRAIN: Batch: 0.011008072586563479 Loss: 2.1935146\n",
      "TRAIN: Batch: 0.011925411968777104 Loss: 3.923425\n",
      "TRAIN: Batch: 0.012842751350990727 Loss: 2.949217\n",
      "TRAIN: Batch: 0.01376009073320435 Loss: 5.6383667\n",
      "TRAIN: Batch: 0.014677430115417974 Loss: 2.3639596\n",
      "TRAIN: Batch: 0.015594769497631597 Loss: 5.4015493\n",
      "TRAIN: Batch: 0.01651210887984522 Loss: 3.7433078\n",
      "TRAIN: Batch: 0.017429448262058844 Loss: 1.6370544\n",
      "TRAIN: Batch: 0.018346787644272465 Loss: 1.5674503\n",
      "TRAIN: Batch: 0.01926412702648609 Loss: 1.0779989\n",
      "TRAIN: Batch: 0.02018146640869971 Loss: 2.9173326\n",
      "TRAIN: Batch: 0.021098805790913337 Loss: 2.2661448\n",
      "TRAIN: Batch: 0.022016145173126958 Loss: 2.7748058\n",
      "TRAIN: Batch: 0.022933484555340583 Loss: 3.004733\n",
      "TRAIN: Batch: 0.023850823937554208 Loss: 2.0334258\n",
      "TRAIN: Batch: 0.02476816331976783 Loss: 2.3749578\n",
      "TRAIN: Batch: 0.025685502701981455 Loss: 3.6987522\n",
      "TRAIN: Batch: 0.026602842084195076 Loss: 2.2137845\n",
      "TRAIN: Batch: 0.0275201814664087 Loss: 7.454465\n",
      "TRAIN: Batch: 0.028437520848622323 Loss: 2.784802\n",
      "TRAIN: Batch: 0.029354860230835948 Loss: 1.847782\n",
      "TRAIN: Batch: 0.03027219961304957 Loss: 0.90155923\n",
      "TRAIN: Batch: 0.031189538995263194 Loss: 1.0749514\n",
      "TRAIN: Batch: 0.03210687837747682 Loss: 5.436688\n",
      "TRAIN: Batch: 0.03302421775969044 Loss: 6.684768\n",
      "TRAIN: Batch: 0.03394155714190406 Loss: 4.3766837\n",
      "TRAIN: Batch: 0.03485889652411769 Loss: 2.6183903\n",
      "TRAIN: Batch: 0.03577623590633131 Loss: 1.5298805\n",
      "TRAIN: Batch: 0.03669357528854493 Loss: 2.6657853\n",
      "TRAIN: Batch: 0.037610914670758555 Loss: 2.1559281\n",
      "TRAIN: Batch: 0.03852825405297218 Loss: 1.5273643\n",
      "TRAIN: Batch: 0.039445593435185805 Loss: 2.8016896\n",
      "TRAIN: Batch: 0.04036293281739942 Loss: 0.85725427\n",
      "TRAIN: Batch: 0.04128027219961305 Loss: 1.2693713\n",
      "TRAIN: Batch: 0.04219761158182667 Loss: 1.9074244\n",
      "TRAIN: Batch: 0.0431149509640403 Loss: 3.7349815\n",
      "TRAIN: Batch: 0.044032290346253916 Loss: 0.98684275\n",
      "TRAIN: Batch: 0.04494962972846754 Loss: 3.2753913\n",
      "TRAIN: Batch: 0.045866969110681166 Loss: 0.84490526\n",
      "TRAIN: Batch: 0.04678430849289479 Loss: 4.5400953\n",
      "TRAIN: Batch: 0.047701647875108416 Loss: 2.9554873\n",
      "TRAIN: Batch: 0.048618987257322034 Loss: 2.6988358\n",
      "TRAIN: Batch: 0.04953632663953566 Loss: 4.9427156\n",
      "TRAIN: Batch: 0.050453666021749284 Loss: 5.4968286\n",
      "TRAIN: Batch: 0.05137100540396291 Loss: 3.248191\n",
      "TRAIN: Batch: 0.05228834478617653 Loss: 4.7750154\n",
      "TRAIN: Batch: 0.05320568416839015 Loss: 1.5792699\n",
      "TRAIN: Batch: 0.05412302355060378 Loss: 2.9578822\n",
      "TRAIN: Batch: 0.0550403629328174 Loss: 2.4779844\n",
      "TRAIN: Batch: 0.05595770231503102 Loss: 3.6606476\n",
      "TRAIN: Batch: 0.056875041697244645 Loss: 3.9333167\n",
      "TRAIN: Batch: 0.05779238107945827 Loss: 6.219516\n",
      "TRAIN: Batch: 0.058709720461671895 Loss: 2.1175494\n",
      "TRAIN: Batch: 0.05962705984388551 Loss: 1.3731608\n",
      "TRAIN: Batch: 0.06054439922609914 Loss: 5.2882023\n",
      "TRAIN: Batch: 0.06146173860831276 Loss: 2.4597883\n",
      "TRAIN: Batch: 0.06237907799052639 Loss: 4.8926835\n",
      "TRAIN: Batch: 0.06329641737274001 Loss: 4.884119\n",
      "TRAIN: Batch: 0.06421375675495364 Loss: 2.0461028\n",
      "TRAIN: Batch: 0.06513109613716725 Loss: 5.796175\n",
      "TRAIN: Batch: 0.06604843551938087 Loss: 2.9138472\n",
      "TRAIN: Batch: 0.0669657749015945 Loss: 1.6779864\n",
      "TRAIN: Batch: 0.06788311428380812 Loss: 1.8993691\n",
      "TRAIN: Batch: 0.06880045366602175 Loss: 2.9611044\n",
      "TRAIN: Batch: 0.06971779304823537 Loss: 3.1473954\n",
      "TRAIN: Batch: 0.070635132430449 Loss: 4.1575985\n",
      "TRAIN: Batch: 0.07155247181266262 Loss: 9.417085\n",
      "TRAIN: Batch: 0.07246981119487625 Loss: 3.623149\n",
      "TRAIN: Batch: 0.07338715057708986 Loss: 3.6480598\n",
      "TRAIN: Batch: 0.07430448995930349 Loss: 2.007777\n",
      "TRAIN: Batch: 0.07522182934151711 Loss: 3.2358747\n",
      "TRAIN: Batch: 0.07613916872373074 Loss: 4.43087\n",
      "TRAIN: Batch: 0.07705650810594436 Loss: 3.597616\n",
      "TRAIN: Batch: 0.07797384748815799 Loss: 2.9324658\n",
      "TRAIN: Batch: 0.07889118687037161 Loss: 2.898106\n",
      "TRAIN: Batch: 0.07980852625258524 Loss: 3.4234107\n",
      "TRAIN: Batch: 0.08072586563479885 Loss: 4.160927\n",
      "TRAIN: Batch: 0.08164320501701247 Loss: 5.6977277\n",
      "TRAIN: Batch: 0.0825605443992261 Loss: 3.143323\n",
      "TRAIN: Batch: 0.08347788378143972 Loss: 4.182151\n",
      "TRAIN: Batch: 0.08439522316365335 Loss: 1.4180379\n",
      "TRAIN: Batch: 0.08531256254586697 Loss: 6.172939\n",
      "TRAIN: Batch: 0.0862299019280806 Loss: 2.776146\n",
      "TRAIN: Batch: 0.08714724131029422 Loss: 1.9715579\n",
      "TRAIN: Batch: 0.08806458069250783 Loss: 5.3553634\n",
      "TRAIN: Batch: 0.08898192007472146 Loss: 1.8656946\n",
      "TRAIN: Batch: 0.08989925945693508 Loss: 1.6952672\n",
      "TRAIN: Batch: 0.09081659883914871 Loss: 2.9938877\n",
      "TRAIN: Batch: 0.09173393822136233 Loss: 3.207698\n",
      "TRAIN: Batch: 0.09265127760357596 Loss: 1.754344\n",
      "TRAIN: Batch: 0.09356861698578958 Loss: 1.4853765\n",
      "TRAIN: Batch: 0.09448595636800321 Loss: 3.229953\n",
      "TRAIN: Batch: 0.09540329575021683 Loss: 0.89191747\n",
      "TRAIN: Batch: 0.09632063513243044 Loss: 7.039327\n",
      "TRAIN: Batch: 0.09723797451464407 Loss: 0.8092583\n",
      "TRAIN: Batch: 0.0981553138968577 Loss: 1.9071078\n",
      "TRAIN: Batch: 0.09907265327907132 Loss: 5.2685347\n",
      "TRAIN: Batch: 0.09998999266128494 Loss: 1.4371905\n",
      "TRAIN: Batch: 0.10090733204349857 Loss: 2.6102858\n",
      "TRAIN: Batch: 0.1018246714257122 Loss: 1.1284468\n",
      "TRAIN: Batch: 0.10274201080792582 Loss: 3.1820877\n",
      "TRAIN: Batch: 0.10365935019013943 Loss: 3.09843\n",
      "TRAIN: Batch: 0.10457668957235305 Loss: 1.4757867\n",
      "TRAIN: Batch: 0.10549402895456668 Loss: 3.0453398\n",
      "TRAIN: Batch: 0.1064113683367803 Loss: 3.8197849\n",
      "TRAIN: Batch: 0.10732870771899393 Loss: 3.584969\n",
      "TRAIN: Batch: 0.10824604710120755 Loss: 2.6658485\n",
      "TRAIN: Batch: 0.10916338648342118 Loss: 4.395699\n",
      "TRAIN: Batch: 0.1100807258656348 Loss: 1.892928\n",
      "TRAIN: Batch: 0.11099806524784842 Loss: 2.213949\n",
      "TRAIN: Batch: 0.11191540463006204 Loss: 9.230759\n",
      "TRAIN: Batch: 0.11283274401227567 Loss: 4.662766\n",
      "TRAIN: Batch: 0.11375008339448929 Loss: 4.0598993\n",
      "TRAIN: Batch: 0.11466742277670292 Loss: 1.4615703\n",
      "TRAIN: Batch: 0.11558476215891654 Loss: 1.2362837\n",
      "TRAIN: Batch: 0.11650210154113017 Loss: 2.4005337\n",
      "TRAIN: Batch: 0.11741944092334379 Loss: 0.72003937\n",
      "TRAIN: Batch: 0.11833678030555742 Loss: 1.7179894\n",
      "TRAIN: Batch: 0.11925411968777103 Loss: 2.8686078\n",
      "TRAIN: Batch: 0.12017145906998465 Loss: 2.4750636\n",
      "TRAIN: Batch: 0.12108879845219828 Loss: 4.4497075\n",
      "TRAIN: Batch: 0.1220061378344119 Loss: 2.2882113\n",
      "TRAIN: Batch: 0.12292347721662553 Loss: 1.638792\n",
      "TRAIN: Batch: 0.12384081659883915 Loss: 2.6713617\n",
      "TRAIN: Batch: 0.12475815598105278 Loss: 4.223713\n",
      "TRAIN: Batch: 0.1256754953632664 Loss: 1.7180293\n",
      "TRAIN: Batch: 0.12659283474548003 Loss: 5.7151604\n",
      "TRAIN: Batch: 0.12751017412769364 Loss: 1.0108131\n",
      "TRAIN: Batch: 0.12842751350990728 Loss: 3.17209\n",
      "TRAIN: Batch: 0.1293448528921209 Loss: 2.5385916\n",
      "TRAIN: Batch: 0.1302621922743345 Loss: 1.3792294\n",
      "TRAIN: Batch: 0.13117953165654814 Loss: 2.4102805\n",
      "TRAIN: Batch: 0.13209687103876175 Loss: 3.405233\n",
      "TRAIN: Batch: 0.1330142104209754 Loss: 2.4739387\n",
      "TRAIN: Batch: 0.133931549803189 Loss: 6.526144\n",
      "TRAIN: Batch: 0.13484888918540264 Loss: 1.9186101\n",
      "TRAIN: Batch: 0.13576622856761625 Loss: 3.4641347\n",
      "TRAIN: Batch: 0.1366835679498299 Loss: 3.0179965\n",
      "TRAIN: Batch: 0.1376009073320435 Loss: 3.564884\n",
      "TRAIN: Batch: 0.1385182467142571 Loss: 1.3188683\n",
      "TRAIN: Batch: 0.13943558609647075 Loss: 1.2930572\n",
      "TRAIN: Batch: 0.14035292547868436 Loss: 2.3700037\n",
      "TRAIN: Batch: 0.141270264860898 Loss: 1.555826\n",
      "TRAIN: Batch: 0.1421876042431116 Loss: 2.3299103\n",
      "TRAIN: Batch: 0.14310494362532525 Loss: 3.5006182\n",
      "TRAIN: Batch: 0.14402228300753886 Loss: 3.5798163\n",
      "TRAIN: Batch: 0.1449396223897525 Loss: 1.5789375\n",
      "TRAIN: Batch: 0.1458569617719661 Loss: 1.1777201\n",
      "TRAIN: Batch: 0.14677430115417972 Loss: 1.442045\n",
      "TRAIN: Batch: 0.14769164053639336 Loss: 1.8584311\n",
      "TRAIN: Batch: 0.14860897991860697 Loss: 2.0146549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.1495263193008206 Loss: 1.4576986\n",
      "TRAIN: Batch: 0.15044365868303422 Loss: 3.5443666\n",
      "TRAIN: Batch: 0.15136099806524786 Loss: 1.5881923\n",
      "TRAIN: Batch: 0.15227833744746147 Loss: 2.8310983\n",
      "TRAIN: Batch: 0.15319567682967508 Loss: 2.0619416\n",
      "TRAIN: Batch: 0.15411301621188872 Loss: 2.2268684\n",
      "TRAIN: Batch: 0.15503035559410233 Loss: 2.1349192\n",
      "TRAIN: Batch: 0.15594769497631597 Loss: 1.9845523\n",
      "TRAIN: Batch: 0.15686503435852958 Loss: 2.215694\n",
      "TRAIN: Batch: 0.15778237374074322 Loss: 0.6104269\n",
      "TRAIN: Batch: 0.15869971312295683 Loss: 3.2541864\n",
      "TRAIN: Batch: 0.15961705250517047 Loss: 1.6022534\n",
      "TRAIN: Batch: 0.16053439188738408 Loss: 2.64585\n",
      "TRAIN: Batch: 0.1614517312695977 Loss: 1.2654737\n",
      "TRAIN: Batch: 0.16236907065181133 Loss: 3.196745\n",
      "TRAIN: Batch: 0.16328641003402494 Loss: 1.9651768\n",
      "TRAIN: Batch: 0.16420374941623858 Loss: 1.5821607\n",
      "TRAIN: Batch: 0.1651210887984522 Loss: 1.7769135\n",
      "TRAIN: Batch: 0.16603842818066583 Loss: 4.5445976\n",
      "TRAIN: Batch: 0.16695576756287944 Loss: 3.0563579\n",
      "TRAIN: Batch: 0.16787310694509308 Loss: 3.8084738\n",
      "TRAIN: Batch: 0.1687904463273067 Loss: 1.1976272\n",
      "TRAIN: Batch: 0.1697077857095203 Loss: 3.215094\n",
      "TRAIN: Batch: 0.17062512509173394 Loss: 2.2722414\n",
      "TRAIN: Batch: 0.17154246447394755 Loss: 8.43562\n",
      "TRAIN: Batch: 0.1724598038561612 Loss: 2.2568202\n",
      "TRAIN: Batch: 0.1733771432383748 Loss: 4.0472956\n",
      "TRAIN: Batch: 0.17429448262058844 Loss: 1.9413216\n",
      "TRAIN: Batch: 0.17521182200280205 Loss: 1.6792101\n",
      "TRAIN: Batch: 0.17612916138501566 Loss: 2.9981456\n",
      "TRAIN: Batch: 0.1770465007672293 Loss: 1.7252777\n",
      "TRAIN: Batch: 0.17796384014944291 Loss: 3.208662\n",
      "TRAIN: Batch: 0.17888117953165655 Loss: 3.5527103\n",
      "TRAIN: Batch: 0.17979851891387016 Loss: 2.8938205\n",
      "TRAIN: Batch: 0.1807158582960838 Loss: 3.5316236\n",
      "TRAIN: Batch: 0.18163319767829741 Loss: 3.7372139\n",
      "TRAIN: Batch: 0.18255053706051105 Loss: 1.4490368\n",
      "TRAIN: Batch: 0.18346787644272466 Loss: 2.6524193\n",
      "TRAIN: Batch: 0.18438521582493828 Loss: 1.7991204\n",
      "TRAIN: Batch: 0.18530255520715191 Loss: 2.6104338\n",
      "TRAIN: Batch: 0.18621989458936553 Loss: 1.1360892\n",
      "TRAIN: Batch: 0.18713723397157916 Loss: 3.72852\n",
      "TRAIN: Batch: 0.18805457335379278 Loss: 3.549182\n",
      "TRAIN: Batch: 0.18897191273600641 Loss: 1.3452861\n",
      "TRAIN: Batch: 0.18988925211822003 Loss: 1.5973895\n",
      "TRAIN: Batch: 0.19080659150043366 Loss: 1.1109558\n",
      "TRAIN: Batch: 0.19172393088264728 Loss: 2.4536836\n",
      "TRAIN: Batch: 0.1926412702648609 Loss: 1.5343229\n",
      "TRAIN: Batch: 0.19355860964707453 Loss: 1.3659197\n",
      "TRAIN: Batch: 0.19447594902928814 Loss: 3.1977754\n",
      "TRAIN: Batch: 0.19539328841150178 Loss: 1.9118445\n",
      "TRAIN: Batch: 0.1963106277937154 Loss: 1.8535951\n",
      "TRAIN: Batch: 0.19722796717592903 Loss: 6.608861\n",
      "TRAIN: Batch: 0.19814530655814264 Loss: 1.4348145\n",
      "TRAIN: Batch: 0.19906264594035625 Loss: 2.8934894\n",
      "TRAIN: Batch: 0.1999799853225699 Loss: 1.8552266\n",
      "TRAIN: Batch: 0.2008973247047835 Loss: 0.7740654\n",
      "TRAIN: Batch: 0.20181466408699714 Loss: 1.8836911\n",
      "TRAIN: Batch: 0.20273200346921075 Loss: 3.0643396\n",
      "TRAIN: Batch: 0.2036493428514244 Loss: 4.5421867\n",
      "TRAIN: Batch: 0.204566682233638 Loss: 2.3977444\n",
      "TRAIN: Batch: 0.20548402161585164 Loss: 1.5708684\n",
      "TRAIN: Batch: 0.20640136099806525 Loss: 1.4749417\n",
      "TRAIN: Batch: 0.20731870038027886 Loss: 2.5816169\n",
      "TRAIN: Batch: 0.2082360397624925 Loss: 1.9458432\n",
      "TRAIN: Batch: 0.2091533791447061 Loss: 4.469246\n",
      "TRAIN: Batch: 0.21007071852691975 Loss: 1.9413505\n",
      "TRAIN: Batch: 0.21098805790913336 Loss: 3.7940695\n",
      "TRAIN: Batch: 0.211905397291347 Loss: 2.7674403\n",
      "TRAIN: Batch: 0.2128227366735606 Loss: 4.1619735\n",
      "TRAIN: Batch: 0.21374007605577425 Loss: 4.994864\n",
      "TRAIN: Batch: 0.21465741543798786 Loss: 2.6907878\n",
      "TRAIN: Batch: 0.21557475482020147 Loss: 4.060685\n",
      "TRAIN: Batch: 0.2164920942024151 Loss: 2.612434\n",
      "TRAIN: Batch: 0.21740943358462872 Loss: 0.7026629\n",
      "TRAIN: Batch: 0.21832677296684236 Loss: 4.2463\n",
      "TRAIN: Batch: 0.21924411234905597 Loss: 2.1460948\n",
      "TRAIN: Batch: 0.2201614517312696 Loss: 2.37476\n",
      "TRAIN: Batch: 0.22107879111348322 Loss: 0.6621663\n",
      "TRAIN: Batch: 0.22199613049569683 Loss: 1.4706867\n",
      "TRAIN: Batch: 0.22291346987791047 Loss: 1.3769546\n",
      "TRAIN: Batch: 0.22383080926012408 Loss: 2.550479\n",
      "TRAIN: Batch: 0.22474814864233772 Loss: 1.2699562\n",
      "TRAIN: Batch: 0.22566548802455133 Loss: 4.2143717\n",
      "TRAIN: Batch: 0.22658282740676497 Loss: 1.0784397\n",
      "TRAIN: Batch: 0.22750016678897858 Loss: 3.9007182\n",
      "TRAIN: Batch: 0.22841750617119222 Loss: 1.894826\n",
      "TRAIN: Batch: 0.22933484555340583 Loss: 1.7581999\n",
      "TRAIN: Batch: 0.23025218493561944 Loss: 2.6195316\n",
      "TRAIN: Batch: 0.23116952431783308 Loss: 2.329297\n",
      "TRAIN: Batch: 0.2320868637000467 Loss: 0.9015841\n",
      "TRAIN: Batch: 0.23300420308226033 Loss: 1.4300177\n",
      "TRAIN: Batch: 0.23392154246447394 Loss: 2.244028\n",
      "TRAIN: Batch: 0.23483888184668758 Loss: 2.4090388\n",
      "TRAIN: Batch: 0.2357562212289012 Loss: 2.7506342\n",
      "TRAIN: Batch: 0.23667356061111483 Loss: 3.2427683\n",
      "TRAIN: Batch: 0.23759089999332844 Loss: 2.7602642\n",
      "TRAIN: Batch: 0.23850823937554205 Loss: 2.315144\n",
      "TRAIN: Batch: 0.2394255787577557 Loss: 1.0970019\n",
      "TRAIN: Batch: 0.2403429181399693 Loss: 1.9281813\n",
      "TRAIN: Batch: 0.24126025752218294 Loss: 3.6185467\n",
      "TRAIN: Batch: 0.24217759690439655 Loss: 1.331301\n",
      "TRAIN: Batch: 0.2430949362866102 Loss: 1.0000136\n",
      "TRAIN: Batch: 0.2440122756688238 Loss: 5.7953672\n",
      "TRAIN: Batch: 0.2449296150510374 Loss: 1.7692757\n",
      "TRAIN: Batch: 0.24584695443325105 Loss: 1.5585374\n",
      "TRAIN: Batch: 0.24676429381546466 Loss: 2.9677033\n",
      "TRAIN: Batch: 0.2476816331976783 Loss: 1.5536139\n",
      "TRAIN: Batch: 0.2485989725798919 Loss: 5.373636\n",
      "TRAIN: Batch: 0.24951631196210555 Loss: 0.952169\n",
      "TRAIN: Batch: 0.25043365134431916 Loss: 3.6881926\n",
      "TRAIN: Batch: 0.2513509907265328 Loss: 1.4305549\n",
      "TRAIN: Batch: 0.25226833010874644 Loss: 1.0835752\n",
      "TRAIN: Batch: 0.25318566949096005 Loss: 9.637644\n",
      "TRAIN: Batch: 0.25410300887317366 Loss: 0.8642644\n",
      "TRAIN: Batch: 0.2550203482553873 Loss: 2.7395205\n",
      "TRAIN: Batch: 0.2559376876376009 Loss: 3.1536508\n",
      "TRAIN: Batch: 0.25685502701981455 Loss: 0.93453217\n",
      "TRAIN: Batch: 0.25777236640202816 Loss: 1.735099\n",
      "TRAIN: Batch: 0.2586897057842418 Loss: 2.3445492\n",
      "TRAIN: Batch: 0.2596070451664554 Loss: 1.452765\n",
      "TRAIN: Batch: 0.260524384548669 Loss: 5.8744493\n",
      "TRAIN: Batch: 0.26144172393088266 Loss: 1.346332\n",
      "TRAIN: Batch: 0.2623590633130963 Loss: 3.981245\n",
      "TRAIN: Batch: 0.2632764026953099 Loss: 1.355862\n",
      "TRAIN: Batch: 0.2641937420775235 Loss: 3.6035867\n",
      "TRAIN: Batch: 0.26511108145973716 Loss: 3.7389534\n",
      "TRAIN: Batch: 0.2660284208419508 Loss: 1.1463649\n",
      "TRAIN: Batch: 0.2669457602241644 Loss: 0.9676943\n",
      "TRAIN: Batch: 0.267863099606378 Loss: 2.6291592\n",
      "TRAIN: Batch: 0.2687804389885916 Loss: 1.8245523\n",
      "TRAIN: Batch: 0.2696977783708053 Loss: 1.5113087\n",
      "TRAIN: Batch: 0.2706151177530189 Loss: 0.73296946\n",
      "TRAIN: Batch: 0.2715324571352325 Loss: 1.3519129\n",
      "TRAIN: Batch: 0.2724497965174461 Loss: 1.6302052\n",
      "TRAIN: Batch: 0.2733671358996598 Loss: 2.0279868\n",
      "TRAIN: Batch: 0.2742844752818734 Loss: 1.417983\n",
      "TRAIN: Batch: 0.275201814664087 Loss: 1.4728949\n",
      "TRAIN: Batch: 0.2761191540463006 Loss: 2.6504154\n",
      "TRAIN: Batch: 0.2770364934285142 Loss: 8.769305\n",
      "TRAIN: Batch: 0.2779538328107279 Loss: 1.2144388\n",
      "TRAIN: Batch: 0.2788711721929415 Loss: 2.1888375\n",
      "TRAIN: Batch: 0.2797885115751551 Loss: 2.051567\n",
      "TRAIN: Batch: 0.2807058509573687 Loss: 0.751076\n",
      "TRAIN: Batch: 0.2816231903395824 Loss: 2.2240112\n",
      "TRAIN: Batch: 0.282540529721796 Loss: 2.036149\n",
      "TRAIN: Batch: 0.2834578691040096 Loss: 2.2750325\n",
      "TRAIN: Batch: 0.2843752084862232 Loss: 4.263953\n",
      "TRAIN: Batch: 0.28529254786843683 Loss: 2.026169\n",
      "TRAIN: Batch: 0.2862098872506505 Loss: 2.087052\n",
      "TRAIN: Batch: 0.2871272266328641 Loss: 1.3046591\n",
      "TRAIN: Batch: 0.2880445660150777 Loss: 0.6097702\n",
      "TRAIN: Batch: 0.28896190539729133 Loss: 1.3900763\n",
      "TRAIN: Batch: 0.289879244779505 Loss: 7.9182525\n",
      "TRAIN: Batch: 0.2907965841617186 Loss: 7.506813\n",
      "TRAIN: Batch: 0.2917139235439322 Loss: 5.8749294\n",
      "TRAIN: Batch: 0.29263126292614583 Loss: 0.8314991\n",
      "TRAIN: Batch: 0.29354860230835944 Loss: 1.3033192\n",
      "TRAIN: Batch: 0.2944659416905731 Loss: 1.4210254\n",
      "TRAIN: Batch: 0.2953832810727867 Loss: 3.0287545\n",
      "TRAIN: Batch: 0.29630062045500033 Loss: 1.4063777\n",
      "TRAIN: Batch: 0.29721795983721394 Loss: 3.0877934\n",
      "TRAIN: Batch: 0.2981352992194276 Loss: 0.6233384\n",
      "TRAIN: Batch: 0.2990526386016412 Loss: 1.81987\n",
      "TRAIN: Batch: 0.29996997798385483 Loss: 4.354436\n",
      "TRAIN: Batch: 0.30088731736606844 Loss: 1.49602\n",
      "TRAIN: Batch: 0.30180465674828205 Loss: 0.5531652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.3027219961304957 Loss: 1.0779663\n",
      "TRAIN: Batch: 0.30363933551270933 Loss: 2.0789192\n",
      "TRAIN: Batch: 0.30455667489492294 Loss: 4.104162\n",
      "TRAIN: Batch: 0.30547401427713655 Loss: 0.8296678\n",
      "TRAIN: Batch: 0.30639135365935016 Loss: 0.5676107\n",
      "TRAIN: Batch: 0.30730869304156383 Loss: 3.3624554\n",
      "TRAIN: Batch: 0.30822603242377744 Loss: 0.7905667\n",
      "TRAIN: Batch: 0.30914337180599105 Loss: 1.4036005\n",
      "TRAIN: Batch: 0.31006071118820466 Loss: 3.0073102\n",
      "TRAIN: Batch: 0.31097805057041833 Loss: 2.717187\n",
      "TRAIN: Batch: 0.31189538995263194 Loss: 1.9475062\n",
      "TRAIN: Batch: 0.31281272933484555 Loss: 1.781596\n",
      "TRAIN: Batch: 0.31373006871705916 Loss: 6.2381864\n",
      "TRAIN: Batch: 0.3146474080992728 Loss: 0.9610556\n",
      "TRAIN: Batch: 0.31556474748148644 Loss: 3.0033946\n",
      "TRAIN: Batch: 0.31648208686370005 Loss: 0.88284624\n",
      "TRAIN: Batch: 0.31739942624591366 Loss: 2.8214786\n",
      "TRAIN: Batch: 0.3183167656281273 Loss: 1.6560102\n",
      "TRAIN: Batch: 0.31923410501034094 Loss: 3.0388002\n",
      "TRAIN: Batch: 0.32015144439255455 Loss: 1.508208\n",
      "TRAIN: Batch: 0.32106878377476816 Loss: 3.5531087\n",
      "TRAIN: Batch: 0.3219861231569818 Loss: 1.3331922\n",
      "TRAIN: Batch: 0.3229034625391954 Loss: 4.3488617\n",
      "TRAIN: Batch: 0.32382080192140905 Loss: 1.1236647\n",
      "TRAIN: Batch: 0.32473814130362266 Loss: 0.6620406\n",
      "TRAIN: Batch: 0.3256554806858363 Loss: 2.390954\n",
      "TRAIN: Batch: 0.3265728200680499 Loss: 1.8558168\n",
      "TRAIN: Batch: 0.32749015945026355 Loss: 3.6757102\n",
      "TRAIN: Batch: 0.32840749883247716 Loss: 5.5014095\n",
      "TRAIN: Batch: 0.3293248382146908 Loss: 3.4784148\n",
      "TRAIN: Batch: 0.3302421775969044 Loss: 0.7217475\n",
      "TRAIN: Batch: 0.331159516979118 Loss: 0.8035437\n",
      "TRAIN: Batch: 0.33207685636133166 Loss: 2.738188\n",
      "TRAIN: Batch: 0.3329941957435453 Loss: 0.92022115\n",
      "TRAIN: Batch: 0.3339115351257589 Loss: 1.2728524\n",
      "TRAIN: Batch: 0.3348288745079725 Loss: 0.80660236\n",
      "TRAIN: Batch: 0.33574621389018616 Loss: 2.095716\n",
      "TRAIN: Batch: 0.3366635532723998 Loss: 5.47177\n",
      "TRAIN: Batch: 0.3375808926546134 Loss: 0.6346364\n",
      "TRAIN: Batch: 0.338498232036827 Loss: 4.627794\n",
      "TRAIN: Batch: 0.3394155714190406 Loss: 2.501514\n",
      "TRAIN: Batch: 0.3403329108012543 Loss: 0.8312905\n",
      "TRAIN: Batch: 0.3412502501834679 Loss: 2.8615108\n",
      "TRAIN: Batch: 0.3421675895656815 Loss: 1.8661056\n",
      "TRAIN: Batch: 0.3430849289478951 Loss: 3.0144286\n",
      "TRAIN: Batch: 0.3440022683301088 Loss: 3.3800712\n",
      "TRAIN: Batch: 0.3449196077123224 Loss: 1.0956948\n",
      "TRAIN: Batch: 0.345836947094536 Loss: 1.1223881\n",
      "TRAIN: Batch: 0.3467542864767496 Loss: 0.8798059\n",
      "TRAIN: Batch: 0.3476716258589632 Loss: 3.5293913\n",
      "TRAIN: Batch: 0.3485889652411769 Loss: 1.9268894\n",
      "TRAIN: Batch: 0.3495063046233905 Loss: 1.1824218\n",
      "TRAIN: Batch: 0.3504236440056041 Loss: 2.375392\n",
      "TRAIN: Batch: 0.3513409833878177 Loss: 0.5046123\n",
      "TRAIN: Batch: 0.35225832277003133 Loss: 3.9356174\n",
      "TRAIN: Batch: 0.353175662152245 Loss: 3.6218145\n",
      "TRAIN: Batch: 0.3540930015344586 Loss: 1.2451949\n",
      "TRAIN: Batch: 0.3550103409166722 Loss: 1.4204171\n",
      "TRAIN: Batch: 0.35592768029888583 Loss: 3.2189946\n",
      "TRAIN: Batch: 0.3568450196810995 Loss: 0.71003664\n",
      "TRAIN: Batch: 0.3577623590633131 Loss: 1.0394745\n",
      "TRAIN: Batch: 0.3586796984455267 Loss: 1.0021715\n",
      "TRAIN: Batch: 0.35959703782774033 Loss: 2.8071954\n",
      "TRAIN: Batch: 0.36051437720995394 Loss: 3.039551\n",
      "TRAIN: Batch: 0.3614317165921676 Loss: 2.461581\n",
      "TRAIN: Batch: 0.3623490559743812 Loss: 1.6350064\n",
      "TRAIN: Batch: 0.36326639535659483 Loss: 3.0398865\n",
      "TRAIN: Batch: 0.36418373473880844 Loss: 1.8700125\n",
      "TRAIN: Batch: 0.3651010741210221 Loss: 1.8072011\n",
      "TRAIN: Batch: 0.3660184135032357 Loss: 3.5010316\n",
      "TRAIN: Batch: 0.36693575288544933 Loss: 2.2532635\n",
      "TRAIN: Batch: 0.36785309226766294 Loss: 2.105408\n",
      "TRAIN: Batch: 0.36877043164987655 Loss: 3.915351\n",
      "TRAIN: Batch: 0.3696877710320902 Loss: 2.505538\n",
      "TRAIN: Batch: 0.37060511041430383 Loss: 2.4390054\n",
      "TRAIN: Batch: 0.37152244979651744 Loss: 4.3152504\n",
      "TRAIN: Batch: 0.37243978917873105 Loss: 3.5952344\n",
      "TRAIN: Batch: 0.3733571285609447 Loss: 2.6639128\n",
      "TRAIN: Batch: 0.37427446794315833 Loss: 1.1174232\n",
      "TRAIN: Batch: 0.37519180732537194 Loss: 2.334758\n",
      "TRAIN: Batch: 0.37610914670758555 Loss: 0.8009023\n",
      "TRAIN: Batch: 0.37702648608979916 Loss: 0.9707314\n",
      "TRAIN: Batch: 0.37794382547201283 Loss: 2.0275846\n",
      "TRAIN: Batch: 0.37886116485422644 Loss: 2.978933\n",
      "TRAIN: Batch: 0.37977850423644005 Loss: 0.99048305\n",
      "TRAIN: Batch: 0.38069584361865366 Loss: 1.7437617\n",
      "TRAIN: Batch: 0.38161318300086733 Loss: 4.3271303\n",
      "TRAIN: Batch: 0.38253052238308094 Loss: 3.51132\n",
      "TRAIN: Batch: 0.38344786176529455 Loss: 2.7243056\n",
      "TRAIN: Batch: 0.38436520114750816 Loss: 2.3188727\n",
      "TRAIN: Batch: 0.3852825405297218 Loss: 0.92997575\n",
      "TRAIN: Batch: 0.38619987991193544 Loss: 3.8982584\n",
      "TRAIN: Batch: 0.38711721929414905 Loss: 2.1084695\n",
      "TRAIN: Batch: 0.38803455867636266 Loss: 3.5744462\n",
      "TRAIN: Batch: 0.3889518980585763 Loss: 2.0479138\n",
      "TRAIN: Batch: 0.38986923744078994 Loss: 2.0652127\n",
      "TRAIN: Batch: 0.39078657682300355 Loss: 0.9399357\n",
      "TRAIN: Batch: 0.39170391620521716 Loss: 0.65252495\n",
      "TRAIN: Batch: 0.3926212555874308 Loss: 3.9492993\n",
      "TRAIN: Batch: 0.3935385949696444 Loss: 2.2074523\n",
      "TRAIN: Batch: 0.39445593435185805 Loss: 2.3697407\n",
      "TRAIN: Batch: 0.39537327373407166 Loss: 1.2482187\n",
      "TRAIN: Batch: 0.3962906131162853 Loss: 1.4008772\n",
      "TRAIN: Batch: 0.3972079524984989 Loss: 3.131845\n",
      "TRAIN: Batch: 0.3981252918807125 Loss: 1.6137383\n",
      "TRAIN: Batch: 0.39904263126292616 Loss: 2.7717474\n",
      "TRAIN: Batch: 0.3999599706451398 Loss: 2.072058\n",
      "TRAIN: Batch: 0.4008773100273534 Loss: 2.1196313\n",
      "TRAIN: Batch: 0.401794649409567 Loss: 0.7088274\n",
      "TRAIN: Batch: 0.40271198879178066 Loss: 5.0260878\n",
      "TRAIN: Batch: 0.4036293281739943 Loss: 1.115847\n",
      "TRAIN: Batch: 0.4045466675562079 Loss: 6.381521\n",
      "TRAIN: Batch: 0.4054640069384215 Loss: 0.6678961\n",
      "TRAIN: Batch: 0.4063813463206351 Loss: 1.6060946\n",
      "TRAIN: Batch: 0.4072986857028488 Loss: 4.1197095\n",
      "TRAIN: Batch: 0.4082160250850624 Loss: 4.4560432\n",
      "TRAIN: Batch: 0.409133364467276 Loss: 6.234337\n",
      "TRAIN: Batch: 0.4100507038494896 Loss: 6.93002\n",
      "TRAIN: Batch: 0.4109680432317033 Loss: 0.5843849\n",
      "TRAIN: Batch: 0.4118853826139169 Loss: 1.587973\n",
      "TRAIN: Batch: 0.4128027219961305 Loss: 1.436215\n",
      "TRAIN: Batch: 0.4137200613783441 Loss: 4.810912\n",
      "TRAIN: Batch: 0.4146374007605577 Loss: 0.7690961\n",
      "TRAIN: Batch: 0.4155547401427714 Loss: 2.0407925\n",
      "TRAIN: Batch: 0.416472079524985 Loss: 0.7550159\n",
      "TRAIN: Batch: 0.4173894189071986 Loss: 2.0357459\n",
      "TRAIN: Batch: 0.4183067582894122 Loss: 4.4064946\n",
      "TRAIN: Batch: 0.4192240976716259 Loss: 3.1268578\n",
      "TRAIN: Batch: 0.4201414370538395 Loss: 5.1281304\n",
      "TRAIN: Batch: 0.4210587764360531 Loss: 3.831173\n",
      "TRAIN: Batch: 0.4219761158182667 Loss: 0.7771259\n",
      "TRAIN: Batch: 0.42289345520048033 Loss: 2.5614004\n",
      "TRAIN: Batch: 0.423810794582694 Loss: 1.7845051\n",
      "TRAIN: Batch: 0.4247281339649076 Loss: 0.5739518\n",
      "TRAIN: Batch: 0.4256454733471212 Loss: 8.723846\n",
      "TRAIN: Batch: 0.42656281272933483 Loss: 2.8726618\n",
      "TRAIN: Batch: 0.4274801521115485 Loss: 1.5236504\n",
      "TRAIN: Batch: 0.4283974914937621 Loss: 4.8398\n",
      "TRAIN: Batch: 0.4293148308759757 Loss: 1.1164708\n",
      "TRAIN: Batch: 0.43023217025818933 Loss: 2.6483996\n",
      "TRAIN: Batch: 0.43114950964040294 Loss: 2.0606775\n",
      "TRAIN: Batch: 0.4320668490226166 Loss: 0.8462264\n",
      "TRAIN: Batch: 0.4329841884048302 Loss: 1.5527418\n",
      "TRAIN: Batch: 0.43390152778704383 Loss: 3.9229457\n",
      "TRAIN: Batch: 0.43481886716925744 Loss: 1.9892731\n",
      "TRAIN: Batch: 0.4357362065514711 Loss: 1.8118014\n",
      "TRAIN: Batch: 0.4366535459336847 Loss: 1.9395466\n",
      "TRAIN: Batch: 0.43757088531589833 Loss: 3.5878396\n",
      "TRAIN: Batch: 0.43848822469811194 Loss: 2.2126207\n",
      "TRAIN: Batch: 0.43940556408032555 Loss: 1.5997952\n",
      "TRAIN: Batch: 0.4403229034625392 Loss: 6.160331\n",
      "TRAIN: Batch: 0.44124024284475283 Loss: 2.4961598\n",
      "TRAIN: Batch: 0.44215758222696644 Loss: 7.3605165\n",
      "TRAIN: Batch: 0.44307492160918005 Loss: 1.7872258\n",
      "TRAIN: Batch: 0.44399226099139366 Loss: 1.0138073\n",
      "TRAIN: Batch: 0.44490960037360733 Loss: 1.7640681\n",
      "TRAIN: Batch: 0.44582693975582094 Loss: 1.4304713\n",
      "TRAIN: Batch: 0.44674427913803455 Loss: 3.7400024\n",
      "TRAIN: Batch: 0.44766161852024816 Loss: 3.655241\n",
      "TRAIN: Batch: 0.44857895790246183 Loss: 3.6368265\n",
      "TRAIN: Batch: 0.44949629728467544 Loss: 0.65304834\n",
      "TRAIN: Batch: 0.45041363666688905 Loss: 2.0996268\n",
      "TRAIN: Batch: 0.45133097604910266 Loss: 1.2506096\n",
      "TRAIN: Batch: 0.4522483154313163 Loss: 1.3925071\n",
      "TRAIN: Batch: 0.45316565481352994 Loss: 1.820837\n",
      "TRAIN: Batch: 0.45408299419574355 Loss: 1.5796866\n",
      "TRAIN: Batch: 0.45500033357795716 Loss: 0.66945356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.4559176729601708 Loss: 0.84225786\n",
      "TRAIN: Batch: 0.45683501234238444 Loss: 1.2339844\n",
      "TRAIN: Batch: 0.45775235172459805 Loss: 2.48161\n",
      "TRAIN: Batch: 0.45866969110681166 Loss: 3.690516\n",
      "TRAIN: Batch: 0.45958703048902527 Loss: 3.7826457\n",
      "TRAIN: Batch: 0.4605043698712389 Loss: 1.7445323\n",
      "TRAIN: Batch: 0.46142170925345255 Loss: 4.9378247\n",
      "TRAIN: Batch: 0.46233904863566616 Loss: 3.2204957\n",
      "TRAIN: Batch: 0.46325638801787977 Loss: 1.4040194\n",
      "TRAIN: Batch: 0.4641737274000934 Loss: 2.9073057\n",
      "TRAIN: Batch: 0.46509106678230705 Loss: 3.638169\n",
      "TRAIN: Batch: 0.46600840616452066 Loss: 2.6800268\n",
      "TRAIN: Batch: 0.46692574554673427 Loss: 2.3541946\n",
      "TRAIN: Batch: 0.4678430849289479 Loss: 1.355185\n",
      "TRAIN: Batch: 0.4687604243111615 Loss: 0.76311076\n",
      "TRAIN: Batch: 0.46967776369337516 Loss: 2.269279\n",
      "TRAIN: Batch: 0.47059510307558877 Loss: 3.6726248\n",
      "TRAIN: Batch: 0.4715124424578024 Loss: 2.8591976\n",
      "TRAIN: Batch: 0.472429781840016 Loss: 1.597491\n",
      "TRAIN: Batch: 0.47334712122222966 Loss: 0.63284147\n",
      "TRAIN: Batch: 0.47426446060444327 Loss: 0.7975546\n",
      "TRAIN: Batch: 0.4751817999866569 Loss: 1.3341699\n",
      "TRAIN: Batch: 0.4760991393688705 Loss: 2.001723\n",
      "TRAIN: Batch: 0.4770164787510841 Loss: 1.5994194\n",
      "TRAIN: Batch: 0.47793381813329777 Loss: 1.117475\n",
      "TRAIN: Batch: 0.4788511575155114 Loss: 2.6211605\n",
      "TRAIN: Batch: 0.479768496897725 Loss: 0.50794077\n",
      "TRAIN: Batch: 0.4806858362799386 Loss: 3.30098\n",
      "TRAIN: Batch: 0.48160317566215227 Loss: 4.6880226\n",
      "TRAIN: Batch: 0.4825205150443659 Loss: 2.183451\n",
      "TRAIN: Batch: 0.4834378544265795 Loss: 1.7460835\n",
      "TRAIN: Batch: 0.4843551938087931 Loss: 3.2517555\n",
      "TRAIN: Batch: 0.4852725331910067 Loss: 5.8727217\n",
      "TRAIN: Batch: 0.4861898725732204 Loss: 2.026736\n",
      "TRAIN: Batch: 0.487107211955434 Loss: 2.6557028\n",
      "TRAIN: Batch: 0.4880245513376476 Loss: 0.480014\n",
      "TRAIN: Batch: 0.4889418907198612 Loss: 6.5617414\n",
      "TRAIN: Batch: 0.4898592301020748 Loss: 1.3080004\n",
      "TRAIN: Batch: 0.4907765694842885 Loss: 3.407792\n",
      "TRAIN: Batch: 0.4916939088665021 Loss: 0.90525424\n",
      "TRAIN: Batch: 0.4926112482487157 Loss: 1.2353722\n",
      "TRAIN: Batch: 0.4935285876309293 Loss: 1.1484752\n",
      "TRAIN: Batch: 0.494445927013143 Loss: 2.2154558\n",
      "TRAIN: Batch: 0.4953632663953566 Loss: 0.8394079\n",
      "TRAIN: Batch: 0.4962806057775702 Loss: 0.9708218\n",
      "TRAIN: Batch: 0.4971979451597838 Loss: 0.988045\n",
      "TRAIN: Batch: 0.49811528454199744 Loss: 0.7235273\n",
      "TRAIN: Batch: 0.4990326239242111 Loss: 2.3329666\n",
      "TRAIN: Batch: 0.4999499633064247 Loss: 3.1457095\n",
      "TRAIN: Batch: 0.5008673026886383 Loss: 0.6052001\n",
      "TRAIN: Batch: 0.5017846420708519 Loss: 4.179548\n",
      "TRAIN: Batch: 0.5027019814530655 Loss: 1.667299\n",
      "TRAIN: Batch: 0.5036193208352792 Loss: 1.6630855\n",
      "TRAIN: Batch: 0.5045366602174929 Loss: 1.419482\n",
      "TRAIN: Batch: 0.5054539995997065 Loss: 6.8023543\n",
      "TRAIN: Batch: 0.5063713389819201 Loss: 1.6695276\n",
      "TRAIN: Batch: 0.5072886783641337 Loss: 1.564371\n",
      "TRAIN: Batch: 0.5082060177463473 Loss: 4.1611576\n",
      "TRAIN: Batch: 0.5091233571285609 Loss: 0.5739659\n",
      "TRAIN: Batch: 0.5100406965107745 Loss: 3.6237452\n",
      "TRAIN: Batch: 0.5109580358929882 Loss: 2.275078\n",
      "TRAIN: Batch: 0.5118753752752018 Loss: 3.5057864\n",
      "TRAIN: Batch: 0.5127927146574154 Loss: 4.1648793\n",
      "TRAIN: Batch: 0.5137100540396291 Loss: 1.2702174\n",
      "TRAIN: Batch: 0.5146273934218427 Loss: 1.8304725\n",
      "TRAIN: Batch: 0.5155447328040563 Loss: 1.21209\n",
      "TRAIN: Batch: 0.5164620721862699 Loss: 0.68899196\n",
      "TRAIN: Batch: 0.5173794115684835 Loss: 2.8502588\n",
      "TRAIN: Batch: 0.5182967509506972 Loss: 1.3088536\n",
      "TRAIN: Batch: 0.5192140903329108 Loss: 2.5990083\n",
      "TRAIN: Batch: 0.5201314297151244 Loss: 1.8367202\n",
      "TRAIN: Batch: 0.521048769097338 Loss: 1.2385628\n",
      "TRAIN: Batch: 0.5219661084795517 Loss: 9.311699\n",
      "TRAIN: Batch: 0.5228834478617653 Loss: 2.405783\n",
      "TRAIN: Batch: 0.5238007872439789 Loss: 0.7041353\n",
      "TRAIN: Batch: 0.5247181266261925 Loss: 2.0811114\n",
      "TRAIN: Batch: 0.5256354660084062 Loss: 4.3340116\n",
      "TRAIN: Batch: 0.5265528053906198 Loss: 0.95581365\n",
      "TRAIN: Batch: 0.5274701447728334 Loss: 1.3154949\n",
      "TRAIN: Batch: 0.528387484155047 Loss: 5.025022\n",
      "TRAIN: Batch: 0.5293048235372606 Loss: 1.9201444\n",
      "TRAIN: Batch: 0.5302221629194743 Loss: 3.044932\n",
      "TRAIN: Batch: 0.5311395023016879 Loss: 1.4845371\n",
      "TRAIN: Batch: 0.5320568416839015 Loss: 0.8465233\n",
      "TRAIN: Batch: 0.5329741810661152 Loss: 1.8203397\n",
      "TRAIN: Batch: 0.5338915204483288 Loss: 2.2914379\n",
      "TRAIN: Batch: 0.5348088598305424 Loss: 1.1379229\n",
      "TRAIN: Batch: 0.535726199212756 Loss: 1.4616988\n",
      "TRAIN: Batch: 0.5366435385949696 Loss: 3.3988144\n",
      "TRAIN: Batch: 0.5375608779771832 Loss: 0.9377824\n",
      "TRAIN: Batch: 0.5384782173593969 Loss: 4.8029795\n",
      "TRAIN: Batch: 0.5393955567416105 Loss: 1.9244694\n",
      "TRAIN: Batch: 0.5403128961238242 Loss: 0.83615017\n",
      "TRAIN: Batch: 0.5412302355060378 Loss: 1.4542384\n",
      "TRAIN: Batch: 0.5421475748882514 Loss: 0.6489156\n",
      "TRAIN: Batch: 0.543064914270465 Loss: 1.9113305\n",
      "TRAIN: Batch: 0.5439822536526786 Loss: 5.0132346\n",
      "TRAIN: Batch: 0.5448995930348922 Loss: 2.2401962\n",
      "TRAIN: Batch: 0.5458169324171058 Loss: 3.5682716\n",
      "TRAIN: Batch: 0.5467342717993195 Loss: 2.2519984\n",
      "TRAIN: Batch: 0.5476516111815332 Loss: 2.596098\n",
      "TRAIN: Batch: 0.5485689505637468 Loss: 1.2177336\n",
      "TRAIN: Batch: 0.5494862899459604 Loss: 2.7221441\n",
      "TRAIN: Batch: 0.550403629328174 Loss: 3.8350902\n",
      "TRAIN: Batch: 0.5513209687103876 Loss: 1.5845518\n",
      "TRAIN: Batch: 0.5522383080926012 Loss: 1.1370356\n",
      "TRAIN: Batch: 0.5531556474748148 Loss: 0.8676232\n",
      "TRAIN: Batch: 0.5540729868570284 Loss: 3.4408522\n",
      "TRAIN: Batch: 0.5549903262392422 Loss: 1.5058117\n",
      "TRAIN: Batch: 0.5559076656214558 Loss: 1.7278684\n",
      "TRAIN: Batch: 0.5568250050036694 Loss: 3.2328753\n",
      "TRAIN: Batch: 0.557742344385883 Loss: 0.5558022\n",
      "TRAIN: Batch: 0.5586596837680966 Loss: 2.8700676\n",
      "TRAIN: Batch: 0.5595770231503102 Loss: 0.5152843\n",
      "TRAIN: Batch: 0.5604943625325238 Loss: 4.688933\n",
      "TRAIN: Batch: 0.5614117019147374 Loss: 7.387472\n",
      "TRAIN: Batch: 0.562329041296951 Loss: 3.8136458\n",
      "TRAIN: Batch: 0.5632463806791648 Loss: 2.9782295\n",
      "TRAIN: Batch: 0.5641637200613784 Loss: 2.4420855\n",
      "TRAIN: Batch: 0.565081059443592 Loss: 4.210965\n",
      "TRAIN: Batch: 0.5659983988258056 Loss: 1.0927072\n",
      "TRAIN: Batch: 0.5669157382080192 Loss: 0.9109822\n",
      "TRAIN: Batch: 0.5678330775902328 Loss: 3.9760358\n",
      "TRAIN: Batch: 0.5687504169724464 Loss: 1.6167185\n",
      "TRAIN: Batch: 0.56966775635466 Loss: 1.6164379\n",
      "TRAIN: Batch: 0.5705850957368737 Loss: 3.1277945\n",
      "TRAIN: Batch: 0.5715024351190874 Loss: 2.013206\n",
      "TRAIN: Batch: 0.572419774501301 Loss: 4.1663923\n",
      "TRAIN: Batch: 0.5733371138835146 Loss: 11.654442\n",
      "TRAIN: Batch: 0.5742544532657282 Loss: 1.239552\n",
      "TRAIN: Batch: 0.5751717926479418 Loss: 1.8312504\n",
      "TRAIN: Batch: 0.5760891320301554 Loss: 0.77704775\n",
      "TRAIN: Batch: 0.577006471412369 Loss: 2.102469\n",
      "TRAIN: Batch: 0.5779238107945827 Loss: 6.538808\n",
      "TRAIN: Batch: 0.5788411501767963 Loss: 1.6955042\n",
      "TRAIN: Batch: 0.57975848955901 Loss: 3.2927487\n",
      "TRAIN: Batch: 0.5806758289412236 Loss: 2.3308332\n",
      "TRAIN: Batch: 0.5815931683234372 Loss: 1.8171571\n",
      "TRAIN: Batch: 0.5825105077056508 Loss: 1.2714843\n",
      "TRAIN: Batch: 0.5834278470878644 Loss: 0.6501778\n",
      "TRAIN: Batch: 0.584345186470078 Loss: 1.7710319\n",
      "TRAIN: Batch: 0.5852625258522917 Loss: 1.2981931\n",
      "TRAIN: Batch: 0.5861798652345053 Loss: 2.5181365\n",
      "TRAIN: Batch: 0.5870972046167189 Loss: 0.60350406\n",
      "TRAIN: Batch: 0.5880145439989326 Loss: 1.1341922\n",
      "TRAIN: Batch: 0.5889318833811462 Loss: 1.8543494\n",
      "TRAIN: Batch: 0.5898492227633598 Loss: 4.790556\n",
      "TRAIN: Batch: 0.5907665621455734 Loss: 0.6332185\n",
      "TRAIN: Batch: 0.591683901527787 Loss: 1.1711552\n",
      "TRAIN: Batch: 0.5926012409100007 Loss: 4.0005774\n",
      "TRAIN: Batch: 0.5935185802922143 Loss: 0.7463729\n",
      "TRAIN: Batch: 0.5944359196744279 Loss: 1.6845421\n",
      "TRAIN: Batch: 0.5953532590566415 Loss: 1.4363123\n",
      "TRAIN: Batch: 0.5962705984388552 Loss: 1.2950709\n",
      "TRAIN: Batch: 0.5971879378210688 Loss: 3.6718705\n",
      "TRAIN: Batch: 0.5981052772032824 Loss: 2.6169925\n",
      "TRAIN: Batch: 0.599022616585496 Loss: 2.076766\n",
      "TRAIN: Batch: 0.5999399559677097 Loss: 1.5912151\n",
      "TRAIN: Batch: 0.6008572953499233 Loss: 2.0623498\n",
      "TRAIN: Batch: 0.6017746347321369 Loss: 3.070847\n",
      "TRAIN: Batch: 0.6026919741143505 Loss: 2.1479344\n",
      "TRAIN: Batch: 0.6036093134965641 Loss: 1.3921096\n",
      "TRAIN: Batch: 0.6045266528787777 Loss: 2.4163961\n",
      "TRAIN: Batch: 0.6054439922609914 Loss: 1.946198\n",
      "TRAIN: Batch: 0.606361331643205 Loss: 1.36605\n",
      "TRAIN: Batch: 0.6072786710254187 Loss: 3.9609504\n",
      "TRAIN: Batch: 0.6081960104076323 Loss: 0.99925196\n",
      "TRAIN: Batch: 0.6091133497898459 Loss: 1.293041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.6100306891720595 Loss: 0.7006029\n",
      "TRAIN: Batch: 0.6109480285542731 Loss: 0.77337676\n",
      "TRAIN: Batch: 0.6118653679364867 Loss: 3.9292998\n",
      "TRAIN: Batch: 0.6127827073187003 Loss: 1.1111407\n",
      "TRAIN: Batch: 0.613700046700914 Loss: 2.1647778\n",
      "TRAIN: Batch: 0.6146173860831277 Loss: 1.7163508\n",
      "TRAIN: Batch: 0.6155347254653413 Loss: 3.771053\n",
      "TRAIN: Batch: 0.6164520648475549 Loss: 1.6795301\n",
      "TRAIN: Batch: 0.6173694042297685 Loss: 2.9285073\n",
      "TRAIN: Batch: 0.6182867436119821 Loss: 1.1680918\n",
      "TRAIN: Batch: 0.6192040829941957 Loss: 1.1564356\n",
      "TRAIN: Batch: 0.6201214223764093 Loss: 2.2552886\n",
      "TRAIN: Batch: 0.6210387617586229 Loss: 3.2271245\n",
      "TRAIN: Batch: 0.6219561011408367 Loss: 1.9864593\n",
      "TRAIN: Batch: 0.6228734405230503 Loss: 2.3465047\n",
      "TRAIN: Batch: 0.6237907799052639 Loss: 1.1092825\n",
      "TRAIN: Batch: 0.6247081192874775 Loss: 1.3267667\n",
      "TRAIN: Batch: 0.6256254586696911 Loss: 0.9587835\n",
      "TRAIN: Batch: 0.6265427980519047 Loss: 1.8047538\n",
      "TRAIN: Batch: 0.6274601374341183 Loss: 0.6654033\n",
      "TRAIN: Batch: 0.6283774768163319 Loss: 1.8723391\n",
      "TRAIN: Batch: 0.6292948161985455 Loss: 2.7753057\n",
      "TRAIN: Batch: 0.6302121555807593 Loss: 3.5781791\n",
      "TRAIN: Batch: 0.6311294949629729 Loss: 3.1142485\n",
      "TRAIN: Batch: 0.6320468343451865 Loss: 1.2646118\n",
      "TRAIN: Batch: 0.6329641737274001 Loss: 2.4402978\n",
      "TRAIN: Batch: 0.6338815131096137 Loss: 1.3470082\n",
      "TRAIN: Batch: 0.6347988524918273 Loss: 2.1104536\n",
      "TRAIN: Batch: 0.6357161918740409 Loss: 1.0567638\n",
      "TRAIN: Batch: 0.6366335312562545 Loss: 2.3990974\n",
      "TRAIN: Batch: 0.6375508706384682 Loss: 1.4996226\n",
      "TRAIN: Batch: 0.6384682100206819 Loss: 1.9031551\n",
      "TRAIN: Batch: 0.6393855494028955 Loss: 2.1431265\n",
      "TRAIN: Batch: 0.6403028887851091 Loss: 4.5716786\n",
      "TRAIN: Batch: 0.6412202281673227 Loss: 1.9193349\n",
      "TRAIN: Batch: 0.6421375675495363 Loss: 2.4538617\n",
      "TRAIN: Batch: 0.6430549069317499 Loss: 1.6090093\n",
      "TRAIN: Batch: 0.6439722463139635 Loss: 1.5077727\n",
      "TRAIN: Batch: 0.6448895856961772 Loss: 2.2124307\n",
      "TRAIN: Batch: 0.6458069250783908 Loss: 3.24606\n",
      "TRAIN: Batch: 0.6467242644606045 Loss: 3.5138822\n",
      "TRAIN: Batch: 0.6476416038428181 Loss: 0.50080323\n",
      "TRAIN: Batch: 0.6485589432250317 Loss: 1.6117082\n",
      "TRAIN: Batch: 0.6494762826072453 Loss: 2.263039\n",
      "TRAIN: Batch: 0.6503936219894589 Loss: 5.515255\n",
      "TRAIN: Batch: 0.6513109613716725 Loss: 1.2022269\n",
      "TRAIN: Batch: 0.6522283007538862 Loss: 1.6330316\n",
      "TRAIN: Batch: 0.6531456401360998 Loss: 1.3175917\n",
      "TRAIN: Batch: 0.6540629795183134 Loss: 4.72236\n",
      "TRAIN: Batch: 0.6549803189005271 Loss: 1.7835202\n",
      "TRAIN: Batch: 0.6558976582827407 Loss: 1.8018701\n",
      "TRAIN: Batch: 0.6568149976649543 Loss: 3.9449525\n",
      "TRAIN: Batch: 0.6577323370471679 Loss: 0.85023874\n",
      "TRAIN: Batch: 0.6586496764293815 Loss: 0.8281978\n",
      "TRAIN: Batch: 0.6595670158115952 Loss: 2.3499072\n",
      "TRAIN: Batch: 0.6604843551938088 Loss: 3.3059282\n",
      "TRAIN: Batch: 0.6614016945760224 Loss: 0.583464\n",
      "TRAIN: Batch: 0.662319033958236 Loss: 4.715868\n",
      "TRAIN: Batch: 0.6632363733404497 Loss: 1.9562353\n",
      "TRAIN: Batch: 0.6641537127226633 Loss: 6.5646963\n",
      "TRAIN: Batch: 0.6650710521048769 Loss: 11.937241\n",
      "TRAIN: Batch: 0.6659883914870905 Loss: 1.9536138\n",
      "TRAIN: Batch: 0.6669057308693042 Loss: 7.105399\n",
      "TRAIN: Batch: 0.6678230702515178 Loss: 1.1972394\n",
      "TRAIN: Batch: 0.6687404096337314 Loss: 0.61396265\n",
      "TRAIN: Batch: 0.669657749015945 Loss: 2.994563\n",
      "TRAIN: Batch: 0.6705750883981586 Loss: 1.9888943\n",
      "TRAIN: Batch: 0.6714924277803723 Loss: 4.6777616\n",
      "TRAIN: Batch: 0.6724097671625859 Loss: 4.287054\n",
      "TRAIN: Batch: 0.6733271065447995 Loss: 0.8132397\n",
      "TRAIN: Batch: 0.6742444459270132 Loss: 2.108293\n",
      "TRAIN: Batch: 0.6751617853092268 Loss: 1.4789565\n",
      "TRAIN: Batch: 0.6760791246914404 Loss: 2.9266117\n",
      "TRAIN: Batch: 0.676996464073654 Loss: 2.1493628\n",
      "TRAIN: Batch: 0.6779138034558676 Loss: 2.08972\n",
      "TRAIN: Batch: 0.6788311428380812 Loss: 0.6702158\n",
      "TRAIN: Batch: 0.6797484822202949 Loss: 5.395271\n",
      "TRAIN: Batch: 0.6806658216025085 Loss: 0.88519585\n",
      "TRAIN: Batch: 0.6815831609847222 Loss: 3.6730247\n",
      "TRAIN: Batch: 0.6825005003669358 Loss: 0.9322982\n",
      "TRAIN: Batch: 0.6834178397491494 Loss: 2.0418139\n",
      "TRAIN: Batch: 0.684335179131363 Loss: 1.7465546\n",
      "TRAIN: Batch: 0.6852525185135766 Loss: 2.3418534\n",
      "TRAIN: Batch: 0.6861698578957902 Loss: 1.1409185\n",
      "TRAIN: Batch: 0.6870871972780038 Loss: 3.461168\n",
      "TRAIN: Batch: 0.6880045366602175 Loss: 3.8300035\n",
      "TRAIN: Batch: 0.6889218760424312 Loss: 0.929575\n",
      "TRAIN: Batch: 0.6898392154246448 Loss: 2.5513587\n",
      "TRAIN: Batch: 0.6907565548068584 Loss: 3.262358\n",
      "TRAIN: Batch: 0.691673894189072 Loss: 0.7702894\n",
      "TRAIN: Batch: 0.6925912335712856 Loss: 4.658191\n",
      "TRAIN: Batch: 0.6935085729534992 Loss: 1.5182078\n",
      "TRAIN: Batch: 0.6944259123357128 Loss: 1.1421082\n",
      "TRAIN: Batch: 0.6953432517179264 Loss: 1.7020738\n",
      "TRAIN: Batch: 0.6962605911001402 Loss: 3.3508925\n",
      "TRAIN: Batch: 0.6971779304823538 Loss: 0.9448534\n",
      "TRAIN: Batch: 0.6980952698645674 Loss: 1.044698\n",
      "TRAIN: Batch: 0.699012609246781 Loss: 1.6851172\n",
      "TRAIN: Batch: 0.6999299486289946 Loss: 0.9750382\n",
      "TRAIN: Batch: 0.7008472880112082 Loss: 2.7225187\n",
      "TRAIN: Batch: 0.7017646273934218 Loss: 2.851951\n",
      "TRAIN: Batch: 0.7026819667756354 Loss: 1.4987847\n",
      "TRAIN: Batch: 0.703599306157849 Loss: 2.0762587\n",
      "TRAIN: Batch: 0.7045166455400627 Loss: 0.6040554\n",
      "TRAIN: Batch: 0.7054339849222764 Loss: 2.522518\n",
      "TRAIN: Batch: 0.70635132430449 Loss: 2.0166233\n",
      "TRAIN: Batch: 0.7072686636867036 Loss: 0.79335296\n",
      "TRAIN: Batch: 0.7081860030689172 Loss: 0.5638693\n",
      "TRAIN: Batch: 0.7091033424511308 Loss: 4.5716834\n",
      "TRAIN: Batch: 0.7100206818333444 Loss: 2.4682837\n",
      "TRAIN: Batch: 0.710938021215558 Loss: 2.4690697\n",
      "TRAIN: Batch: 0.7118553605977717 Loss: 0.7324129\n",
      "TRAIN: Batch: 0.7127726999799853 Loss: 1.7394555\n",
      "TRAIN: Batch: 0.713690039362199 Loss: 4.7063985\n",
      "TRAIN: Batch: 0.7146073787444126 Loss: 2.3981965\n",
      "TRAIN: Batch: 0.7155247181266262 Loss: 2.6705391\n",
      "TRAIN: Batch: 0.7164420575088398 Loss: 1.3428383\n",
      "TRAIN: Batch: 0.7173593968910534 Loss: 0.9862121\n",
      "TRAIN: Batch: 0.718276736273267 Loss: 2.3245533\n",
      "TRAIN: Batch: 0.7191940756554807 Loss: 1.4901348\n",
      "TRAIN: Batch: 0.7201114150376943 Loss: 1.857622\n",
      "TRAIN: Batch: 0.7210287544199079 Loss: 7.2585235\n",
      "TRAIN: Batch: 0.7219460938021216 Loss: 1.2264493\n",
      "TRAIN: Batch: 0.7228634331843352 Loss: 2.8794253\n",
      "TRAIN: Batch: 0.7237807725665488 Loss: 3.4224348\n",
      "TRAIN: Batch: 0.7246981119487624 Loss: 0.9691434\n",
      "TRAIN: Batch: 0.725615451330976 Loss: 2.1523492\n",
      "TRAIN: Batch: 0.7265327907131897 Loss: 2.889914\n",
      "TRAIN: Batch: 0.7274501300954033 Loss: 4.048362\n",
      "TRAIN: Batch: 0.7283674694776169 Loss: 1.3920072\n",
      "TRAIN: Batch: 0.7292848088598305 Loss: 1.2030278\n",
      "TRAIN: Batch: 0.7302021482420442 Loss: 2.4470475\n",
      "TRAIN: Batch: 0.7311194876242578 Loss: 1.5690966\n",
      "TRAIN: Batch: 0.7320368270064714 Loss: 2.178214\n",
      "TRAIN: Batch: 0.732954166388685 Loss: 2.513522\n",
      "TRAIN: Batch: 0.7338715057708987 Loss: 3.1016116\n",
      "TRAIN: Batch: 0.7347888451531123 Loss: 3.2461994\n",
      "TRAIN: Batch: 0.7357061845353259 Loss: 0.9128419\n",
      "TRAIN: Batch: 0.7366235239175395 Loss: 0.6193724\n",
      "TRAIN: Batch: 0.7375408632997531 Loss: 1.1517432\n",
      "TRAIN: Batch: 0.7384582026819668 Loss: 1.7163967\n",
      "TRAIN: Batch: 0.7393755420641804 Loss: 2.4910424\n",
      "TRAIN: Batch: 0.740292881446394 Loss: 3.4600422\n",
      "TRAIN: Batch: 0.7412102208286077 Loss: 1.0334542\n",
      "TRAIN: Batch: 0.7421275602108213 Loss: 0.9929804\n",
      "TRAIN: Batch: 0.7430448995930349 Loss: 0.74076277\n",
      "TRAIN: Batch: 0.7439622389752485 Loss: 4.0927362\n",
      "TRAIN: Batch: 0.7448795783574621 Loss: 3.0149112\n",
      "TRAIN: Batch: 0.7457969177396757 Loss: 0.6912407\n",
      "TRAIN: Batch: 0.7467142571218894 Loss: 1.0369606\n",
      "TRAIN: Batch: 0.747631596504103 Loss: 2.3817117\n",
      "TRAIN: Batch: 0.7485489358863167 Loss: 1.1419761\n",
      "TRAIN: Batch: 0.7494662752685303 Loss: 1.1149287\n",
      "TRAIN: Batch: 0.7503836146507439 Loss: 4.3855677\n",
      "TRAIN: Batch: 0.7513009540329575 Loss: 0.9503883\n",
      "TRAIN: Batch: 0.7522182934151711 Loss: 0.9428795\n",
      "TRAIN: Batch: 0.7531356327973847 Loss: 2.1657076\n",
      "TRAIN: Batch: 0.7540529721795983 Loss: 2.0635707\n",
      "TRAIN: Batch: 0.754970311561812 Loss: inf\n",
      "TRAIN: Batch: 0.7558876509440257 Loss: 1.8880979\n",
      "TRAIN: Batch: 0.7568049903262393 Loss: 0.7519009\n",
      "TRAIN: Batch: 0.7577223297084529 Loss: 1.0268248\n",
      "TRAIN: Batch: 0.7586396690906665 Loss: 1.0602322\n",
      "TRAIN: Batch: 0.7595570084728801 Loss: 3.106104\n",
      "TRAIN: Batch: 0.7604743478550937 Loss: 0.6433239\n",
      "TRAIN: Batch: 0.7613916872373073 Loss: 1.8500081\n",
      "TRAIN: Batch: 0.7623090266195209 Loss: 1.2990303\n",
      "TRAIN: Batch: 0.7632263660017347 Loss: 1.6323618\n",
      "TRAIN: Batch: 0.7641437053839483 Loss: 1.1058023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.7650610447661619 Loss: 2.007506\n",
      "TRAIN: Batch: 0.7659783841483755 Loss: 0.7103114\n",
      "TRAIN: Batch: 0.7668957235305891 Loss: 0.6461123\n",
      "TRAIN: Batch: 0.7678130629128027 Loss: 1.7286605\n",
      "TRAIN: Batch: 0.7687304022950163 Loss: 0.6896874\n",
      "TRAIN: Batch: 0.7696477416772299 Loss: 1.9720744\n",
      "TRAIN: Batch: 0.7705650810594435 Loss: 2.280753\n",
      "TRAIN: Batch: 0.7714824204416573 Loss: 0.7655201\n",
      "TRAIN: Batch: 0.7723997598238709 Loss: 4.2014103\n",
      "TRAIN: Batch: 0.7733170992060845 Loss: 1.319899\n",
      "TRAIN: Batch: 0.7742344385882981 Loss: 0.69854593\n",
      "TRAIN: Batch: 0.7751517779705117 Loss: 1.3496479\n",
      "TRAIN: Batch: 0.7760691173527253 Loss: 2.2805347\n",
      "TRAIN: Batch: 0.7769864567349389 Loss: 2.5164757\n",
      "TRAIN: Batch: 0.7779037961171525 Loss: 3.9283457\n",
      "TRAIN: Batch: 0.7788211354993662 Loss: 1.7715876\n",
      "TRAIN: Batch: 0.7797384748815799 Loss: 0.68464273\n",
      "TRAIN: Batch: 0.7806558142637935 Loss: 1.3652302\n",
      "TRAIN: Batch: 0.7815731536460071 Loss: 2.6415434\n",
      "TRAIN: Batch: 0.7824904930282207 Loss: 1.3709348\n",
      "TRAIN: Batch: 0.7834078324104343 Loss: 1.1508964\n",
      "TRAIN: Batch: 0.7843251717926479 Loss: 2.6799803\n",
      "TRAIN: Batch: 0.7852425111748615 Loss: 1.7098708\n",
      "TRAIN: Batch: 0.7861598505570752 Loss: 2.3046758\n",
      "TRAIN: Batch: 0.7870771899392888 Loss: 2.2813332\n",
      "TRAIN: Batch: 0.7879945293215025 Loss: 0.96860397\n",
      "TRAIN: Batch: 0.7889118687037161 Loss: 0.8218993\n",
      "TRAIN: Batch: 0.7898292080859297 Loss: 3.752471\n",
      "TRAIN: Batch: 0.7907465474681433 Loss: 0.9851351\n",
      "TRAIN: Batch: 0.7916638868503569 Loss: 0.6326359\n",
      "TRAIN: Batch: 0.7925812262325705 Loss: 2.345572\n",
      "TRAIN: Batch: 0.7934985656147842 Loss: 1.7639511\n",
      "TRAIN: Batch: 0.7944159049969978 Loss: 2.7082567\n",
      "TRAIN: Batch: 0.7953332443792114 Loss: 6.9348025\n",
      "TRAIN: Batch: 0.796250583761425 Loss: 2.9515734\n",
      "TRAIN: Batch: 0.7971679231436387 Loss: 2.2557201\n",
      "TRAIN: Batch: 0.7980852625258523 Loss: 1.0385134\n",
      "TRAIN: Batch: 0.7990026019080659 Loss: 0.8259717\n",
      "TRAIN: Batch: 0.7999199412902795 Loss: 2.214304\n",
      "TRAIN: Batch: 0.8008372806724932 Loss: 1.2714921\n",
      "TRAIN: Batch: 0.8017546200547068 Loss: 2.0731988\n",
      "TRAIN: Batch: 0.8026719594369204 Loss: 1.0401042\n",
      "TRAIN: Batch: 0.803589298819134 Loss: 3.8993325\n",
      "TRAIN: Batch: 0.8045066382013476 Loss: 0.88763034\n",
      "TRAIN: Batch: 0.8054239775835613 Loss: 2.8205261\n",
      "TRAIN: Batch: 0.8063413169657749 Loss: 2.2131362\n",
      "TRAIN: Batch: 0.8072586563479885 Loss: 2.1057453\n",
      "TRAIN: Batch: 0.8081759957302022 Loss: 0.88280356\n",
      "TRAIN: Batch: 0.8090933351124158 Loss: 1.2250347\n",
      "TRAIN: Batch: 0.8100106744946294 Loss: 2.4772277\n",
      "TRAIN: Batch: 0.810928013876843 Loss: 4.0233884\n",
      "TRAIN: Batch: 0.8118453532590566 Loss: 1.136852\n",
      "TRAIN: Batch: 0.8127626926412702 Loss: 2.4063942\n",
      "TRAIN: Batch: 0.8136800320234839 Loss: 4.785165\n",
      "TRAIN: Batch: 0.8145973714056975 Loss: 1.9509125\n",
      "TRAIN: Batch: 0.8155147107879112 Loss: 1.2411867\n",
      "TRAIN: Batch: 0.8164320501701248 Loss: 1.6968746\n",
      "TRAIN: Batch: 0.8173493895523384 Loss: 1.9016507\n",
      "TRAIN: Batch: 0.818266728934552 Loss: 0.8019038\n",
      "TRAIN: Batch: 0.8191840683167656 Loss: 0.48955187\n",
      "TRAIN: Batch: 0.8201014076989792 Loss: 1.4246968\n",
      "TRAIN: Batch: 0.8210187470811928 Loss: 1.506747\n",
      "TRAIN: Batch: 0.8219360864634065 Loss: 2.148641\n",
      "TRAIN: Batch: 0.8228534258456202 Loss: 0.803126\n",
      "TRAIN: Batch: 0.8237707652278338 Loss: 1.7263327\n",
      "TRAIN: Batch: 0.8246881046100474 Loss: 3.3448424\n",
      "TRAIN: Batch: 0.825605443992261 Loss: 0.8224127\n",
      "TRAIN: Batch: 0.8265227833744746 Loss: 3.3099737\n",
      "TRAIN: Batch: 0.8274401227566882 Loss: 1.2816205\n",
      "TRAIN: Batch: 0.8283574621389018 Loss: 1.9041976\n",
      "TRAIN: Batch: 0.8292748015211154 Loss: 3.0703306\n",
      "TRAIN: Batch: 0.8301921409033292 Loss: 1.3349456\n",
      "TRAIN: Batch: 0.8311094802855428 Loss: 0.9179124\n",
      "TRAIN: Batch: 0.8320268196677564 Loss: 1.0185302\n",
      "TRAIN: Batch: 0.83294415904997 Loss: 1.2511406\n",
      "TRAIN: Batch: 0.8338614984321836 Loss: 1.1476585\n",
      "TRAIN: Batch: 0.8347788378143972 Loss: 1.7172267\n",
      "TRAIN: Batch: 0.8356961771966108 Loss: 3.8565547\n",
      "TRAIN: Batch: 0.8366135165788244 Loss: 1.4668498\n",
      "TRAIN: Batch: 0.837530855961038 Loss: 1.6487176\n",
      "TRAIN: Batch: 0.8384481953432518 Loss: 1.0928676\n",
      "TRAIN: Batch: 0.8393655347254654 Loss: 5.5265813\n",
      "TRAIN: Batch: 0.840282874107679 Loss: 6.4827375\n",
      "TRAIN: Batch: 0.8412002134898926 Loss: 3.8452964\n",
      "TRAIN: Batch: 0.8421175528721062 Loss: 3.8840919\n",
      "TRAIN: Batch: 0.8430348922543198 Loss: 0.7675891\n",
      "TRAIN: Batch: 0.8439522316365334 Loss: 1.476208\n",
      "TRAIN: Batch: 0.844869571018747 Loss: 0.91608864\n",
      "TRAIN: Batch: 0.8457869104009607 Loss: 2.4878507\n",
      "TRAIN: Batch: 0.8467042497831744 Loss: 1.6206216\n",
      "TRAIN: Batch: 0.847621589165388 Loss: 4.4565406\n",
      "TRAIN: Batch: 0.8485389285476016 Loss: 1.5029956\n",
      "TRAIN: Batch: 0.8494562679298152 Loss: 4.266878\n",
      "TRAIN: Batch: 0.8503736073120288 Loss: 5.7092505\n",
      "TRAIN: Batch: 0.8512909466942424 Loss: 3.3623538\n",
      "TRAIN: Batch: 0.852208286076456 Loss: 1.4087137\n",
      "TRAIN: Batch: 0.8531256254586697 Loss: 0.88037515\n",
      "TRAIN: Batch: 0.8540429648408833 Loss: 1.4186931\n",
      "TRAIN: Batch: 0.854960304223097 Loss: 0.5789203\n",
      "TRAIN: Batch: 0.8558776436053106 Loss: 0.88950914\n",
      "TRAIN: Batch: 0.8567949829875242 Loss: 1.166834\n",
      "TRAIN: Batch: 0.8577123223697378 Loss: 2.1005614\n",
      "TRAIN: Batch: 0.8586296617519514 Loss: 1.4909997\n",
      "TRAIN: Batch: 0.859547001134165 Loss: 1.5207067\n",
      "TRAIN: Batch: 0.8604643405163787 Loss: 1.4566543\n",
      "TRAIN: Batch: 0.8613816798985923 Loss: 3.5062687\n",
      "TRAIN: Batch: 0.8622990192808059 Loss: 1.6091499\n",
      "TRAIN: Batch: 0.8632163586630196 Loss: 3.301633\n",
      "TRAIN: Batch: 0.8641336980452332 Loss: 1.0630403\n",
      "TRAIN: Batch: 0.8650510374274468 Loss: 2.0898979\n",
      "TRAIN: Batch: 0.8659683768096604 Loss: 5.61404\n",
      "TRAIN: Batch: 0.866885716191874 Loss: 2.8002057\n",
      "TRAIN: Batch: 0.8678030555740877 Loss: 0.69483334\n",
      "TRAIN: Batch: 0.8687203949563013 Loss: 0.93143266\n",
      "TRAIN: Batch: 0.8696377343385149 Loss: 1.4725666\n",
      "TRAIN: Batch: 0.8705550737207285 Loss: 0.7396316\n",
      "TRAIN: Batch: 0.8714724131029422 Loss: 0.68175817\n",
      "TRAIN: Batch: 0.8723897524851558 Loss: 0.726575\n",
      "TRAIN: Batch: 0.8733070918673694 Loss: 2.6620598\n",
      "TRAIN: Batch: 0.874224431249583 Loss: 2.123403\n",
      "TRAIN: Batch: 0.8751417706317967 Loss: 1.0562942\n",
      "TRAIN: Batch: 0.8760591100140103 Loss: 1.6396168\n",
      "TRAIN: Batch: 0.8769764493962239 Loss: 0.8929416\n",
      "TRAIN: Batch: 0.8778937887784375 Loss: 3.4010463\n",
      "TRAIN: Batch: 0.8788111281606511 Loss: 3.6163907\n",
      "TRAIN: Batch: 0.8797284675428648 Loss: 4.1669607\n",
      "TRAIN: Batch: 0.8806458069250784 Loss: 5.82169\n",
      "TRAIN: Batch: 0.881563146307292 Loss: 3.697151\n",
      "TRAIN: Batch: 0.8824804856895057 Loss: 3.54612\n",
      "TRAIN: Batch: 0.8833978250717193 Loss: 3.0806067\n",
      "TRAIN: Batch: 0.8843151644539329 Loss: 1.6231747\n",
      "TRAIN: Batch: 0.8852325038361465 Loss: 1.0518281\n",
      "TRAIN: Batch: 0.8861498432183601 Loss: 1.757457\n",
      "TRAIN: Batch: 0.8870671826005737 Loss: 0.93630147\n",
      "TRAIN: Batch: 0.8879845219827873 Loss: 1.0756507\n",
      "TRAIN: Batch: 0.888901861365001 Loss: 2.1101894\n",
      "TRAIN: Batch: 0.8898192007472147 Loss: 4.025877\n",
      "TRAIN: Batch: 0.8907365401294283 Loss: 1.320539\n",
      "TRAIN: Batch: 0.8916538795116419 Loss: 1.573059\n",
      "TRAIN: Batch: 0.8925712188938555 Loss: 5.7006655\n",
      "TRAIN: Batch: 0.8934885582760691 Loss: 2.7816234\n",
      "TRAIN: Batch: 0.8944058976582827 Loss: 0.60578144\n",
      "TRAIN: Batch: 0.8953232370404963 Loss: 0.89263785\n",
      "TRAIN: Batch: 0.8962405764227099 Loss: 1.522867\n",
      "TRAIN: Batch: 0.8971579158049237 Loss: 1.4394265\n",
      "TRAIN: Batch: 0.8980752551871373 Loss: 2.3798494\n",
      "TRAIN: Batch: 0.8989925945693509 Loss: 4.231119\n",
      "TRAIN: Batch: 0.8999099339515645 Loss: 0.70027643\n",
      "TRAIN: Batch: 0.9008272733337781 Loss: 1.2232254\n",
      "TRAIN: Batch: 0.9017446127159917 Loss: 3.0212069\n",
      "TRAIN: Batch: 0.9026619520982053 Loss: 6.9333444\n",
      "TRAIN: Batch: 0.9035792914804189 Loss: 1.070811\n",
      "TRAIN: Batch: 0.9044966308626325 Loss: 2.476825\n",
      "TRAIN: Batch: 0.9054139702448463 Loss: 1.3906112\n",
      "TRAIN: Batch: 0.9063313096270599 Loss: 0.6384139\n",
      "TRAIN: Batch: 0.9072486490092735 Loss: 3.5496767\n",
      "TRAIN: Batch: 0.9081659883914871 Loss: inf\n",
      "TRAIN: Batch: 0.9090833277737007 Loss: 0.79460895\n",
      "TRAIN: Batch: 0.9100006671559143 Loss: 0.8619956\n",
      "TRAIN: Batch: 0.9109180065381279 Loss: 0.7347448\n",
      "TRAIN: Batch: 0.9118353459203415 Loss: 1.50159\n",
      "TRAIN: Batch: 0.9127526853025552 Loss: 1.8982762\n",
      "TRAIN: Batch: 0.9136700246847689 Loss: 1.5436206\n",
      "TRAIN: Batch: 0.9145873640669825 Loss: 1.304918\n",
      "TRAIN: Batch: 0.9155047034491961 Loss: 3.8419898\n",
      "TRAIN: Batch: 0.9164220428314097 Loss: 2.9832768\n",
      "TRAIN: Batch: 0.9173393822136233 Loss: 0.93244684\n",
      "TRAIN: Batch: 0.9182567215958369 Loss: 0.8863232\n",
      "TRAIN: Batch: 0.9191740609780505 Loss: 2.3603654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.9200914003602642 Loss: 1.0930078\n",
      "TRAIN: Batch: 0.9210087397424778 Loss: 1.0162625\n",
      "TRAIN: Batch: 0.9219260791246915 Loss: 4.3186703\n",
      "TRAIN: Batch: 0.9228434185069051 Loss: 4.4450917\n",
      "TRAIN: Batch: 0.9237607578891187 Loss: 1.0764066\n",
      "TRAIN: Batch: 0.9246780972713323 Loss: 2.4362118\n",
      "TRAIN: Batch: 0.9255954366535459 Loss: 3.1273503\n",
      "TRAIN: Batch: 0.9265127760357595 Loss: 0.81604856\n",
      "TRAIN: Batch: 0.9274301154179732 Loss: 2.135413\n",
      "TRAIN: Batch: 0.9283474548001868 Loss: 1.0947828\n",
      "TRAIN: Batch: 0.9292647941824004 Loss: 1.0123564\n",
      "TRAIN: Batch: 0.9301821335646141 Loss: 2.078736\n",
      "TRAIN: Batch: 0.9310994729468277 Loss: 1.8424454\n",
      "TRAIN: Batch: 0.9320168123290413 Loss: 3.8178647\n",
      "TRAIN: Batch: 0.9329341517112549 Loss: 1.391329\n",
      "TRAIN: Batch: 0.9338514910934685 Loss: 3.0563436\n",
      "TRAIN: Batch: 0.9347688304756822 Loss: 5.693544\n",
      "TRAIN: Batch: 0.9356861698578958 Loss: 1.5011445\n",
      "TRAIN: Batch: 0.9366035092401094 Loss: 1.1267991\n",
      "TRAIN: Batch: 0.937520848622323 Loss: 0.50946283\n",
      "TRAIN: Batch: 0.9384381880045367 Loss: 1.371599\n",
      "TRAIN: Batch: 0.9393555273867503 Loss: 1.7545165\n",
      "TRAIN: Batch: 0.9402728667689639 Loss: 1.1551176\n",
      "TRAIN: Batch: 0.9411902061511775 Loss: 5.8020916\n",
      "TRAIN: Batch: 0.9421075455333912 Loss: 0.82705986\n",
      "TRAIN: Batch: 0.9430248849156048 Loss: 0.5957966\n",
      "TRAIN: Batch: 0.9439422242978184 Loss: 4.797385\n",
      "TRAIN: Batch: 0.944859563680032 Loss: 1.3607156\n",
      "TRAIN: Batch: 0.9457769030622456 Loss: 6.487541\n",
      "TRAIN: Batch: 0.9466942424444593 Loss: 0.70477724\n",
      "TRAIN: Batch: 0.9476115818266729 Loss: 0.62591696\n",
      "TRAIN: Batch: 0.9485289212088865 Loss: 2.1514847\n",
      "TRAIN: Batch: 0.9494462605911002 Loss: 0.8497243\n",
      "TRAIN: Batch: 0.9503635999733138 Loss: 1.9973783\n",
      "TRAIN: Batch: 0.9512809393555274 Loss: 2.1191893\n",
      "TRAIN: Batch: 0.952198278737741 Loss: 1.4174756\n",
      "TRAIN: Batch: 0.9531156181199546 Loss: 2.0097528\n",
      "TRAIN: Batch: 0.9540329575021682 Loss: 2.919649\n",
      "TRAIN: Batch: 0.9549502968843819 Loss: 1.6093678\n",
      "TRAIN: Batch: 0.9558676362665955 Loss: 0.92098725\n",
      "TRAIN: Batch: 0.9567849756488092 Loss: 3.2880921\n",
      "TRAIN: Batch: 0.9577023150310228 Loss: 1.5131869\n",
      "TRAIN: Batch: 0.9586196544132364 Loss: 2.585336\n",
      "TRAIN: Batch: 0.95953699379545 Loss: 1.4532454\n",
      "TRAIN: Batch: 0.9604543331776636 Loss: 1.3808166\n",
      "TRAIN: Batch: 0.9613716725598772 Loss: 0.90486217\n",
      "TRAIN: Batch: 0.9622890119420908 Loss: 2.5065894\n",
      "TRAIN: Batch: 0.9632063513243045 Loss: 1.9431708\n",
      "TRAIN: Batch: 0.9641236907065182 Loss: 1.1489351\n",
      "TRAIN: Batch: 0.9650410300887318 Loss: 1.4664679\n",
      "TRAIN: Batch: 0.9659583694709454 Loss: 2.1561625\n",
      "TRAIN: Batch: 0.966875708853159 Loss: 0.69551945\n",
      "TRAIN: Batch: 0.9677930482353726 Loss: 0.49918073\n",
      "TRAIN: Batch: 0.9687103876175862 Loss: 0.96207815\n",
      "TRAIN: Batch: 0.9696277269997998 Loss: 0.9227971\n",
      "TRAIN: Batch: 0.9705450663820134 Loss: 1.264064\n",
      "TRAIN: Batch: 0.9714624057642272 Loss: 0.9239064\n",
      "TRAIN: Batch: 0.9723797451464408 Loss: 1.1508769\n",
      "TRAIN: Batch: 0.9732970845286544 Loss: 1.446485\n",
      "TRAIN: Batch: 0.974214423910868 Loss: 2.3280313\n",
      "TRAIN: Batch: 0.9751317632930816 Loss: 1.2064542\n",
      "TRAIN: Batch: 0.9760491026752952 Loss: 3.52429\n",
      "TRAIN: Batch: 0.9769664420575088 Loss: 1.0483183\n",
      "TRAIN: Batch: 0.9778837814397224 Loss: 2.3233154\n",
      "TRAIN: Batch: 0.978801120821936 Loss: 1.1116291\n",
      "TRAIN: Batch: 0.9797184602041497 Loss: 1.2381786\n",
      "TRAIN: Batch: 0.9806357995863634 Loss: 2.326061\n",
      "TRAIN: Batch: 0.981553138968577 Loss: 1.2890005\n",
      "TRAIN: Batch: 0.9824704783507906 Loss: 1.4796295\n",
      "TRAIN: Batch: 0.9833878177330042 Loss: 1.6222115\n",
      "TRAIN: Batch: 0.9843051571152178 Loss: 2.7843626\n",
      "TRAIN: Batch: 0.9852224964974314 Loss: 1.3088517\n",
      "TRAIN: Batch: 0.986139835879645 Loss: 2.7646427\n",
      "TRAIN: Batch: 0.9870571752618587 Loss: 1.64233\n",
      "TRAIN: Batch: 0.9879745146440723 Loss: 0.655368\n",
      "TRAIN: Batch: 0.988891854026286 Loss: 3.9549334\n",
      "TRAIN: Batch: 0.9898091934084996 Loss: 0.9137772\n",
      "TRAIN: Batch: 0.9907265327907132 Loss: 4.433964\n",
      "TRAIN: Batch: 0.9916438721729268 Loss: 2.787631\n",
      "TRAIN: Batch: 0.9925612115551404 Loss: 1.1079183\n",
      "TRAIN: Batch: 0.993478550937354 Loss: 3.032841\n",
      "TRAIN: Batch: 0.9943958903195677 Loss: 0.5348054\n",
      "TRAIN: Batch: 0.9953132297017813 Loss: 12.186531\n",
      "TRAIN: Batch: 0.9962305690839949 Loss: 1.5110185\n",
      "TRAIN: Batch: 0.9971479084662086 Loss: 1.3286592\n",
      "TRAIN: Batch: 0.9980652478484222 Loss: 0.57170933\n",
      "TRAIN: Batch: 0.9989825872306358 Loss: 4.1293283\n",
      "TRAIN: Batch: 0.9998999266128494 Loss: 1.0500913\n",
      "Validating NN\n",
      "0\n",
      "4000\n",
      "8000\n",
      "12000\n",
      "16000\n",
      "20000\n",
      "24000\n",
      "28000\n",
      "32000\n",
      "36000\n",
      "40000\n",
      "44000\n",
      "48000\n",
      "52000\n",
      "56000\n",
      "60000\n",
      "64000\n",
      "68000\n",
      "72000\n",
      "76000\n",
      "80000\n",
      "84000\n",
      "88000\n",
      "92000\n",
      "96000\n",
      "100000\n",
      "104000\n",
      "108000\n",
      "112000\n",
      "116000\n",
      "120000\n",
      "124000\n",
      "128000\n",
      "132000\n",
      "136000\n",
      "140000\n",
      "144000\n",
      "148000\n",
      "152000\n",
      "156000\n",
      "160000\n",
      "164000\n",
      "168000\n",
      "172000\n",
      "176000\n",
      "180000\n",
      "184000\n",
      "188000\n",
      "192000\n",
      "196000\n",
      "200000\n",
      "204000\n",
      "208000\n",
      "212000\n",
      "216000\n",
      "220000\n",
      "224000\n",
      "228000\n",
      "232000\n",
      "236000\n",
      "240000\n",
      "244000\n",
      "248000\n",
      "252000\n",
      "256000\n",
      "260000\n",
      "264000\n",
      "268000\n",
      "272000\n",
      "276000\n",
      "280000\n",
      "284000\n",
      "288000\n",
      "292000\n",
      "296000\n",
      "300000\n",
      "304000\n",
      "308000\n",
      "312000\n",
      "VALID: Character error rate: 8.124682%. Word accuracy: 81.889340%.\n",
      "Character error rate improved, save model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpoint_management.py:236: generate_checkpoint_state_proto (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than editing the Checkpoint proto manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpoint_management.py:236: generate_checkpoint_state_proto (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than editing the Checkpoint proto manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  Training...\n",
      "TRAIN: Batch: 0.0 Loss: 1.5985504\n",
      "TRAIN: Batch: 0.0009173393822136234 Loss: 1.309416\n",
      "TRAIN: Batch: 0.0018346787644272467 Loss: 0.66038966\n",
      "TRAIN: Batch: 0.0027520181466408698 Loss: 3.9418762\n",
      "TRAIN: Batch: 0.0036693575288544934 Loss: 2.5427277\n",
      "TRAIN: Batch: 0.004586696911068116 Loss: 1.0230002\n",
      "TRAIN: Batch: 0.0055040362932817395 Loss: 0.6753164\n",
      "TRAIN: Batch: 0.006421375675495364 Loss: 0.48601922\n",
      "TRAIN: Batch: 0.007338715057708987 Loss: 1.4216577\n",
      "TRAIN: Batch: 0.00825605443992261 Loss: 0.8479975\n",
      "TRAIN: Batch: 0.009173393822136233 Loss: 4.4877653\n",
      "TRAIN: Batch: 0.010090733204349856 Loss: 0.66917104\n",
      "TRAIN: Batch: 0.011008072586563479 Loss: 2.496261\n",
      "TRAIN: Batch: 0.011925411968777104 Loss: 1.4777433\n",
      "TRAIN: Batch: 0.012842751350990727 Loss: 4.173238\n",
      "TRAIN: Batch: 0.01376009073320435 Loss: 2.3433454\n",
      "TRAIN: Batch: 0.014677430115417974 Loss: 1.5688614\n",
      "TRAIN: Batch: 0.015594769497631597 Loss: 3.3531926\n",
      "TRAIN: Batch: 0.01651210887984522 Loss: 3.9813228\n",
      "TRAIN: Batch: 0.017429448262058844 Loss: 0.70396304\n",
      "TRAIN: Batch: 0.018346787644272465 Loss: 0.7321871\n",
      "TRAIN: Batch: 0.01926412702648609 Loss: 1.0441933\n",
      "TRAIN: Batch: 0.02018146640869971 Loss: 7.6138744\n",
      "TRAIN: Batch: 0.021098805790913337 Loss: 1.4811256\n",
      "TRAIN: Batch: 0.022016145173126958 Loss: 2.2736874\n",
      "TRAIN: Batch: 0.022933484555340583 Loss: 2.32957\n",
      "TRAIN: Batch: 0.023850823937554208 Loss: 0.72738403\n",
      "TRAIN: Batch: 0.02476816331976783 Loss: 0.6893283\n",
      "TRAIN: Batch: 0.025685502701981455 Loss: 1.006643\n",
      "TRAIN: Batch: 0.026602842084195076 Loss: 0.6247372\n",
      "TRAIN: Batch: 0.0275201814664087 Loss: 2.0691419\n",
      "TRAIN: Batch: 0.028437520848622323 Loss: 3.257042\n",
      "TRAIN: Batch: 0.029354860230835948 Loss: 0.96425414\n",
      "TRAIN: Batch: 0.03027219961304957 Loss: 2.5892968\n",
      "TRAIN: Batch: 0.031189538995263194 Loss: 3.745999\n",
      "TRAIN: Batch: 0.03210687837747682 Loss: 1.1514843\n",
      "TRAIN: Batch: 0.03302421775969044 Loss: 0.85976166\n",
      "TRAIN: Batch: 0.03394155714190406 Loss: 4.090499\n",
      "TRAIN: Batch: 0.03485889652411769 Loss: 1.4995993\n",
      "TRAIN: Batch: 0.03577623590633131 Loss: 2.8664398\n",
      "TRAIN: Batch: 0.03669357528854493 Loss: 2.0409193\n",
      "TRAIN: Batch: 0.037610914670758555 Loss: 1.4354044\n",
      "TRAIN: Batch: 0.03852825405297218 Loss: 1.1333729\n",
      "TRAIN: Batch: 0.039445593435185805 Loss: 0.5606035\n",
      "TRAIN: Batch: 0.04036293281739942 Loss: 3.0299015\n",
      "TRAIN: Batch: 0.04128027219961305 Loss: 1.0133549\n",
      "TRAIN: Batch: 0.04219761158182667 Loss: 0.6751195\n",
      "TRAIN: Batch: 0.0431149509640403 Loss: 0.976261\n",
      "TRAIN: Batch: 0.044032290346253916 Loss: 1.6627293\n",
      "TRAIN: Batch: 0.04494962972846754 Loss: 2.3770316\n",
      "TRAIN: Batch: 0.045866969110681166 Loss: 0.7883589\n",
      "TRAIN: Batch: 0.04678430849289479 Loss: 5.3863654\n",
      "TRAIN: Batch: 0.047701647875108416 Loss: 0.7504388\n",
      "TRAIN: Batch: 0.048618987257322034 Loss: 2.3740022\n",
      "TRAIN: Batch: 0.04953632663953566 Loss: 0.717425\n",
      "TRAIN: Batch: 0.050453666021749284 Loss: 2.7091274\n",
      "TRAIN: Batch: 0.05137100540396291 Loss: 0.6783022\n",
      "TRAIN: Batch: 0.05228834478617653 Loss: 0.5526302\n",
      "TRAIN: Batch: 0.05320568416839015 Loss: 1.9710865\n",
      "TRAIN: Batch: 0.05412302355060378 Loss: 2.1918666\n",
      "TRAIN: Batch: 0.0550403629328174 Loss: 1.6284789\n",
      "TRAIN: Batch: 0.05595770231503102 Loss: 0.6770359\n",
      "TRAIN: Batch: 0.056875041697244645 Loss: 1.9425738\n",
      "TRAIN: Batch: 0.05779238107945827 Loss: 1.5401224\n",
      "TRAIN: Batch: 0.058709720461671895 Loss: 2.8301942\n",
      "TRAIN: Batch: 0.05962705984388551 Loss: 0.55250335\n",
      "TRAIN: Batch: 0.06054439922609914 Loss: 2.1572728\n",
      "TRAIN: Batch: 0.06146173860831276 Loss: 4.5991907\n",
      "TRAIN: Batch: 0.06237907799052639 Loss: 2.4117632\n",
      "TRAIN: Batch: 0.06329641737274001 Loss: 0.98483956\n",
      "TRAIN: Batch: 0.06421375675495364 Loss: 3.3920116\n",
      "TRAIN: Batch: 0.06513109613716725 Loss: 1.1568956\n",
      "TRAIN: Batch: 0.06604843551938087 Loss: 3.9763396\n",
      "TRAIN: Batch: 0.0669657749015945 Loss: 0.640284\n",
      "TRAIN: Batch: 0.06788311428380812 Loss: 1.4236778\n",
      "TRAIN: Batch: 0.06880045366602175 Loss: 1.7107453\n",
      "TRAIN: Batch: 0.06971779304823537 Loss: 1.6787388\n",
      "TRAIN: Batch: 0.070635132430449 Loss: 0.95852673\n",
      "TRAIN: Batch: 0.07155247181266262 Loss: 1.9134878\n",
      "TRAIN: Batch: 0.07246981119487625 Loss: 0.98389626\n",
      "TRAIN: Batch: 0.07338715057708986 Loss: 1.0811262\n",
      "TRAIN: Batch: 0.07430448995930349 Loss: 2.1722126\n",
      "TRAIN: Batch: 0.07522182934151711 Loss: 5.342223\n",
      "TRAIN: Batch: 0.07613916872373074 Loss: 1.2883435\n",
      "TRAIN: Batch: 0.07705650810594436 Loss: 2.4999747\n",
      "TRAIN: Batch: 0.07797384748815799 Loss: 0.92058796\n",
      "TRAIN: Batch: 0.07889118687037161 Loss: 0.889846\n",
      "TRAIN: Batch: 0.07980852625258524 Loss: 0.7348281\n",
      "TRAIN: Batch: 0.08072586563479885 Loss: 2.2494094\n",
      "TRAIN: Batch: 0.08164320501701247 Loss: 0.666648\n",
      "TRAIN: Batch: 0.0825605443992261 Loss: 2.4250362\n",
      "TRAIN: Batch: 0.08347788378143972 Loss: 1.6556635\n",
      "TRAIN: Batch: 0.08439522316365335 Loss: 2.1393178\n",
      "TRAIN: Batch: 0.08531256254586697 Loss: 0.9040034\n",
      "TRAIN: Batch: 0.0862299019280806 Loss: 1.4687551\n",
      "TRAIN: Batch: 0.08714724131029422 Loss: 0.6152086\n",
      "TRAIN: Batch: 0.08806458069250783 Loss: 1.1228004\n",
      "TRAIN: Batch: 0.08898192007472146 Loss: 0.7939822\n",
      "TRAIN: Batch: 0.08989925945693508 Loss: 2.0560594\n",
      "TRAIN: Batch: 0.09081659883914871 Loss: 0.86887956\n",
      "TRAIN: Batch: 0.09173393822136233 Loss: 1.6356846\n",
      "TRAIN: Batch: 0.09265127760357596 Loss: 1.5279733\n",
      "TRAIN: Batch: 0.09356861698578958 Loss: 0.758597\n",
      "TRAIN: Batch: 0.09448595636800321 Loss: 3.2453284\n",
      "TRAIN: Batch: 0.09540329575021683 Loss: 1.8056903\n",
      "TRAIN: Batch: 0.09632063513243044 Loss: 2.487425\n",
      "TRAIN: Batch: 0.09723797451464407 Loss: 2.47457\n",
      "TRAIN: Batch: 0.0981553138968577 Loss: 2.1475828\n",
      "TRAIN: Batch: 0.09907265327907132 Loss: 2.0923147\n",
      "TRAIN: Batch: 0.09998999266128494 Loss: 1.0556462\n",
      "TRAIN: Batch: 0.10090733204349857 Loss: 11.351283\n",
      "TRAIN: Batch: 0.1018246714257122 Loss: 5.636834\n",
      "TRAIN: Batch: 0.10274201080792582 Loss: 0.54265046\n",
      "TRAIN: Batch: 0.10365935019013943 Loss: 1.0351784\n",
      "TRAIN: Batch: 0.10457668957235305 Loss: 1.0074295\n",
      "TRAIN: Batch: 0.10549402895456668 Loss: 0.81669724\n",
      "TRAIN: Batch: 0.1064113683367803 Loss: 0.6868912\n",
      "TRAIN: Batch: 0.10732870771899393 Loss: 0.4807618\n",
      "TRAIN: Batch: 0.10824604710120755 Loss: 2.5058784\n",
      "TRAIN: Batch: 0.10916338648342118 Loss: 1.0523859\n",
      "TRAIN: Batch: 0.1100807258656348 Loss: 0.52954555\n",
      "TRAIN: Batch: 0.11099806524784842 Loss: 1.1865506\n",
      "TRAIN: Batch: 0.11191540463006204 Loss: 4.3568535\n",
      "TRAIN: Batch: 0.11283274401227567 Loss: 2.1790009\n",
      "TRAIN: Batch: 0.11375008339448929 Loss: 5.9599266\n",
      "TRAIN: Batch: 0.11466742277670292 Loss: 4.430248\n",
      "TRAIN: Batch: 0.11558476215891654 Loss: 3.3651264\n",
      "TRAIN: Batch: 0.11650210154113017 Loss: 3.3212028\n",
      "TRAIN: Batch: 0.11741944092334379 Loss: 3.2214699\n",
      "TRAIN: Batch: 0.11833678030555742 Loss: 2.0748181\n",
      "TRAIN: Batch: 0.11925411968777103 Loss: 1.6973307\n",
      "TRAIN: Batch: 0.12017145906998465 Loss: 3.043186\n",
      "TRAIN: Batch: 0.12108879845219828 Loss: 5.5730524\n",
      "TRAIN: Batch: 0.1220061378344119 Loss: 1.5787952\n",
      "TRAIN: Batch: 0.12292347721662553 Loss: 2.1893916\n",
      "TRAIN: Batch: 0.12384081659883915 Loss: 3.7071524\n",
      "TRAIN: Batch: 0.12475815598105278 Loss: 0.5794599\n",
      "TRAIN: Batch: 0.1256754953632664 Loss: 0.914935\n",
      "TRAIN: Batch: 0.12659283474548003 Loss: 0.88456583\n",
      "TRAIN: Batch: 0.12751017412769364 Loss: 2.3151758\n",
      "TRAIN: Batch: 0.12842751350990728 Loss: 0.6599637\n",
      "TRAIN: Batch: 0.1293448528921209 Loss: 1.08695\n",
      "TRAIN: Batch: 0.1302621922743345 Loss: 14.966541\n",
      "TRAIN: Batch: 0.13117953165654814 Loss: 2.1546528\n",
      "TRAIN: Batch: 0.13209687103876175 Loss: 2.157002\n",
      "TRAIN: Batch: 0.1330142104209754 Loss: 1.7082269\n",
      "TRAIN: Batch: 0.133931549803189 Loss: 3.484742\n",
      "TRAIN: Batch: 0.13484888918540264 Loss: 1.8673911\n",
      "TRAIN: Batch: 0.13576622856761625 Loss: 1.8206666\n",
      "TRAIN: Batch: 0.1366835679498299 Loss: 0.53024536\n",
      "TRAIN: Batch: 0.1376009073320435 Loss: 1.064934\n",
      "TRAIN: Batch: 0.1385182467142571 Loss: 1.3476489\n",
      "TRAIN: Batch: 0.13943558609647075 Loss: 2.5159643\n",
      "TRAIN: Batch: 0.14035292547868436 Loss: 3.5155613\n",
      "TRAIN: Batch: 0.141270264860898 Loss: 0.7388201\n",
      "TRAIN: Batch: 0.1421876042431116 Loss: 1.684262\n",
      "TRAIN: Batch: 0.14310494362532525 Loss: 1.2134063\n",
      "TRAIN: Batch: 0.14402228300753886 Loss: 1.194865\n",
      "TRAIN: Batch: 0.1449396223897525 Loss: 1.1862499\n",
      "TRAIN: Batch: 0.1458569617719661 Loss: 0.6573972\n",
      "TRAIN: Batch: 0.14677430115417972 Loss: 1.1155494\n",
      "TRAIN: Batch: 0.14769164053639336 Loss: 4.777247\n",
      "TRAIN: Batch: 0.14860897991860697 Loss: 1.0312554\n",
      "TRAIN: Batch: 0.1495263193008206 Loss: 2.404297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.15044365868303422 Loss: 1.3178089\n",
      "TRAIN: Batch: 0.15136099806524786 Loss: 0.5803506\n",
      "TRAIN: Batch: 0.15227833744746147 Loss: 1.5643276\n",
      "TRAIN: Batch: 0.15319567682967508 Loss: 2.3756576\n",
      "TRAIN: Batch: 0.15411301621188872 Loss: 1.3713115\n",
      "TRAIN: Batch: 0.15503035559410233 Loss: 1.0603817\n",
      "TRAIN: Batch: 0.15594769497631597 Loss: 2.8462043\n",
      "TRAIN: Batch: 0.15686503435852958 Loss: 1.9790936\n",
      "TRAIN: Batch: 0.15778237374074322 Loss: 0.5282061\n",
      "TRAIN: Batch: 0.15869971312295683 Loss: 1.3738396\n",
      "TRAIN: Batch: 0.15961705250517047 Loss: 1.2422752\n",
      "TRAIN: Batch: 0.16053439188738408 Loss: 1.3855929\n",
      "TRAIN: Batch: 0.1614517312695977 Loss: 1.3980393\n",
      "TRAIN: Batch: 0.16236907065181133 Loss: 1.1666365\n",
      "TRAIN: Batch: 0.16328641003402494 Loss: 1.490579\n",
      "TRAIN: Batch: 0.16420374941623858 Loss: 1.8699902\n",
      "TRAIN: Batch: 0.1651210887984522 Loss: 3.1141045\n",
      "TRAIN: Batch: 0.16603842818066583 Loss: 0.87725663\n",
      "TRAIN: Batch: 0.16695576756287944 Loss: 2.2038803\n",
      "TRAIN: Batch: 0.16787310694509308 Loss: 0.6243043\n",
      "TRAIN: Batch: 0.1687904463273067 Loss: 1.278892\n",
      "TRAIN: Batch: 0.1697077857095203 Loss: 0.95395786\n",
      "TRAIN: Batch: 0.17062512509173394 Loss: 3.788565\n",
      "TRAIN: Batch: 0.17154246447394755 Loss: 0.8678491\n",
      "TRAIN: Batch: 0.1724598038561612 Loss: 2.0309765\n",
      "TRAIN: Batch: 0.1733771432383748 Loss: 4.5693183\n",
      "TRAIN: Batch: 0.17429448262058844 Loss: 1.0674764\n",
      "TRAIN: Batch: 0.17521182200280205 Loss: 1.2011912\n",
      "TRAIN: Batch: 0.17612916138501566 Loss: 0.6664078\n",
      "TRAIN: Batch: 0.1770465007672293 Loss: 1.1561127\n",
      "TRAIN: Batch: 0.17796384014944291 Loss: 2.4123356\n",
      "TRAIN: Batch: 0.17888117953165655 Loss: 1.6823568\n",
      "TRAIN: Batch: 0.17979851891387016 Loss: 8.64272\n",
      "TRAIN: Batch: 0.1807158582960838 Loss: 1.7656522\n",
      "TRAIN: Batch: 0.18163319767829741 Loss: 4.941626\n",
      "TRAIN: Batch: 0.18255053706051105 Loss: 1.979454\n",
      "TRAIN: Batch: 0.18346787644272466 Loss: 0.95574003\n",
      "TRAIN: Batch: 0.18438521582493828 Loss: 0.8146844\n",
      "TRAIN: Batch: 0.18530255520715191 Loss: 0.7700441\n",
      "TRAIN: Batch: 0.18621989458936553 Loss: 9.124458\n",
      "TRAIN: Batch: 0.18713723397157916 Loss: 0.80097747\n",
      "TRAIN: Batch: 0.18805457335379278 Loss: 1.0053926\n",
      "TRAIN: Batch: 0.18897191273600641 Loss: 1.2524457\n",
      "TRAIN: Batch: 0.18988925211822003 Loss: 1.8120831\n",
      "TRAIN: Batch: 0.19080659150043366 Loss: 1.2797194\n",
      "TRAIN: Batch: 0.19172393088264728 Loss: 0.9266772\n",
      "TRAIN: Batch: 0.1926412702648609 Loss: 1.3012451\n",
      "TRAIN: Batch: 0.19355860964707453 Loss: 9.355159\n",
      "TRAIN: Batch: 0.19447594902928814 Loss: 2.46985\n",
      "TRAIN: Batch: 0.19539328841150178 Loss: 0.8589531\n",
      "TRAIN: Batch: 0.1963106277937154 Loss: 3.9272907\n",
      "TRAIN: Batch: 0.19722796717592903 Loss: 0.5029445\n",
      "TRAIN: Batch: 0.19814530655814264 Loss: 1.7969508\n",
      "TRAIN: Batch: 0.19906264594035625 Loss: 1.5940857\n",
      "TRAIN: Batch: 0.1999799853225699 Loss: 2.6302395\n",
      "TRAIN: Batch: 0.2008973247047835 Loss: 0.82386214\n",
      "TRAIN: Batch: 0.20181466408699714 Loss: 0.65184724\n",
      "TRAIN: Batch: 0.20273200346921075 Loss: 1.9270779\n",
      "TRAIN: Batch: 0.2036493428514244 Loss: 2.98016\n",
      "TRAIN: Batch: 0.204566682233638 Loss: 1.5962949\n",
      "TRAIN: Batch: 0.20548402161585164 Loss: 0.6609211\n",
      "TRAIN: Batch: 0.20640136099806525 Loss: 2.2840233\n",
      "TRAIN: Batch: 0.20731870038027886 Loss: 1.4133292\n",
      "TRAIN: Batch: 0.2082360397624925 Loss: 1.7077458\n",
      "TRAIN: Batch: 0.2091533791447061 Loss: 4.0986505\n",
      "TRAIN: Batch: 0.21007071852691975 Loss: 0.7184696\n",
      "TRAIN: Batch: 0.21098805790913336 Loss: 1.0008143\n",
      "TRAIN: Batch: 0.211905397291347 Loss: 0.84221756\n",
      "TRAIN: Batch: 0.2128227366735606 Loss: 2.1279888\n",
      "TRAIN: Batch: 0.21374007605577425 Loss: 2.4966743\n",
      "TRAIN: Batch: 0.21465741543798786 Loss: 1.9528491\n",
      "TRAIN: Batch: 0.21557475482020147 Loss: 1.6043472\n",
      "TRAIN: Batch: 0.2164920942024151 Loss: 3.0487373\n",
      "TRAIN: Batch: 0.21740943358462872 Loss: 12.622284\n",
      "TRAIN: Batch: 0.21832677296684236 Loss: 4.972517\n",
      "TRAIN: Batch: 0.21924411234905597 Loss: 1.0954719\n",
      "TRAIN: Batch: 0.2201614517312696 Loss: 0.6222857\n",
      "TRAIN: Batch: 0.22107879111348322 Loss: 3.6906326\n",
      "TRAIN: Batch: 0.22199613049569683 Loss: 1.2300951\n",
      "TRAIN: Batch: 0.22291346987791047 Loss: 1.900733\n",
      "TRAIN: Batch: 0.22383080926012408 Loss: 1.992307\n",
      "TRAIN: Batch: 0.22474814864233772 Loss: 1.3268462\n",
      "TRAIN: Batch: 0.22566548802455133 Loss: 2.858839\n",
      "TRAIN: Batch: 0.22658282740676497 Loss: 3.0129983\n",
      "TRAIN: Batch: 0.22750016678897858 Loss: 0.4957699\n",
      "TRAIN: Batch: 0.22841750617119222 Loss: 0.7392895\n",
      "TRAIN: Batch: 0.22933484555340583 Loss: 3.2671244\n",
      "TRAIN: Batch: 0.23025218493561944 Loss: 1.1270913\n",
      "TRAIN: Batch: 0.23116952431783308 Loss: 1.0743219\n",
      "TRAIN: Batch: 0.2320868637000467 Loss: 0.9759939\n",
      "TRAIN: Batch: 0.23300420308226033 Loss: 1.1028411\n",
      "TRAIN: Batch: 0.23392154246447394 Loss: 0.9512228\n",
      "TRAIN: Batch: 0.23483888184668758 Loss: 1.8646662\n",
      "TRAIN: Batch: 0.2357562212289012 Loss: 1.1571966\n",
      "TRAIN: Batch: 0.23667356061111483 Loss: 1.1061255\n",
      "TRAIN: Batch: 0.23759089999332844 Loss: 1.9935688\n",
      "TRAIN: Batch: 0.23850823937554205 Loss: 0.98622787\n",
      "TRAIN: Batch: 0.2394255787577557 Loss: 0.60269237\n",
      "TRAIN: Batch: 0.2403429181399693 Loss: 3.7563343\n",
      "TRAIN: Batch: 0.24126025752218294 Loss: 10.410333\n",
      "TRAIN: Batch: 0.24217759690439655 Loss: 4.058667\n",
      "TRAIN: Batch: 0.2430949362866102 Loss: 3.8759518\n",
      "TRAIN: Batch: 0.2440122756688238 Loss: 0.55617285\n",
      "TRAIN: Batch: 0.2449296150510374 Loss: 1.7989681\n",
      "TRAIN: Batch: 0.24584695443325105 Loss: 1.4115498\n",
      "TRAIN: Batch: 0.24676429381546466 Loss: 0.94396853\n",
      "TRAIN: Batch: 0.2476816331976783 Loss: 1.2416892\n",
      "TRAIN: Batch: 0.2485989725798919 Loss: 0.7490972\n",
      "TRAIN: Batch: 0.24951631196210555 Loss: 2.8233366\n",
      "TRAIN: Batch: 0.25043365134431916 Loss: 0.5539559\n",
      "TRAIN: Batch: 0.2513509907265328 Loss: 1.2650707\n",
      "TRAIN: Batch: 0.25226833010874644 Loss: 1.8398305\n",
      "TRAIN: Batch: 0.25318566949096005 Loss: 1.1223211\n",
      "TRAIN: Batch: 0.25410300887317366 Loss: 1.2075388\n",
      "TRAIN: Batch: 0.2550203482553873 Loss: 1.8833202\n",
      "TRAIN: Batch: 0.2559376876376009 Loss: 1.2453705\n",
      "TRAIN: Batch: 0.25685502701981455 Loss: 0.67872524\n",
      "TRAIN: Batch: 0.25777236640202816 Loss: 2.347739\n",
      "TRAIN: Batch: 0.2586897057842418 Loss: 2.2030551\n",
      "TRAIN: Batch: 0.2596070451664554 Loss: 0.5481464\n",
      "TRAIN: Batch: 0.260524384548669 Loss: 2.7612438\n",
      "TRAIN: Batch: 0.26144172393088266 Loss: 0.8302287\n",
      "TRAIN: Batch: 0.2623590633130963 Loss: 3.2501957\n",
      "TRAIN: Batch: 0.2632764026953099 Loss: 1.1437023\n",
      "TRAIN: Batch: 0.2641937420775235 Loss: 2.3209217\n",
      "TRAIN: Batch: 0.26511108145973716 Loss: 4.771405\n",
      "TRAIN: Batch: 0.2660284208419508 Loss: 0.59549344\n",
      "TRAIN: Batch: 0.2669457602241644 Loss: 0.60900265\n",
      "TRAIN: Batch: 0.267863099606378 Loss: 4.533455\n",
      "TRAIN: Batch: 0.2687804389885916 Loss: 1.1225016\n",
      "TRAIN: Batch: 0.2696977783708053 Loss: 1.4180429\n",
      "TRAIN: Batch: 0.2706151177530189 Loss: 1.2342687\n",
      "TRAIN: Batch: 0.2715324571352325 Loss: 1.8136168\n",
      "TRAIN: Batch: 0.2724497965174461 Loss: 1.9610554\n",
      "TRAIN: Batch: 0.2733671358996598 Loss: 0.54275656\n",
      "TRAIN: Batch: 0.2742844752818734 Loss: 0.8417226\n",
      "TRAIN: Batch: 0.275201814664087 Loss: 2.6994753\n",
      "TRAIN: Batch: 0.2761191540463006 Loss: 0.83728933\n",
      "TRAIN: Batch: 0.2770364934285142 Loss: 3.4822617\n",
      "TRAIN: Batch: 0.2779538328107279 Loss: 1.3350382\n",
      "TRAIN: Batch: 0.2788711721929415 Loss: 1.7746\n",
      "TRAIN: Batch: 0.2797885115751551 Loss: 1.2688454\n",
      "TRAIN: Batch: 0.2807058509573687 Loss: 2.2275429\n",
      "TRAIN: Batch: 0.2816231903395824 Loss: 2.792658\n",
      "TRAIN: Batch: 0.282540529721796 Loss: 1.1798345\n",
      "TRAIN: Batch: 0.2834578691040096 Loss: 1.5421151\n",
      "TRAIN: Batch: 0.2843752084862232 Loss: 0.49793303\n",
      "TRAIN: Batch: 0.28529254786843683 Loss: 0.59614563\n",
      "TRAIN: Batch: 0.2862098872506505 Loss: 4.051608\n",
      "TRAIN: Batch: 0.2871272266328641 Loss: 1.6189145\n",
      "TRAIN: Batch: 0.2880445660150777 Loss: 2.1948307\n",
      "TRAIN: Batch: 0.28896190539729133 Loss: 1.0374347\n",
      "TRAIN: Batch: 0.289879244779505 Loss: 0.50326073\n",
      "TRAIN: Batch: 0.2907965841617186 Loss: 2.3878593\n",
      "TRAIN: Batch: 0.2917139235439322 Loss: 0.8047443\n",
      "TRAIN: Batch: 0.29263126292614583 Loss: 1.6950078\n",
      "TRAIN: Batch: 0.29354860230835944 Loss: 2.2057915\n",
      "TRAIN: Batch: 0.2944659416905731 Loss: 1.5610157\n",
      "TRAIN: Batch: 0.2953832810727867 Loss: 2.6654823\n",
      "TRAIN: Batch: 0.29630062045500033 Loss: 2.33999\n",
      "TRAIN: Batch: 0.29721795983721394 Loss: 2.94309\n",
      "TRAIN: Batch: 0.2981352992194276 Loss: 2.9594347\n",
      "TRAIN: Batch: 0.2990526386016412 Loss: 0.92808306\n",
      "TRAIN: Batch: 0.29996997798385483 Loss: 5.853657\n",
      "TRAIN: Batch: 0.30088731736606844 Loss: 0.56115586\n",
      "TRAIN: Batch: 0.30180465674828205 Loss: 3.803046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.3027219961304957 Loss: 1.5423512\n",
      "TRAIN: Batch: 0.30363933551270933 Loss: 1.9532516\n",
      "TRAIN: Batch: 0.30455667489492294 Loss: 2.3192954\n",
      "TRAIN: Batch: 0.30547401427713655 Loss: 0.63126504\n",
      "TRAIN: Batch: 0.30639135365935016 Loss: 3.4894364\n",
      "TRAIN: Batch: 0.30730869304156383 Loss: 1.5561538\n",
      "TRAIN: Batch: 0.30822603242377744 Loss: 0.94298685\n",
      "TRAIN: Batch: 0.30914337180599105 Loss: 0.5697491\n",
      "TRAIN: Batch: 0.31006071118820466 Loss: 1.383879\n",
      "TRAIN: Batch: 0.31097805057041833 Loss: 0.86968124\n",
      "TRAIN: Batch: 0.31189538995263194 Loss: 0.7014506\n",
      "TRAIN: Batch: 0.31281272933484555 Loss: 0.9149958\n",
      "TRAIN: Batch: 0.31373006871705916 Loss: 2.9782505\n",
      "TRAIN: Batch: 0.3146474080992728 Loss: 0.83157825\n",
      "TRAIN: Batch: 0.31556474748148644 Loss: 0.829961\n",
      "TRAIN: Batch: 0.31648208686370005 Loss: 1.0218176\n",
      "TRAIN: Batch: 0.31739942624591366 Loss: 1.0114884\n",
      "TRAIN: Batch: 0.3183167656281273 Loss: 0.82280123\n",
      "TRAIN: Batch: 0.31923410501034094 Loss: 2.5424178\n",
      "TRAIN: Batch: 0.32015144439255455 Loss: 1.3242981\n",
      "TRAIN: Batch: 0.32106878377476816 Loss: 3.1255217\n",
      "TRAIN: Batch: 0.3219861231569818 Loss: 2.483335\n",
      "TRAIN: Batch: 0.3229034625391954 Loss: 0.47551608\n",
      "TRAIN: Batch: 0.32382080192140905 Loss: 1.2639089\n",
      "TRAIN: Batch: 0.32473814130362266 Loss: 1.7695633\n",
      "TRAIN: Batch: 0.3256554806858363 Loss: 3.619989\n",
      "TRAIN: Batch: 0.3265728200680499 Loss: 1.8151119\n",
      "TRAIN: Batch: 0.32749015945026355 Loss: 1.5769757\n",
      "TRAIN: Batch: 0.32840749883247716 Loss: 1.428784\n",
      "TRAIN: Batch: 0.3293248382146908 Loss: 0.6304933\n",
      "TRAIN: Batch: 0.3302421775969044 Loss: 1.1286314\n",
      "TRAIN: Batch: 0.331159516979118 Loss: 2.415423\n",
      "TRAIN: Batch: 0.33207685636133166 Loss: 0.58662426\n",
      "TRAIN: Batch: 0.3329941957435453 Loss: 1.60563\n",
      "TRAIN: Batch: 0.3339115351257589 Loss: 2.225016\n",
      "TRAIN: Batch: 0.3348288745079725 Loss: 0.72593844\n",
      "TRAIN: Batch: 0.33574621389018616 Loss: 0.784231\n",
      "TRAIN: Batch: 0.3366635532723998 Loss: 1.1758637\n",
      "TRAIN: Batch: 0.3375808926546134 Loss: 1.5500329\n",
      "TRAIN: Batch: 0.338498232036827 Loss: 0.8168004\n",
      "TRAIN: Batch: 0.3394155714190406 Loss: 0.9820976\n",
      "TRAIN: Batch: 0.3403329108012543 Loss: 2.0096178\n",
      "TRAIN: Batch: 0.3412502501834679 Loss: 0.9030664\n",
      "TRAIN: Batch: 0.3421675895656815 Loss: 1.7634959\n",
      "TRAIN: Batch: 0.3430849289478951 Loss: 1.569364\n",
      "TRAIN: Batch: 0.3440022683301088 Loss: 0.8564124\n",
      "TRAIN: Batch: 0.3449196077123224 Loss: 1.0185806\n",
      "TRAIN: Batch: 0.345836947094536 Loss: 1.1816394\n",
      "TRAIN: Batch: 0.3467542864767496 Loss: 2.555798\n",
      "TRAIN: Batch: 0.3476716258589632 Loss: 4.634963\n",
      "TRAIN: Batch: 0.3485889652411769 Loss: 3.5642123\n",
      "TRAIN: Batch: 0.3495063046233905 Loss: 2.2584248\n",
      "TRAIN: Batch: 0.3504236440056041 Loss: 1.3001139\n",
      "TRAIN: Batch: 0.3513409833878177 Loss: 2.6041641\n",
      "TRAIN: Batch: 0.35225832277003133 Loss: 3.999281\n",
      "TRAIN: Batch: 0.353175662152245 Loss: 2.3376358\n",
      "TRAIN: Batch: 0.3540930015344586 Loss: 0.83870137\n",
      "TRAIN: Batch: 0.3550103409166722 Loss: 2.5204432\n",
      "TRAIN: Batch: 0.35592768029888583 Loss: 1.2159662\n",
      "TRAIN: Batch: 0.3568450196810995 Loss: 0.98504066\n",
      "TRAIN: Batch: 0.3577623590633131 Loss: 1.6082984\n",
      "TRAIN: Batch: 0.3586796984455267 Loss: 1.6850843\n",
      "TRAIN: Batch: 0.35959703782774033 Loss: inf\n",
      "TRAIN: Batch: 0.36051437720995394 Loss: 0.83554196\n",
      "TRAIN: Batch: 0.3614317165921676 Loss: 1.8208379\n",
      "TRAIN: Batch: 0.3623490559743812 Loss: 0.8266442\n",
      "TRAIN: Batch: 0.36326639535659483 Loss: 0.5955756\n",
      "TRAIN: Batch: 0.36418373473880844 Loss: 2.069934\n",
      "TRAIN: Batch: 0.3651010741210221 Loss: 3.1615016\n",
      "TRAIN: Batch: 0.3660184135032357 Loss: 5.8492875\n",
      "TRAIN: Batch: 0.36693575288544933 Loss: 0.72251606\n",
      "TRAIN: Batch: 0.36785309226766294 Loss: 1.2782292\n",
      "TRAIN: Batch: 0.36877043164987655 Loss: 2.8647585\n",
      "TRAIN: Batch: 0.3696877710320902 Loss: 1.1511139\n",
      "TRAIN: Batch: 0.37060511041430383 Loss: 3.1154327\n",
      "TRAIN: Batch: 0.37152244979651744 Loss: 1.3500899\n",
      "TRAIN: Batch: 0.37243978917873105 Loss: 0.8553442\n",
      "TRAIN: Batch: 0.3733571285609447 Loss: 2.3183541\n",
      "TRAIN: Batch: 0.37427446794315833 Loss: 1.0084689\n",
      "TRAIN: Batch: 0.37519180732537194 Loss: 5.687905\n",
      "TRAIN: Batch: 0.37610914670758555 Loss: 2.8211982\n",
      "TRAIN: Batch: 0.37702648608979916 Loss: 2.4046328\n",
      "TRAIN: Batch: 0.37794382547201283 Loss: 2.6451323\n",
      "TRAIN: Batch: 0.37886116485422644 Loss: 2.2777832\n",
      "TRAIN: Batch: 0.37977850423644005 Loss: 2.7625196\n",
      "TRAIN: Batch: 0.38069584361865366 Loss: 0.97564644\n",
      "TRAIN: Batch: 0.38161318300086733 Loss: 2.1843073\n",
      "TRAIN: Batch: 0.38253052238308094 Loss: 1.3570354\n",
      "TRAIN: Batch: 0.38344786176529455 Loss: 1.4629171\n",
      "TRAIN: Batch: 0.38436520114750816 Loss: 0.5825822\n",
      "TRAIN: Batch: 0.3852825405297218 Loss: 1.2245169\n",
      "TRAIN: Batch: 0.38619987991193544 Loss: 2.1996913\n",
      "TRAIN: Batch: 0.38711721929414905 Loss: 1.2406731\n",
      "TRAIN: Batch: 0.38803455867636266 Loss: 0.6577947\n",
      "TRAIN: Batch: 0.3889518980585763 Loss: 2.648116\n",
      "TRAIN: Batch: 0.38986923744078994 Loss: 2.0157316\n",
      "TRAIN: Batch: 0.39078657682300355 Loss: 0.49183092\n",
      "TRAIN: Batch: 0.39170391620521716 Loss: 1.6423528\n",
      "TRAIN: Batch: 0.3926212555874308 Loss: 2.1899278\n",
      "TRAIN: Batch: 0.3935385949696444 Loss: 1.2048911\n",
      "TRAIN: Batch: 0.39445593435185805 Loss: 1.5657237\n",
      "TRAIN: Batch: 0.39537327373407166 Loss: 0.7741213\n",
      "TRAIN: Batch: 0.3962906131162853 Loss: 0.5989262\n",
      "TRAIN: Batch: 0.3972079524984989 Loss: 2.237075\n",
      "TRAIN: Batch: 0.3981252918807125 Loss: 6.6726604\n",
      "TRAIN: Batch: 0.39904263126292616 Loss: 0.50123084\n",
      "TRAIN: Batch: 0.3999599706451398 Loss: 0.61455584\n",
      "TRAIN: Batch: 0.4008773100273534 Loss: 0.9934221\n",
      "TRAIN: Batch: 0.401794649409567 Loss: 5.6322494\n",
      "TRAIN: Batch: 0.40271198879178066 Loss: 1.1323669\n",
      "TRAIN: Batch: 0.4036293281739943 Loss: 1.6784594\n",
      "TRAIN: Batch: 0.4045466675562079 Loss: 1.3998206\n",
      "TRAIN: Batch: 0.4054640069384215 Loss: 1.1165309\n",
      "TRAIN: Batch: 0.4063813463206351 Loss: 0.920039\n",
      "TRAIN: Batch: 0.4072986857028488 Loss: 1.068094\n",
      "TRAIN: Batch: 0.4082160250850624 Loss: 0.81949556\n",
      "TRAIN: Batch: 0.409133364467276 Loss: 1.8500943\n",
      "TRAIN: Batch: 0.4100507038494896 Loss: 1.1903827\n",
      "TRAIN: Batch: 0.4109680432317033 Loss: 1.4075868\n",
      "TRAIN: Batch: 0.4118853826139169 Loss: 0.8194897\n",
      "TRAIN: Batch: 0.4128027219961305 Loss: 0.84519666\n",
      "TRAIN: Batch: 0.4137200613783441 Loss: 0.5724114\n",
      "TRAIN: Batch: 0.4146374007605577 Loss: 0.7212658\n",
      "TRAIN: Batch: 0.4155547401427714 Loss: 0.67146325\n",
      "TRAIN: Batch: 0.416472079524985 Loss: 1.7740328\n",
      "TRAIN: Batch: 0.4173894189071986 Loss: 2.3681097\n",
      "TRAIN: Batch: 0.4183067582894122 Loss: 1.5543168\n",
      "TRAIN: Batch: 0.4192240976716259 Loss: 1.052553\n",
      "TRAIN: Batch: 0.4201414370538395 Loss: 0.60468656\n",
      "TRAIN: Batch: 0.4210587764360531 Loss: 1.441462\n",
      "TRAIN: Batch: 0.4219761158182667 Loss: 7.063969\n",
      "TRAIN: Batch: 0.42289345520048033 Loss: 1.0673549\n",
      "TRAIN: Batch: 0.423810794582694 Loss: 0.76327413\n",
      "TRAIN: Batch: 0.4247281339649076 Loss: 1.2974665\n",
      "TRAIN: Batch: 0.4256454733471212 Loss: 1.3656702\n",
      "TRAIN: Batch: 0.42656281272933483 Loss: 2.217218\n",
      "TRAIN: Batch: 0.4274801521115485 Loss: 2.804731\n",
      "TRAIN: Batch: 0.4283974914937621 Loss: 3.5391386\n",
      "TRAIN: Batch: 0.4293148308759757 Loss: 1.1946555\n",
      "TRAIN: Batch: 0.43023217025818933 Loss: 4.256198\n",
      "TRAIN: Batch: 0.43114950964040294 Loss: 0.85885036\n",
      "TRAIN: Batch: 0.4320668490226166 Loss: 4.641675\n",
      "TRAIN: Batch: 0.4329841884048302 Loss: 3.1700077\n",
      "TRAIN: Batch: 0.43390152778704383 Loss: 4.965537\n",
      "TRAIN: Batch: 0.43481886716925744 Loss: 2.173079\n",
      "TRAIN: Batch: 0.4357362065514711 Loss: 1.0400804\n",
      "TRAIN: Batch: 0.4366535459336847 Loss: 1.4089111\n",
      "TRAIN: Batch: 0.43757088531589833 Loss: 2.636563\n",
      "TRAIN: Batch: 0.43848822469811194 Loss: 1.801356\n",
      "TRAIN: Batch: 0.43940556408032555 Loss: 0.9988655\n",
      "TRAIN: Batch: 0.4403229034625392 Loss: 0.52911437\n",
      "TRAIN: Batch: 0.44124024284475283 Loss: 1.6636033\n",
      "TRAIN: Batch: 0.44215758222696644 Loss: 1.3400778\n",
      "TRAIN: Batch: 0.44307492160918005 Loss: 0.5348792\n",
      "TRAIN: Batch: 0.44399226099139366 Loss: 1.74249\n",
      "TRAIN: Batch: 0.44490960037360733 Loss: 2.2203023\n",
      "TRAIN: Batch: 0.44582693975582094 Loss: 0.727866\n",
      "TRAIN: Batch: 0.44674427913803455 Loss: 1.1176414\n",
      "TRAIN: Batch: 0.44766161852024816 Loss: 1.877981\n",
      "TRAIN: Batch: 0.44857895790246183 Loss: 0.69392896\n",
      "TRAIN: Batch: 0.44949629728467544 Loss: 1.1713032\n",
      "TRAIN: Batch: 0.45041363666688905 Loss: 0.59589815\n",
      "TRAIN: Batch: 0.45133097604910266 Loss: 1.2507658\n",
      "TRAIN: Batch: 0.4522483154313163 Loss: 2.1768246\n",
      "TRAIN: Batch: 0.45316565481352994 Loss: 0.9366255\n",
      "TRAIN: Batch: 0.45408299419574355 Loss: 1.825409\n",
      "TRAIN: Batch: 0.45500033357795716 Loss: 0.98938906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.4559176729601708 Loss: 0.90304726\n",
      "TRAIN: Batch: 0.45683501234238444 Loss: 1.9536079\n",
      "TRAIN: Batch: 0.45775235172459805 Loss: 1.048586\n",
      "TRAIN: Batch: 0.45866969110681166 Loss: 10.023961\n",
      "TRAIN: Batch: 0.45958703048902527 Loss: 1.8555692\n",
      "TRAIN: Batch: 0.4605043698712389 Loss: 2.4168727\n",
      "TRAIN: Batch: 0.46142170925345255 Loss: 1.5605565\n",
      "TRAIN: Batch: 0.46233904863566616 Loss: 0.864113\n",
      "TRAIN: Batch: 0.46325638801787977 Loss: 1.8720251\n",
      "TRAIN: Batch: 0.4641737274000934 Loss: 3.32577\n",
      "TRAIN: Batch: 0.46509106678230705 Loss: 2.1108954\n",
      "TRAIN: Batch: 0.46600840616452066 Loss: 1.3648143\n",
      "TRAIN: Batch: 0.46692574554673427 Loss: 1.383534\n",
      "TRAIN: Batch: 0.4678430849289479 Loss: 0.56063724\n",
      "TRAIN: Batch: 0.4687604243111615 Loss: 1.9159856\n",
      "TRAIN: Batch: 0.46967776369337516 Loss: 1.4459366\n",
      "TRAIN: Batch: 0.47059510307558877 Loss: 0.928218\n",
      "TRAIN: Batch: 0.4715124424578024 Loss: 1.5200914\n",
      "TRAIN: Batch: 0.472429781840016 Loss: 3.9060335\n",
      "TRAIN: Batch: 0.47334712122222966 Loss: 0.9719602\n",
      "TRAIN: Batch: 0.47426446060444327 Loss: 1.3077285\n",
      "TRAIN: Batch: 0.4751817999866569 Loss: 2.0034971\n",
      "TRAIN: Batch: 0.4760991393688705 Loss: 1.7076206\n",
      "TRAIN: Batch: 0.4770164787510841 Loss: 3.7995315\n",
      "TRAIN: Batch: 0.47793381813329777 Loss: 0.4728841\n",
      "TRAIN: Batch: 0.4788511575155114 Loss: 10.574409\n",
      "TRAIN: Batch: 0.479768496897725 Loss: 0.5969081\n",
      "TRAIN: Batch: 0.4806858362799386 Loss: 1.9082246\n",
      "TRAIN: Batch: 0.48160317566215227 Loss: 1.1020592\n",
      "TRAIN: Batch: 0.4825205150443659 Loss: 0.9068612\n",
      "TRAIN: Batch: 0.4834378544265795 Loss: 0.80193055\n",
      "TRAIN: Batch: 0.4843551938087931 Loss: 0.7284756\n",
      "TRAIN: Batch: 0.4852725331910067 Loss: 0.7617731\n",
      "TRAIN: Batch: 0.4861898725732204 Loss: 1.9480107\n",
      "TRAIN: Batch: 0.487107211955434 Loss: 2.4991128\n",
      "TRAIN: Batch: 0.4880245513376476 Loss: 1.9019389\n",
      "TRAIN: Batch: 0.4889418907198612 Loss: 2.0263655\n",
      "TRAIN: Batch: 0.4898592301020748 Loss: 0.9309231\n",
      "TRAIN: Batch: 0.4907765694842885 Loss: 1.1221857\n",
      "TRAIN: Batch: 0.4916939088665021 Loss: 1.3588002\n",
      "TRAIN: Batch: 0.4926112482487157 Loss: 2.3770497\n",
      "TRAIN: Batch: 0.4935285876309293 Loss: 1.344611\n",
      "TRAIN: Batch: 0.494445927013143 Loss: 2.1123674\n",
      "TRAIN: Batch: 0.4953632663953566 Loss: 0.72928333\n",
      "TRAIN: Batch: 0.4962806057775702 Loss: 1.2916753\n",
      "TRAIN: Batch: 0.4971979451597838 Loss: 1.8430521\n",
      "TRAIN: Batch: 0.49811528454199744 Loss: 1.6766617\n",
      "TRAIN: Batch: 0.4990326239242111 Loss: 0.7099366\n",
      "TRAIN: Batch: 0.4999499633064247 Loss: 1.2400575\n",
      "TRAIN: Batch: 0.5008673026886383 Loss: 3.0571148\n",
      "TRAIN: Batch: 0.5017846420708519 Loss: 0.89005494\n",
      "TRAIN: Batch: 0.5027019814530655 Loss: 2.4689717\n",
      "TRAIN: Batch: 0.5036193208352792 Loss: 0.598619\n",
      "TRAIN: Batch: 0.5045366602174929 Loss: 1.7920122\n",
      "TRAIN: Batch: 0.5054539995997065 Loss: 1.810526\n",
      "TRAIN: Batch: 0.5063713389819201 Loss: 1.8480709\n",
      "TRAIN: Batch: 0.5072886783641337 Loss: 0.937218\n",
      "TRAIN: Batch: 0.5082060177463473 Loss: 1.1504252\n",
      "TRAIN: Batch: 0.5091233571285609 Loss: 3.6011295\n",
      "TRAIN: Batch: 0.5100406965107745 Loss: 2.3678613\n",
      "TRAIN: Batch: 0.5109580358929882 Loss: 1.355015\n",
      "TRAIN: Batch: 0.5118753752752018 Loss: 0.7301693\n",
      "TRAIN: Batch: 0.5127927146574154 Loss: 0.6698293\n",
      "TRAIN: Batch: 0.5137100540396291 Loss: 1.9679288\n",
      "TRAIN: Batch: 0.5146273934218427 Loss: 3.2959976\n",
      "TRAIN: Batch: 0.5155447328040563 Loss: 0.59244335\n",
      "TRAIN: Batch: 0.5164620721862699 Loss: 2.5657146\n",
      "TRAIN: Batch: 0.5173794115684835 Loss: 4.29912\n",
      "TRAIN: Batch: 0.5182967509506972 Loss: 3.3839393\n",
      "TRAIN: Batch: 0.5192140903329108 Loss: 0.64088595\n",
      "TRAIN: Batch: 0.5201314297151244 Loss: 0.93363005\n",
      "TRAIN: Batch: 0.521048769097338 Loss: 1.4125855\n",
      "TRAIN: Batch: 0.5219661084795517 Loss: 3.2936676\n",
      "TRAIN: Batch: 0.5228834478617653 Loss: 2.0828116\n",
      "TRAIN: Batch: 0.5238007872439789 Loss: 2.9752033\n",
      "TRAIN: Batch: 0.5247181266261925 Loss: 1.0308232\n",
      "TRAIN: Batch: 0.5256354660084062 Loss: 0.96693754\n",
      "TRAIN: Batch: 0.5265528053906198 Loss: 1.7969038\n",
      "TRAIN: Batch: 0.5274701447728334 Loss: 2.5885558\n",
      "TRAIN: Batch: 0.528387484155047 Loss: 2.2986293\n",
      "TRAIN: Batch: 0.5293048235372606 Loss: 2.6330242\n",
      "TRAIN: Batch: 0.5302221629194743 Loss: 1.217176\n",
      "TRAIN: Batch: 0.5311395023016879 Loss: 1.4119384\n",
      "TRAIN: Batch: 0.5320568416839015 Loss: 2.3935056\n",
      "TRAIN: Batch: 0.5329741810661152 Loss: 0.87608004\n",
      "TRAIN: Batch: 0.5338915204483288 Loss: 5.107338\n",
      "TRAIN: Batch: 0.5348088598305424 Loss: 0.603124\n",
      "TRAIN: Batch: 0.535726199212756 Loss: 2.208047\n",
      "TRAIN: Batch: 0.5366435385949696 Loss: 1.0944011\n",
      "TRAIN: Batch: 0.5375608779771832 Loss: 1.6406264\n",
      "TRAIN: Batch: 0.5384782173593969 Loss: 3.033147\n",
      "TRAIN: Batch: 0.5393955567416105 Loss: 0.7797049\n",
      "TRAIN: Batch: 0.5403128961238242 Loss: 3.1401167\n",
      "TRAIN: Batch: 0.5412302355060378 Loss: 1.1144233\n",
      "TRAIN: Batch: 0.5421475748882514 Loss: 0.7083446\n",
      "TRAIN: Batch: 0.543064914270465 Loss: 2.8313432\n",
      "TRAIN: Batch: 0.5439822536526786 Loss: 0.72984385\n",
      "TRAIN: Batch: 0.5448995930348922 Loss: 0.6209364\n",
      "TRAIN: Batch: 0.5458169324171058 Loss: 2.2687728\n",
      "TRAIN: Batch: 0.5467342717993195 Loss: 0.5254004\n",
      "TRAIN: Batch: 0.5476516111815332 Loss: 1.3099726\n",
      "TRAIN: Batch: 0.5485689505637468 Loss: 2.1162214\n",
      "TRAIN: Batch: 0.5494862899459604 Loss: 1.6698251\n",
      "TRAIN: Batch: 0.550403629328174 Loss: 1.2822242\n",
      "TRAIN: Batch: 0.5513209687103876 Loss: 1.5287787\n",
      "TRAIN: Batch: 0.5522383080926012 Loss: 1.4664085\n",
      "TRAIN: Batch: 0.5531556474748148 Loss: 1.0790547\n",
      "TRAIN: Batch: 0.5540729868570284 Loss: 2.5245852\n",
      "TRAIN: Batch: 0.5549903262392422 Loss: 1.974123\n",
      "TRAIN: Batch: 0.5559076656214558 Loss: 3.43475\n",
      "TRAIN: Batch: 0.5568250050036694 Loss: 1.7312586\n",
      "TRAIN: Batch: 0.557742344385883 Loss: 2.9665802\n",
      "TRAIN: Batch: 0.5586596837680966 Loss: 2.156644\n",
      "TRAIN: Batch: 0.5595770231503102 Loss: 0.97608125\n",
      "TRAIN: Batch: 0.5604943625325238 Loss: 2.5109515\n",
      "TRAIN: Batch: 0.5614117019147374 Loss: 1.7618173\n",
      "TRAIN: Batch: 0.562329041296951 Loss: 1.9226192\n",
      "TRAIN: Batch: 0.5632463806791648 Loss: 0.90786016\n",
      "TRAIN: Batch: 0.5641637200613784 Loss: 1.6178229\n",
      "TRAIN: Batch: 0.565081059443592 Loss: 3.4733553\n",
      "TRAIN: Batch: 0.5659983988258056 Loss: 0.8896973\n",
      "TRAIN: Batch: 0.5669157382080192 Loss: 0.89432245\n",
      "TRAIN: Batch: 0.5678330775902328 Loss: 1.8321614\n",
      "TRAIN: Batch: 0.5687504169724464 Loss: 1.6722865\n",
      "TRAIN: Batch: 0.56966775635466 Loss: 1.7123337\n",
      "TRAIN: Batch: 0.5705850957368737 Loss: 1.4621975\n",
      "TRAIN: Batch: 0.5715024351190874 Loss: 1.1090184\n",
      "TRAIN: Batch: 0.572419774501301 Loss: 2.6882133\n",
      "TRAIN: Batch: 0.5733371138835146 Loss: 1.5609828\n",
      "TRAIN: Batch: 0.5742544532657282 Loss: 1.417872\n",
      "TRAIN: Batch: 0.5751717926479418 Loss: 0.9893992\n",
      "TRAIN: Batch: 0.5760891320301554 Loss: 1.5393479\n",
      "TRAIN: Batch: 0.577006471412369 Loss: 1.5043237\n",
      "TRAIN: Batch: 0.5779238107945827 Loss: 2.3692324\n",
      "TRAIN: Batch: 0.5788411501767963 Loss: 0.718543\n",
      "TRAIN: Batch: 0.57975848955901 Loss: 0.50003266\n",
      "TRAIN: Batch: 0.5806758289412236 Loss: 0.7106929\n",
      "TRAIN: Batch: 0.5815931683234372 Loss: 2.7718298\n",
      "TRAIN: Batch: 0.5825105077056508 Loss: 1.2172987\n",
      "TRAIN: Batch: 0.5834278470878644 Loss: 0.6463457\n",
      "TRAIN: Batch: 0.584345186470078 Loss: 0.75662106\n",
      "TRAIN: Batch: 0.5852625258522917 Loss: 2.483025\n",
      "TRAIN: Batch: 0.5861798652345053 Loss: 0.8445085\n",
      "TRAIN: Batch: 0.5870972046167189 Loss: 2.1221313\n",
      "TRAIN: Batch: 0.5880145439989326 Loss: 5.3159194\n",
      "TRAIN: Batch: 0.5889318833811462 Loss: 2.1341875\n",
      "TRAIN: Batch: 0.5898492227633598 Loss: 3.2725704\n",
      "TRAIN: Batch: 0.5907665621455734 Loss: 1.2882556\n",
      "TRAIN: Batch: 0.591683901527787 Loss: 2.3516092\n",
      "TRAIN: Batch: 0.5926012409100007 Loss: 1.7843517\n",
      "TRAIN: Batch: 0.5935185802922143 Loss: 0.9492196\n",
      "TRAIN: Batch: 0.5944359196744279 Loss: 1.5256283\n",
      "TRAIN: Batch: 0.5953532590566415 Loss: 0.86205244\n",
      "TRAIN: Batch: 0.5962705984388552 Loss: 4.2118454\n",
      "TRAIN: Batch: 0.5971879378210688 Loss: 1.0121864\n",
      "TRAIN: Batch: 0.5981052772032824 Loss: 1.4173609\n",
      "TRAIN: Batch: 0.599022616585496 Loss: 0.49032968\n",
      "TRAIN: Batch: 0.5999399559677097 Loss: 0.905869\n",
      "TRAIN: Batch: 0.6008572953499233 Loss: 4.032126\n",
      "TRAIN: Batch: 0.6017746347321369 Loss: 2.0291524\n",
      "TRAIN: Batch: 0.6026919741143505 Loss: 2.271104\n",
      "TRAIN: Batch: 0.6036093134965641 Loss: 0.6408412\n",
      "TRAIN: Batch: 0.6045266528787777 Loss: 4.9413056\n",
      "TRAIN: Batch: 0.6054439922609914 Loss: 3.7111697\n",
      "TRAIN: Batch: 0.606361331643205 Loss: 0.66227716\n",
      "TRAIN: Batch: 0.6072786710254187 Loss: 1.1475488\n",
      "TRAIN: Batch: 0.6081960104076323 Loss: 1.1463737\n",
      "TRAIN: Batch: 0.6091133497898459 Loss: 0.62979823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.6100306891720595 Loss: 1.7196605\n",
      "TRAIN: Batch: 0.6109480285542731 Loss: 0.7455338\n",
      "TRAIN: Batch: 0.6118653679364867 Loss: 3.489612\n",
      "TRAIN: Batch: 0.6127827073187003 Loss: 0.6969068\n",
      "TRAIN: Batch: 0.613700046700914 Loss: 5.4409113\n",
      "TRAIN: Batch: 0.6146173860831277 Loss: 1.3505192\n",
      "TRAIN: Batch: 0.6155347254653413 Loss: 1.159929\n",
      "TRAIN: Batch: 0.6164520648475549 Loss: 1.5963837\n",
      "TRAIN: Batch: 0.6173694042297685 Loss: 5.1864676\n",
      "TRAIN: Batch: 0.6182867436119821 Loss: 0.99581885\n",
      "TRAIN: Batch: 0.6192040829941957 Loss: 1.2190005\n",
      "TRAIN: Batch: 0.6201214223764093 Loss: 0.47767305\n",
      "TRAIN: Batch: 0.6210387617586229 Loss: 1.0454414\n",
      "TRAIN: Batch: 0.6219561011408367 Loss: 1.1475395\n",
      "TRAIN: Batch: 0.6228734405230503 Loss: 1.7969714\n",
      "TRAIN: Batch: 0.6237907799052639 Loss: 2.8236897\n",
      "TRAIN: Batch: 0.6247081192874775 Loss: 0.50134206\n",
      "TRAIN: Batch: 0.6256254586696911 Loss: 0.51349103\n",
      "TRAIN: Batch: 0.6265427980519047 Loss: 1.3459678\n",
      "TRAIN: Batch: 0.6274601374341183 Loss: 2.955493\n",
      "TRAIN: Batch: 0.6283774768163319 Loss: 2.4686637\n",
      "TRAIN: Batch: 0.6292948161985455 Loss: 2.1344972\n",
      "TRAIN: Batch: 0.6302121555807593 Loss: 1.1435504\n",
      "TRAIN: Batch: 0.6311294949629729 Loss: 3.4917116\n",
      "TRAIN: Batch: 0.6320468343451865 Loss: 2.6895127\n",
      "TRAIN: Batch: 0.6329641737274001 Loss: 2.0906801\n",
      "TRAIN: Batch: 0.6338815131096137 Loss: 1.2013589\n",
      "TRAIN: Batch: 0.6347988524918273 Loss: 1.3922951\n",
      "TRAIN: Batch: 0.6357161918740409 Loss: 3.0194843\n",
      "TRAIN: Batch: 0.6366335312562545 Loss: 1.5825241\n",
      "TRAIN: Batch: 0.6375508706384682 Loss: 2.2797592\n",
      "TRAIN: Batch: 0.6384682100206819 Loss: 1.1917968\n",
      "TRAIN: Batch: 0.6393855494028955 Loss: 1.1356589\n",
      "TRAIN: Batch: 0.6403028887851091 Loss: 0.81092\n",
      "TRAIN: Batch: 0.6412202281673227 Loss: 2.7331357\n",
      "TRAIN: Batch: 0.6421375675495363 Loss: 2.4035962\n",
      "TRAIN: Batch: 0.6430549069317499 Loss: 3.8246663\n",
      "TRAIN: Batch: 0.6439722463139635 Loss: 4.620691\n",
      "TRAIN: Batch: 0.6448895856961772 Loss: 1.2561066\n",
      "TRAIN: Batch: 0.6458069250783908 Loss: 0.6185266\n",
      "TRAIN: Batch: 0.6467242644606045 Loss: 1.03656\n",
      "TRAIN: Batch: 0.6476416038428181 Loss: 0.49652782\n",
      "TRAIN: Batch: 0.6485589432250317 Loss: 2.5041335\n",
      "TRAIN: Batch: 0.6494762826072453 Loss: 2.4431221\n",
      "TRAIN: Batch: 0.6503936219894589 Loss: 2.5989904\n",
      "TRAIN: Batch: 0.6513109613716725 Loss: 3.5094388\n",
      "TRAIN: Batch: 0.6522283007538862 Loss: 0.6402601\n",
      "TRAIN: Batch: 0.6531456401360998 Loss: 1.5619483\n",
      "TRAIN: Batch: 0.6540629795183134 Loss: 1.5869758\n",
      "TRAIN: Batch: 0.6549803189005271 Loss: 0.6342138\n",
      "TRAIN: Batch: 0.6558976582827407 Loss: 1.3550968\n",
      "TRAIN: Batch: 0.6568149976649543 Loss: 1.6517848\n",
      "TRAIN: Batch: 0.6577323370471679 Loss: 2.0969365\n",
      "TRAIN: Batch: 0.6586496764293815 Loss: 4.853838\n",
      "TRAIN: Batch: 0.6595670158115952 Loss: 4.65971\n",
      "TRAIN: Batch: 0.6604843551938088 Loss: 1.2621195\n",
      "TRAIN: Batch: 0.6614016945760224 Loss: 0.50807863\n",
      "TRAIN: Batch: 0.662319033958236 Loss: 0.51903117\n",
      "TRAIN: Batch: 0.6632363733404497 Loss: 1.6361165\n",
      "TRAIN: Batch: 0.6641537127226633 Loss: 1.5934625\n",
      "TRAIN: Batch: 0.6650710521048769 Loss: 0.63519657\n",
      "TRAIN: Batch: 0.6659883914870905 Loss: 0.72285056\n",
      "TRAIN: Batch: 0.6669057308693042 Loss: 3.2398465\n",
      "TRAIN: Batch: 0.6678230702515178 Loss: 1.816412\n",
      "TRAIN: Batch: 0.6687404096337314 Loss: 1.9628623\n",
      "TRAIN: Batch: 0.669657749015945 Loss: 1.648391\n",
      "TRAIN: Batch: 0.6705750883981586 Loss: 1.3324604\n",
      "TRAIN: Batch: 0.6714924277803723 Loss: 0.7321043\n",
      "TRAIN: Batch: 0.6724097671625859 Loss: 1.1006849\n",
      "TRAIN: Batch: 0.6733271065447995 Loss: 0.7052711\n",
      "TRAIN: Batch: 0.6742444459270132 Loss: 2.0121174\n",
      "TRAIN: Batch: 0.6751617853092268 Loss: 3.9195616\n",
      "TRAIN: Batch: 0.6760791246914404 Loss: 2.8872075\n",
      "TRAIN: Batch: 0.676996464073654 Loss: 1.9708238\n",
      "TRAIN: Batch: 0.6779138034558676 Loss: 0.71335894\n",
      "TRAIN: Batch: 0.6788311428380812 Loss: 1.3275762\n",
      "TRAIN: Batch: 0.6797484822202949 Loss: 2.4145172\n",
      "TRAIN: Batch: 0.6806658216025085 Loss: 3.13296\n",
      "TRAIN: Batch: 0.6815831609847222 Loss: 1.1737872\n",
      "TRAIN: Batch: 0.6825005003669358 Loss: 1.417016\n",
      "TRAIN: Batch: 0.6834178397491494 Loss: 0.7040738\n",
      "TRAIN: Batch: 0.684335179131363 Loss: 2.1488812\n",
      "TRAIN: Batch: 0.6852525185135766 Loss: 2.5198605\n",
      "TRAIN: Batch: 0.6861698578957902 Loss: 0.737147\n",
      "TRAIN: Batch: 0.6870871972780038 Loss: 0.74143106\n",
      "TRAIN: Batch: 0.6880045366602175 Loss: 3.971569\n",
      "TRAIN: Batch: 0.6889218760424312 Loss: 3.0697098\n",
      "TRAIN: Batch: 0.6898392154246448 Loss: 1.6648415\n",
      "TRAIN: Batch: 0.6907565548068584 Loss: 0.5240345\n",
      "TRAIN: Batch: 0.691673894189072 Loss: 0.5088098\n",
      "TRAIN: Batch: 0.6925912335712856 Loss: 0.63062215\n",
      "TRAIN: Batch: 0.6935085729534992 Loss: 2.4205046\n",
      "TRAIN: Batch: 0.6944259123357128 Loss: 1.1509042\n",
      "TRAIN: Batch: 0.6953432517179264 Loss: 0.7660339\n",
      "TRAIN: Batch: 0.6962605911001402 Loss: 2.443302\n",
      "TRAIN: Batch: 0.6971779304823538 Loss: 0.5454786\n",
      "TRAIN: Batch: 0.6980952698645674 Loss: 2.1011767\n",
      "TRAIN: Batch: 0.699012609246781 Loss: 0.5416178\n",
      "TRAIN: Batch: 0.6999299486289946 Loss: 4.225424\n",
      "TRAIN: Batch: 0.7008472880112082 Loss: 1.0527245\n",
      "TRAIN: Batch: 0.7017646273934218 Loss: 0.53607917\n",
      "TRAIN: Batch: 0.7026819667756354 Loss: 1.0467885\n",
      "TRAIN: Batch: 0.703599306157849 Loss: 1.4774973\n",
      "TRAIN: Batch: 0.7045166455400627 Loss: 0.5183376\n",
      "TRAIN: Batch: 0.7054339849222764 Loss: 1.3887544\n",
      "TRAIN: Batch: 0.70635132430449 Loss: 0.8659043\n",
      "TRAIN: Batch: 0.7072686636867036 Loss: 0.8091799\n",
      "TRAIN: Batch: 0.7081860030689172 Loss: 1.0921924\n",
      "TRAIN: Batch: 0.7091033424511308 Loss: 1.6354331\n",
      "TRAIN: Batch: 0.7100206818333444 Loss: 2.8622932\n",
      "TRAIN: Batch: 0.710938021215558 Loss: 0.7655087\n",
      "TRAIN: Batch: 0.7118553605977717 Loss: 2.7266252\n",
      "TRAIN: Batch: 0.7127726999799853 Loss: 3.0244093\n",
      "TRAIN: Batch: 0.713690039362199 Loss: 1.1908323\n",
      "TRAIN: Batch: 0.7146073787444126 Loss: 1.040707\n",
      "TRAIN: Batch: 0.7155247181266262 Loss: 1.0404797\n",
      "TRAIN: Batch: 0.7164420575088398 Loss: 1.4883013\n",
      "TRAIN: Batch: 0.7173593968910534 Loss: 2.0848308\n",
      "TRAIN: Batch: 0.718276736273267 Loss: 0.54030824\n",
      "TRAIN: Batch: 0.7191940756554807 Loss: 0.6332233\n",
      "TRAIN: Batch: 0.7201114150376943 Loss: 0.976085\n",
      "TRAIN: Batch: 0.7210287544199079 Loss: 1.1126673\n",
      "TRAIN: Batch: 0.7219460938021216 Loss: 0.97224176\n",
      "TRAIN: Batch: 0.7228634331843352 Loss: 6.284559\n",
      "TRAIN: Batch: 0.7237807725665488 Loss: 3.4652736\n",
      "TRAIN: Batch: 0.7246981119487624 Loss: 1.8902421\n",
      "TRAIN: Batch: 0.725615451330976 Loss: 1.6149756\n",
      "TRAIN: Batch: 0.7265327907131897 Loss: 1.197508\n",
      "TRAIN: Batch: 0.7274501300954033 Loss: 4.35582\n",
      "TRAIN: Batch: 0.7283674694776169 Loss: 4.945513\n",
      "TRAIN: Batch: 0.7292848088598305 Loss: 1.6748023\n",
      "TRAIN: Batch: 0.7302021482420442 Loss: 0.8097462\n",
      "TRAIN: Batch: 0.7311194876242578 Loss: 1.0660024\n",
      "TRAIN: Batch: 0.7320368270064714 Loss: 0.9530399\n",
      "TRAIN: Batch: 0.732954166388685 Loss: 3.4157844\n",
      "TRAIN: Batch: 0.7338715057708987 Loss: 1.0589297\n",
      "TRAIN: Batch: 0.7347888451531123 Loss: 1.0448043\n",
      "TRAIN: Batch: 0.7357061845353259 Loss: 4.2199945\n",
      "TRAIN: Batch: 0.7366235239175395 Loss: 0.8638428\n",
      "TRAIN: Batch: 0.7375408632997531 Loss: 1.3027842\n",
      "TRAIN: Batch: 0.7384582026819668 Loss: 2.6811614\n",
      "TRAIN: Batch: 0.7393755420641804 Loss: 1.1124421\n",
      "TRAIN: Batch: 0.740292881446394 Loss: 2.4321718\n",
      "TRAIN: Batch: 0.7412102208286077 Loss: 0.83742213\n",
      "TRAIN: Batch: 0.7421275602108213 Loss: 0.9407693\n",
      "TRAIN: Batch: 0.7430448995930349 Loss: 4.3949265\n",
      "TRAIN: Batch: 0.7439622389752485 Loss: 1.078263\n",
      "TRAIN: Batch: 0.7448795783574621 Loss: 0.99246514\n",
      "TRAIN: Batch: 0.7457969177396757 Loss: 0.69377875\n",
      "TRAIN: Batch: 0.7467142571218894 Loss: 1.0894082\n",
      "TRAIN: Batch: 0.747631596504103 Loss: 4.527318\n",
      "TRAIN: Batch: 0.7485489358863167 Loss: 0.7308074\n",
      "TRAIN: Batch: 0.7494662752685303 Loss: 1.15796\n",
      "TRAIN: Batch: 0.7503836146507439 Loss: 0.877491\n",
      "TRAIN: Batch: 0.7513009540329575 Loss: 1.2130724\n",
      "TRAIN: Batch: 0.7522182934151711 Loss: 2.7518985\n",
      "TRAIN: Batch: 0.7531356327973847 Loss: 0.83249176\n",
      "TRAIN: Batch: 0.7540529721795983 Loss: 3.480158\n",
      "TRAIN: Batch: 0.754970311561812 Loss: 0.93779826\n",
      "TRAIN: Batch: 0.7558876509440257 Loss: 1.7833136\n",
      "TRAIN: Batch: 0.7568049903262393 Loss: 1.0405703\n",
      "TRAIN: Batch: 0.7577223297084529 Loss: 0.9429899\n",
      "TRAIN: Batch: 0.7586396690906665 Loss: 1.5743531\n",
      "TRAIN: Batch: 0.7595570084728801 Loss: 0.7044655\n",
      "TRAIN: Batch: 0.7604743478550937 Loss: 0.4650845\n",
      "TRAIN: Batch: 0.7613916872373073 Loss: 1.5434818\n",
      "TRAIN: Batch: 0.7623090266195209 Loss: 0.8863193\n",
      "TRAIN: Batch: 0.7632263660017347 Loss: 1.511467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.7641437053839483 Loss: 1.1334157\n",
      "TRAIN: Batch: 0.7650610447661619 Loss: 0.7205365\n",
      "TRAIN: Batch: 0.7659783841483755 Loss: 2.5978806\n",
      "TRAIN: Batch: 0.7668957235305891 Loss: 1.0127517\n",
      "TRAIN: Batch: 0.7678130629128027 Loss: 2.6435628\n",
      "TRAIN: Batch: 0.7687304022950163 Loss: 1.1078978\n",
      "TRAIN: Batch: 0.7696477416772299 Loss: 2.1996968\n",
      "TRAIN: Batch: 0.7705650810594435 Loss: 4.410697\n",
      "TRAIN: Batch: 0.7714824204416573 Loss: 2.5712492\n",
      "TRAIN: Batch: 0.7723997598238709 Loss: 2.0725024\n",
      "TRAIN: Batch: 0.7733170992060845 Loss: 1.2178192\n",
      "TRAIN: Batch: 0.7742344385882981 Loss: 0.9598355\n",
      "TRAIN: Batch: 0.7751517779705117 Loss: 3.1368487\n",
      "TRAIN: Batch: 0.7760691173527253 Loss: 2.8742464\n",
      "TRAIN: Batch: 0.7769864567349389 Loss: 1.329114\n",
      "TRAIN: Batch: 0.7779037961171525 Loss: 1.4592723\n",
      "TRAIN: Batch: 0.7788211354993662 Loss: 0.59706575\n",
      "TRAIN: Batch: 0.7797384748815799 Loss: 0.50242144\n",
      "TRAIN: Batch: 0.7806558142637935 Loss: 1.2638324\n",
      "TRAIN: Batch: 0.7815731536460071 Loss: 0.7181313\n",
      "TRAIN: Batch: 0.7824904930282207 Loss: 0.8126316\n",
      "TRAIN: Batch: 0.7834078324104343 Loss: 1.0743325\n",
      "TRAIN: Batch: 0.7843251717926479 Loss: 1.0924749\n",
      "TRAIN: Batch: 0.7852425111748615 Loss: 2.5724564\n",
      "TRAIN: Batch: 0.7861598505570752 Loss: 0.6124904\n",
      "TRAIN: Batch: 0.7870771899392888 Loss: 3.491252\n",
      "TRAIN: Batch: 0.7879945293215025 Loss: 1.2123598\n",
      "TRAIN: Batch: 0.7889118687037161 Loss: 1.2016317\n",
      "TRAIN: Batch: 0.7898292080859297 Loss: 7.63972\n",
      "TRAIN: Batch: 0.7907465474681433 Loss: 0.7164247\n",
      "TRAIN: Batch: 0.7916638868503569 Loss: 1.5988176\n",
      "TRAIN: Batch: 0.7925812262325705 Loss: 2.4597666\n",
      "TRAIN: Batch: 0.7934985656147842 Loss: 0.8558885\n",
      "TRAIN: Batch: 0.7944159049969978 Loss: 1.540597\n",
      "TRAIN: Batch: 0.7953332443792114 Loss: 2.6490884\n",
      "TRAIN: Batch: 0.796250583761425 Loss: 0.4808917\n",
      "TRAIN: Batch: 0.7971679231436387 Loss: 1.2656549\n",
      "TRAIN: Batch: 0.7980852625258523 Loss: 1.8850124\n",
      "TRAIN: Batch: 0.7990026019080659 Loss: 1.4581933\n",
      "TRAIN: Batch: 0.7999199412902795 Loss: 2.8038108\n",
      "TRAIN: Batch: 0.8008372806724932 Loss: 2.0192306\n",
      "TRAIN: Batch: 0.8017546200547068 Loss: 1.9610816\n",
      "TRAIN: Batch: 0.8026719594369204 Loss: 1.1366286\n",
      "TRAIN: Batch: 0.803589298819134 Loss: 0.5964849\n",
      "TRAIN: Batch: 0.8045066382013476 Loss: 0.59542423\n",
      "TRAIN: Batch: 0.8054239775835613 Loss: 4.795983\n",
      "TRAIN: Batch: 0.8063413169657749 Loss: 0.5306992\n",
      "TRAIN: Batch: 0.8072586563479885 Loss: 2.516159\n",
      "TRAIN: Batch: 0.8081759957302022 Loss: 1.1916394\n",
      "TRAIN: Batch: 0.8090933351124158 Loss: 0.6640681\n",
      "TRAIN: Batch: 0.8100106744946294 Loss: 1.7950634\n",
      "TRAIN: Batch: 0.810928013876843 Loss: 0.57640076\n",
      "TRAIN: Batch: 0.8118453532590566 Loss: 1.7462895\n",
      "TRAIN: Batch: 0.8127626926412702 Loss: 2.549118\n",
      "TRAIN: Batch: 0.8136800320234839 Loss: 1.0008521\n",
      "TRAIN: Batch: 0.8145973714056975 Loss: 2.8711305\n",
      "TRAIN: Batch: 0.8155147107879112 Loss: 4.258266\n",
      "TRAIN: Batch: 0.8164320501701248 Loss: 0.72724855\n",
      "TRAIN: Batch: 0.8173493895523384 Loss: 2.7081962\n",
      "TRAIN: Batch: 0.818266728934552 Loss: 3.9201262\n",
      "TRAIN: Batch: 0.8191840683167656 Loss: 1.856913\n",
      "TRAIN: Batch: 0.8201014076989792 Loss: 2.2993999\n",
      "TRAIN: Batch: 0.8210187470811928 Loss: 1.5584135\n",
      "TRAIN: Batch: 0.8219360864634065 Loss: 0.92748\n",
      "TRAIN: Batch: 0.8228534258456202 Loss: 0.5652681\n",
      "TRAIN: Batch: 0.8237707652278338 Loss: 1.71263\n",
      "TRAIN: Batch: 0.8246881046100474 Loss: 8.392522\n",
      "TRAIN: Batch: 0.825605443992261 Loss: 1.099915\n",
      "TRAIN: Batch: 0.8265227833744746 Loss: 5.433513\n",
      "TRAIN: Batch: 0.8274401227566882 Loss: 4.4310884\n",
      "TRAIN: Batch: 0.8283574621389018 Loss: 0.53739697\n",
      "TRAIN: Batch: 0.8292748015211154 Loss: 2.1406064\n",
      "TRAIN: Batch: 0.8301921409033292 Loss: 1.7116263\n",
      "TRAIN: Batch: 0.8311094802855428 Loss: 2.311488\n",
      "TRAIN: Batch: 0.8320268196677564 Loss: 2.0260916\n",
      "TRAIN: Batch: 0.83294415904997 Loss: 0.5003951\n",
      "TRAIN: Batch: 0.8338614984321836 Loss: 1.8557087\n",
      "TRAIN: Batch: 0.8347788378143972 Loss: 1.0280256\n",
      "TRAIN: Batch: 0.8356961771966108 Loss: 0.5089923\n",
      "TRAIN: Batch: 0.8366135165788244 Loss: 2.3346725\n",
      "TRAIN: Batch: 0.837530855961038 Loss: 1.5787055\n",
      "TRAIN: Batch: 0.8384481953432518 Loss: 2.2472222\n",
      "TRAIN: Batch: 0.8393655347254654 Loss: 1.3519007\n",
      "TRAIN: Batch: 0.840282874107679 Loss: 3.7207036\n",
      "TRAIN: Batch: 0.8412002134898926 Loss: 1.5268188\n",
      "TRAIN: Batch: 0.8421175528721062 Loss: 1.4878802\n",
      "TRAIN: Batch: 0.8430348922543198 Loss: 1.3919674\n",
      "TRAIN: Batch: 0.8439522316365334 Loss: 0.6427313\n",
      "TRAIN: Batch: 0.844869571018747 Loss: 1.1611428\n",
      "TRAIN: Batch: 0.8457869104009607 Loss: 4.252573\n",
      "TRAIN: Batch: 0.8467042497831744 Loss: 1.8194116\n",
      "TRAIN: Batch: 0.847621589165388 Loss: 1.1622252\n",
      "TRAIN: Batch: 0.8485389285476016 Loss: 1.9377176\n",
      "TRAIN: Batch: 0.8494562679298152 Loss: 3.1667924\n",
      "TRAIN: Batch: 0.8503736073120288 Loss: 1.5865792\n",
      "TRAIN: Batch: 0.8512909466942424 Loss: 2.121787\n",
      "TRAIN: Batch: 0.852208286076456 Loss: 1.457226\n",
      "TRAIN: Batch: 0.8531256254586697 Loss: 1.5606459\n",
      "TRAIN: Batch: 0.8540429648408833 Loss: 6.7871523\n",
      "TRAIN: Batch: 0.854960304223097 Loss: 0.5651086\n",
      "TRAIN: Batch: 0.8558776436053106 Loss: 2.0825624\n",
      "TRAIN: Batch: 0.8567949829875242 Loss: 1.002\n",
      "TRAIN: Batch: 0.8577123223697378 Loss: 0.992905\n",
      "TRAIN: Batch: 0.8586296617519514 Loss: 3.319404\n",
      "TRAIN: Batch: 0.859547001134165 Loss: 2.3993473\n",
      "TRAIN: Batch: 0.8604643405163787 Loss: 0.8623247\n",
      "TRAIN: Batch: 0.8613816798985923 Loss: 0.87419975\n",
      "TRAIN: Batch: 0.8622990192808059 Loss: 2.4546518\n",
      "TRAIN: Batch: 0.8632163586630196 Loss: 0.99220693\n",
      "TRAIN: Batch: 0.8641336980452332 Loss: 1.3177756\n",
      "TRAIN: Batch: 0.8650510374274468 Loss: 0.5849748\n",
      "TRAIN: Batch: 0.8659683768096604 Loss: 2.1743364\n",
      "TRAIN: Batch: 0.866885716191874 Loss: 3.9882972\n",
      "TRAIN: Batch: 0.8678030555740877 Loss: 0.61370325\n",
      "TRAIN: Batch: 0.8687203949563013 Loss: 1.1870173\n",
      "TRAIN: Batch: 0.8696377343385149 Loss: 2.4730592\n",
      "TRAIN: Batch: 0.8705550737207285 Loss: 0.6574669\n",
      "TRAIN: Batch: 0.8714724131029422 Loss: 0.7802655\n",
      "TRAIN: Batch: 0.8723897524851558 Loss: 1.7913616\n",
      "TRAIN: Batch: 0.8733070918673694 Loss: 1.4727916\n",
      "TRAIN: Batch: 0.874224431249583 Loss: 2.4477563\n",
      "TRAIN: Batch: 0.8751417706317967 Loss: 2.7555275\n",
      "TRAIN: Batch: 0.8760591100140103 Loss: 2.6681633\n",
      "TRAIN: Batch: 0.8769764493962239 Loss: 1.021196\n",
      "TRAIN: Batch: 0.8778937887784375 Loss: 4.1085105\n",
      "TRAIN: Batch: 0.8788111281606511 Loss: 1.3355068\n",
      "TRAIN: Batch: 0.8797284675428648 Loss: 1.0925086\n",
      "TRAIN: Batch: 0.8806458069250784 Loss: 0.5321062\n",
      "TRAIN: Batch: 0.881563146307292 Loss: 0.59495276\n",
      "TRAIN: Batch: 0.8824804856895057 Loss: 2.4087048\n",
      "TRAIN: Batch: 0.8833978250717193 Loss: 3.1706328\n",
      "TRAIN: Batch: 0.8843151644539329 Loss: 1.5853963\n",
      "TRAIN: Batch: 0.8852325038361465 Loss: 0.47124913\n",
      "TRAIN: Batch: 0.8861498432183601 Loss: 2.4842398\n",
      "TRAIN: Batch: 0.8870671826005737 Loss: 2.228739\n",
      "TRAIN: Batch: 0.8879845219827873 Loss: 0.86972415\n",
      "TRAIN: Batch: 0.888901861365001 Loss: 0.6657706\n",
      "TRAIN: Batch: 0.8898192007472147 Loss: 2.0029771\n",
      "TRAIN: Batch: 0.8907365401294283 Loss: 0.8413728\n",
      "TRAIN: Batch: 0.8916538795116419 Loss: 1.2915971\n",
      "TRAIN: Batch: 0.8925712188938555 Loss: 1.1105982\n",
      "TRAIN: Batch: 0.8934885582760691 Loss: 1.8767784\n",
      "TRAIN: Batch: 0.8944058976582827 Loss: 0.96984947\n",
      "TRAIN: Batch: 0.8953232370404963 Loss: 2.6786137\n",
      "TRAIN: Batch: 0.8962405764227099 Loss: 3.6560757\n",
      "TRAIN: Batch: 0.8971579158049237 Loss: 1.3612034\n",
      "TRAIN: Batch: 0.8980752551871373 Loss: 5.481506\n",
      "TRAIN: Batch: 0.8989925945693509 Loss: 1.3245661\n",
      "TRAIN: Batch: 0.8999099339515645 Loss: 0.6864662\n",
      "TRAIN: Batch: 0.9008272733337781 Loss: 1.5867852\n",
      "TRAIN: Batch: 0.9017446127159917 Loss: 1.0230693\n",
      "TRAIN: Batch: 0.9026619520982053 Loss: 0.9747984\n",
      "TRAIN: Batch: 0.9035792914804189 Loss: 0.48583794\n",
      "TRAIN: Batch: 0.9044966308626325 Loss: 8.199246\n",
      "TRAIN: Batch: 0.9054139702448463 Loss: 1.8745348\n",
      "TRAIN: Batch: 0.9063313096270599 Loss: 0.64717627\n",
      "TRAIN: Batch: 0.9072486490092735 Loss: 0.61659455\n",
      "TRAIN: Batch: 0.9081659883914871 Loss: 1.7535884\n",
      "TRAIN: Batch: 0.9090833277737007 Loss: 1.2565197\n",
      "TRAIN: Batch: 0.9100006671559143 Loss: 5.674911\n",
      "TRAIN: Batch: 0.9109180065381279 Loss: 3.0396757\n",
      "TRAIN: Batch: 0.9118353459203415 Loss: 2.1375515\n",
      "TRAIN: Batch: 0.9127526853025552 Loss: 1.0019189\n",
      "TRAIN: Batch: 0.9136700246847689 Loss: 3.8841445\n",
      "TRAIN: Batch: 0.9145873640669825 Loss: 0.96236396\n",
      "TRAIN: Batch: 0.9155047034491961 Loss: 2.7437253\n",
      "TRAIN: Batch: 0.9164220428314097 Loss: 2.0840518\n",
      "TRAIN: Batch: 0.9173393822136233 Loss: 1.2556689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.9182567215958369 Loss: 0.9672139\n",
      "TRAIN: Batch: 0.9191740609780505 Loss: 1.9933009\n",
      "TRAIN: Batch: 0.9200914003602642 Loss: 0.6189116\n",
      "TRAIN: Batch: 0.9210087397424778 Loss: 2.7362273\n",
      "TRAIN: Batch: 0.9219260791246915 Loss: 1.036169\n",
      "TRAIN: Batch: 0.9228434185069051 Loss: 1.3203312\n",
      "TRAIN: Batch: 0.9237607578891187 Loss: 1.8476565\n",
      "TRAIN: Batch: 0.9246780972713323 Loss: 1.1896756\n",
      "TRAIN: Batch: 0.9255954366535459 Loss: 0.4878193\n",
      "TRAIN: Batch: 0.9265127760357595 Loss: 1.105065\n",
      "TRAIN: Batch: 0.9274301154179732 Loss: 1.2363276\n",
      "TRAIN: Batch: 0.9283474548001868 Loss: 2.7795444\n",
      "TRAIN: Batch: 0.9292647941824004 Loss: 0.9780661\n",
      "TRAIN: Batch: 0.9301821335646141 Loss: 0.54071957\n",
      "TRAIN: Batch: 0.9310994729468277 Loss: 2.9277706\n",
      "TRAIN: Batch: 0.9320168123290413 Loss: 0.94753045\n",
      "TRAIN: Batch: 0.9329341517112549 Loss: 0.6961124\n",
      "TRAIN: Batch: 0.9338514910934685 Loss: 0.83467853\n",
      "TRAIN: Batch: 0.9347688304756822 Loss: 2.3197947\n",
      "TRAIN: Batch: 0.9356861698578958 Loss: 2.685305\n",
      "TRAIN: Batch: 0.9366035092401094 Loss: 2.1847515\n",
      "TRAIN: Batch: 0.937520848622323 Loss: 3.1050153\n",
      "TRAIN: Batch: 0.9384381880045367 Loss: 2.4195864\n",
      "TRAIN: Batch: 0.9393555273867503 Loss: 3.1398048\n",
      "TRAIN: Batch: 0.9402728667689639 Loss: 0.89933956\n",
      "TRAIN: Batch: 0.9411902061511775 Loss: 1.2844052\n",
      "TRAIN: Batch: 0.9421075455333912 Loss: 0.4774206\n",
      "TRAIN: Batch: 0.9430248849156048 Loss: 1.7133052\n",
      "TRAIN: Batch: 0.9439422242978184 Loss: 0.4709871\n",
      "TRAIN: Batch: 0.944859563680032 Loss: 1.236478\n",
      "TRAIN: Batch: 0.9457769030622456 Loss: 0.5116324\n",
      "TRAIN: Batch: 0.9466942424444593 Loss: 0.8449281\n",
      "TRAIN: Batch: 0.9476115818266729 Loss: 1.2136081\n",
      "TRAIN: Batch: 0.9485289212088865 Loss: 4.3081393\n",
      "TRAIN: Batch: 0.9494462605911002 Loss: 1.6757157\n",
      "TRAIN: Batch: 0.9503635999733138 Loss: 1.708493\n",
      "TRAIN: Batch: 0.9512809393555274 Loss: 1.8018966\n",
      "TRAIN: Batch: 0.952198278737741 Loss: 0.9645994\n",
      "TRAIN: Batch: 0.9531156181199546 Loss: 2.9641347\n",
      "TRAIN: Batch: 0.9540329575021682 Loss: 1.7642756\n",
      "TRAIN: Batch: 0.9549502968843819 Loss: 1.1511797\n",
      "TRAIN: Batch: 0.9558676362665955 Loss: 0.8945211\n",
      "TRAIN: Batch: 0.9567849756488092 Loss: 0.7715962\n",
      "TRAIN: Batch: 0.9577023150310228 Loss: 0.8971784\n",
      "TRAIN: Batch: 0.9586196544132364 Loss: 0.57861596\n",
      "TRAIN: Batch: 0.95953699379545 Loss: 1.8939022\n",
      "TRAIN: Batch: 0.9604543331776636 Loss: 8.934281\n",
      "TRAIN: Batch: 0.9613716725598772 Loss: 1.1228391\n",
      "TRAIN: Batch: 0.9622890119420908 Loss: 0.64001334\n",
      "TRAIN: Batch: 0.9632063513243045 Loss: 1.2709248\n",
      "TRAIN: Batch: 0.9641236907065182 Loss: 0.53988576\n",
      "TRAIN: Batch: 0.9650410300887318 Loss: 2.2319043\n",
      "TRAIN: Batch: 0.9659583694709454 Loss: 7.082217\n",
      "TRAIN: Batch: 0.966875708853159 Loss: 1.1825786\n",
      "TRAIN: Batch: 0.9677930482353726 Loss: 1.7110792\n",
      "TRAIN: Batch: 0.9687103876175862 Loss: 0.9011253\n",
      "TRAIN: Batch: 0.9696277269997998 Loss: 0.9720416\n",
      "TRAIN: Batch: 0.9705450663820134 Loss: 0.9941026\n",
      "TRAIN: Batch: 0.9714624057642272 Loss: 0.52015406\n",
      "TRAIN: Batch: 0.9723797451464408 Loss: 1.7420527\n",
      "TRAIN: Batch: 0.9732970845286544 Loss: 1.2788451\n",
      "TRAIN: Batch: 0.974214423910868 Loss: 3.1842558\n",
      "TRAIN: Batch: 0.9751317632930816 Loss: 0.8763703\n",
      "TRAIN: Batch: 0.9760491026752952 Loss: 1.1252992\n",
      "TRAIN: Batch: 0.9769664420575088 Loss: 0.68809897\n",
      "TRAIN: Batch: 0.9778837814397224 Loss: 1.3124506\n",
      "TRAIN: Batch: 0.978801120821936 Loss: 0.8202747\n",
      "TRAIN: Batch: 0.9797184602041497 Loss: 1.0725592\n",
      "TRAIN: Batch: 0.9806357995863634 Loss: 2.2261782\n",
      "TRAIN: Batch: 0.981553138968577 Loss: 1.0297072\n",
      "TRAIN: Batch: 0.9824704783507906 Loss: 2.47533\n",
      "TRAIN: Batch: 0.9833878177330042 Loss: 2.5330524\n",
      "TRAIN: Batch: 0.9843051571152178 Loss: 2.563803\n",
      "TRAIN: Batch: 0.9852224964974314 Loss: 1.1946375\n",
      "TRAIN: Batch: 0.986139835879645 Loss: 1.6445678\n",
      "TRAIN: Batch: 0.9870571752618587 Loss: 1.3365389\n",
      "TRAIN: Batch: 0.9879745146440723 Loss: 2.7351542\n",
      "TRAIN: Batch: 0.988891854026286 Loss: 1.6116071\n",
      "TRAIN: Batch: 0.9898091934084996 Loss: 1.740536\n",
      "TRAIN: Batch: 0.9907265327907132 Loss: 2.0370824\n",
      "TRAIN: Batch: 0.9916438721729268 Loss: 0.4682038\n",
      "TRAIN: Batch: 0.9925612115551404 Loss: 0.634511\n",
      "TRAIN: Batch: 0.993478550937354 Loss: 0.74750984\n",
      "TRAIN: Batch: 0.9943958903195677 Loss: 1.9401128\n",
      "TRAIN: Batch: 0.9953132297017813 Loss: 0.66168374\n",
      "TRAIN: Batch: 0.9962305690839949 Loss: 0.48146528\n",
      "TRAIN: Batch: 0.9971479084662086 Loss: 6.6334605\n",
      "TRAIN: Batch: 0.9980652478484222 Loss: 1.0359935\n",
      "TRAIN: Batch: 0.9989825872306358 Loss: 2.9720078\n",
      "TRAIN: Batch: 0.9998999266128494 Loss: 1.7180563\n",
      "Validating NN\n",
      "0\n",
      "4000\n",
      "8000\n",
      "12000\n",
      "16000\n",
      "20000\n",
      "24000\n",
      "28000\n",
      "32000\n",
      "36000\n",
      "40000\n",
      "44000\n",
      "48000\n",
      "52000\n",
      "56000\n",
      "60000\n",
      "64000\n",
      "68000\n",
      "72000\n",
      "76000\n",
      "80000\n",
      "84000\n",
      "88000\n",
      "92000\n",
      "96000\n",
      "100000\n",
      "104000\n",
      "108000\n",
      "112000\n",
      "116000\n",
      "120000\n",
      "124000\n",
      "128000\n",
      "132000\n",
      "136000\n",
      "140000\n",
      "144000\n",
      "148000\n",
      "152000\n",
      "156000\n",
      "160000\n",
      "164000\n",
      "168000\n",
      "172000\n",
      "176000\n",
      "180000\n",
      "184000\n",
      "188000\n",
      "192000\n",
      "196000\n",
      "200000\n",
      "204000\n",
      "208000\n",
      "212000\n",
      "216000\n",
      "220000\n",
      "224000\n",
      "228000\n",
      "232000\n",
      "236000\n",
      "240000\n",
      "244000\n",
      "248000\n",
      "252000\n",
      "256000\n",
      "260000\n",
      "264000\n",
      "268000\n",
      "272000\n",
      "276000\n",
      "280000\n",
      "284000\n",
      "288000\n",
      "292000\n",
      "296000\n",
      "300000\n",
      "304000\n",
      "308000\n",
      "312000\n",
      "VALID: Character error rate: 6.350984%. Word accuracy: 85.208835%.\n",
      "Character error rate improved, save model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1257: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1257: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3  Training...\n",
      "TRAIN: Batch: 0.0 Loss: 0.8667438\n",
      "TRAIN: Batch: 0.0009173393822136234 Loss: 3.0541754\n",
      "TRAIN: Batch: 0.0018346787644272467 Loss: 0.62192047\n",
      "TRAIN: Batch: 0.0027520181466408698 Loss: 2.3069916\n",
      "TRAIN: Batch: 0.0036693575288544934 Loss: 2.2546802\n",
      "TRAIN: Batch: 0.004586696911068116 Loss: 1.1131108\n",
      "TRAIN: Batch: 0.0055040362932817395 Loss: 1.6598246\n",
      "TRAIN: Batch: 0.006421375675495364 Loss: 2.5232759\n",
      "TRAIN: Batch: 0.007338715057708987 Loss: 1.0430806\n",
      "TRAIN: Batch: 0.00825605443992261 Loss: 1.9556623\n",
      "TRAIN: Batch: 0.009173393822136233 Loss: 0.7884383\n",
      "TRAIN: Batch: 0.010090733204349856 Loss: 1.322845\n",
      "TRAIN: Batch: 0.011008072586563479 Loss: 0.60729074\n",
      "TRAIN: Batch: 0.011925411968777104 Loss: 1.5705166\n",
      "TRAIN: Batch: 0.012842751350990727 Loss: 0.905859\n",
      "TRAIN: Batch: 0.01376009073320435 Loss: 1.3936541\n",
      "TRAIN: Batch: 0.014677430115417974 Loss: 0.49580613\n",
      "TRAIN: Batch: 0.015594769497631597 Loss: 1.2937149\n",
      "TRAIN: Batch: 0.01651210887984522 Loss: 0.62736905\n",
      "TRAIN: Batch: 0.017429448262058844 Loss: 1.2954253\n",
      "TRAIN: Batch: 0.018346787644272465 Loss: 4.446778\n",
      "TRAIN: Batch: 0.01926412702648609 Loss: 0.98462844\n",
      "TRAIN: Batch: 0.02018146640869971 Loss: 0.8866178\n",
      "TRAIN: Batch: 0.021098805790913337 Loss: 2.2782073\n",
      "TRAIN: Batch: 0.022016145173126958 Loss: 2.2688677\n",
      "TRAIN: Batch: 0.022933484555340583 Loss: 0.58578503\n",
      "TRAIN: Batch: 0.023850823937554208 Loss: 1.3437163\n",
      "TRAIN: Batch: 0.02476816331976783 Loss: 1.2666141\n",
      "TRAIN: Batch: 0.025685502701981455 Loss: 2.012733\n",
      "TRAIN: Batch: 0.026602842084195076 Loss: 1.2911197\n",
      "TRAIN: Batch: 0.0275201814664087 Loss: 2.2588224\n",
      "TRAIN: Batch: 0.028437520848622323 Loss: 0.5933956\n",
      "TRAIN: Batch: 0.029354860230835948 Loss: 3.43161\n",
      "TRAIN: Batch: 0.03027219961304957 Loss: 0.5667927\n",
      "TRAIN: Batch: 0.031189538995263194 Loss: 2.0568202\n",
      "TRAIN: Batch: 0.03210687837747682 Loss: 2.8282661\n",
      "TRAIN: Batch: 0.03302421775969044 Loss: 1.1711708\n",
      "TRAIN: Batch: 0.03394155714190406 Loss: 1.6229918\n",
      "TRAIN: Batch: 0.03485889652411769 Loss: 1.3528996\n",
      "TRAIN: Batch: 0.03577623590633131 Loss: 0.7675118\n",
      "TRAIN: Batch: 0.03669357528854493 Loss: 0.82022786\n",
      "TRAIN: Batch: 0.037610914670758555 Loss: 2.8030696\n",
      "TRAIN: Batch: 0.03852825405297218 Loss: 2.2246234\n",
      "TRAIN: Batch: 0.039445593435185805 Loss: 3.6929054\n",
      "TRAIN: Batch: 0.04036293281739942 Loss: 1.2889103\n",
      "TRAIN: Batch: 0.04128027219961305 Loss: 0.94983864\n",
      "TRAIN: Batch: 0.04219761158182667 Loss: 0.5706878\n",
      "TRAIN: Batch: 0.0431149509640403 Loss: 0.9620864\n",
      "TRAIN: Batch: 0.044032290346253916 Loss: 1.4183179\n",
      "TRAIN: Batch: 0.04494962972846754 Loss: 1.0892123\n",
      "TRAIN: Batch: 0.045866969110681166 Loss: 2.6663759\n",
      "TRAIN: Batch: 0.04678430849289479 Loss: 4.7493033\n",
      "TRAIN: Batch: 0.047701647875108416 Loss: 0.9932772\n",
      "TRAIN: Batch: 0.048618987257322034 Loss: 0.9472646\n",
      "TRAIN: Batch: 0.04953632663953566 Loss: 1.3467288\n",
      "TRAIN: Batch: 0.050453666021749284 Loss: 1.1468651\n",
      "TRAIN: Batch: 0.05137100540396291 Loss: 0.50935835\n",
      "TRAIN: Batch: 0.05228834478617653 Loss: 5.9397736\n",
      "TRAIN: Batch: 0.05320568416839015 Loss: 1.7041506\n",
      "TRAIN: Batch: 0.05412302355060378 Loss: 0.7756723\n",
      "TRAIN: Batch: 0.0550403629328174 Loss: 0.7770742\n",
      "TRAIN: Batch: 0.05595770231503102 Loss: 1.0556979\n",
      "TRAIN: Batch: 0.056875041697244645 Loss: 1.9963087\n",
      "TRAIN: Batch: 0.05779238107945827 Loss: 0.7470157\n",
      "TRAIN: Batch: 0.058709720461671895 Loss: 1.0709727\n",
      "TRAIN: Batch: 0.05962705984388551 Loss: 6.7845774\n",
      "TRAIN: Batch: 0.06054439922609914 Loss: 0.55650324\n",
      "TRAIN: Batch: 0.06146173860831276 Loss: 0.6104425\n",
      "TRAIN: Batch: 0.06237907799052639 Loss: 1.1775084\n",
      "TRAIN: Batch: 0.06329641737274001 Loss: 0.9600482\n",
      "TRAIN: Batch: 0.06421375675495364 Loss: 4.6264853\n",
      "TRAIN: Batch: 0.06513109613716725 Loss: 0.7442719\n",
      "TRAIN: Batch: 0.06604843551938087 Loss: 0.48340827\n",
      "TRAIN: Batch: 0.0669657749015945 Loss: 1.5171084\n",
      "TRAIN: Batch: 0.06788311428380812 Loss: 0.69922507\n",
      "TRAIN: Batch: 0.06880045366602175 Loss: 0.51033664\n",
      "TRAIN: Batch: 0.06971779304823537 Loss: 3.0352237\n",
      "TRAIN: Batch: 0.070635132430449 Loss: 0.74278677\n",
      "TRAIN: Batch: 0.07155247181266262 Loss: 0.757288\n",
      "TRAIN: Batch: 0.07246981119487625 Loss: 2.1406198\n",
      "TRAIN: Batch: 0.07338715057708986 Loss: 0.58586156\n",
      "TRAIN: Batch: 0.07430448995930349 Loss: 1.616215\n",
      "TRAIN: Batch: 0.07522182934151711 Loss: 0.68641233\n",
      "TRAIN: Batch: 0.07613916872373074 Loss: 1.2937018\n",
      "TRAIN: Batch: 0.07705650810594436 Loss: 1.1704011\n",
      "TRAIN: Batch: 0.07797384748815799 Loss: 0.54858446\n",
      "TRAIN: Batch: 0.07889118687037161 Loss: 1.1482527\n",
      "TRAIN: Batch: 0.07980852625258524 Loss: 1.6694152\n",
      "TRAIN: Batch: 0.08072586563479885 Loss: 0.97269404\n",
      "TRAIN: Batch: 0.08164320501701247 Loss: 0.9969115\n",
      "TRAIN: Batch: 0.0825605443992261 Loss: 0.98032093\n",
      "TRAIN: Batch: 0.08347788378143972 Loss: 1.7555021\n",
      "TRAIN: Batch: 0.08439522316365335 Loss: 2.8034372\n",
      "TRAIN: Batch: 0.08531256254586697 Loss: 1.1744263\n",
      "TRAIN: Batch: 0.0862299019280806 Loss: 0.8108527\n",
      "TRAIN: Batch: 0.08714724131029422 Loss: 2.0124855\n",
      "TRAIN: Batch: 0.08806458069250783 Loss: 0.8746091\n",
      "TRAIN: Batch: 0.08898192007472146 Loss: 2.1681955\n",
      "TRAIN: Batch: 0.08989925945693508 Loss: 0.72242236\n",
      "TRAIN: Batch: 0.09081659883914871 Loss: 0.7157228\n",
      "TRAIN: Batch: 0.09173393822136233 Loss: 1.0819237\n",
      "TRAIN: Batch: 0.09265127760357596 Loss: 0.63762474\n",
      "TRAIN: Batch: 0.09356861698578958 Loss: 1.519634\n",
      "TRAIN: Batch: 0.09448595636800321 Loss: 1.1829011\n",
      "TRAIN: Batch: 0.09540329575021683 Loss: 1.4402378\n",
      "TRAIN: Batch: 0.09632063513243044 Loss: 1.2003199\n",
      "TRAIN: Batch: 0.09723797451464407 Loss: 5.1815143\n",
      "TRAIN: Batch: 0.0981553138968577 Loss: 2.6027617\n",
      "TRAIN: Batch: 0.09907265327907132 Loss: 1.1199156\n",
      "TRAIN: Batch: 0.09998999266128494 Loss: 0.85262114\n",
      "TRAIN: Batch: 0.10090733204349857 Loss: 1.5064948\n",
      "TRAIN: Batch: 0.1018246714257122 Loss: 2.1362414\n",
      "TRAIN: Batch: 0.10274201080792582 Loss: 1.1996839\n",
      "TRAIN: Batch: 0.10365935019013943 Loss: 1.2982355\n",
      "TRAIN: Batch: 0.10457668957235305 Loss: 1.061482\n",
      "TRAIN: Batch: 0.10549402895456668 Loss: 1.1407875\n",
      "TRAIN: Batch: 0.1064113683367803 Loss: 10.571685\n",
      "TRAIN: Batch: 0.10732870771899393 Loss: 3.236867\n",
      "TRAIN: Batch: 0.10824604710120755 Loss: 0.6129534\n",
      "TRAIN: Batch: 0.10916338648342118 Loss: 1.3284137\n",
      "TRAIN: Batch: 0.1100807258656348 Loss: 5.4776297\n",
      "TRAIN: Batch: 0.11099806524784842 Loss: 1.3661441\n",
      "TRAIN: Batch: 0.11191540463006204 Loss: 1.2206252\n",
      "TRAIN: Batch: 0.11283274401227567 Loss: 3.871354\n",
      "TRAIN: Batch: 0.11375008339448929 Loss: 1.4818847\n",
      "TRAIN: Batch: 0.11466742277670292 Loss: 0.58240604\n",
      "TRAIN: Batch: 0.11558476215891654 Loss: 2.1261272\n",
      "TRAIN: Batch: 0.11650210154113017 Loss: 1.249306\n",
      "TRAIN: Batch: 0.11741944092334379 Loss: 2.2535646\n",
      "TRAIN: Batch: 0.11833678030555742 Loss: 0.68026364\n",
      "TRAIN: Batch: 0.11925411968777103 Loss: 1.4234879\n",
      "TRAIN: Batch: 0.12017145906998465 Loss: 1.3545234\n",
      "TRAIN: Batch: 0.12108879845219828 Loss: 0.47264194\n",
      "TRAIN: Batch: 0.1220061378344119 Loss: 2.2892728\n",
      "TRAIN: Batch: 0.12292347721662553 Loss: 2.9528532\n",
      "TRAIN: Batch: 0.12384081659883915 Loss: 2.4453263\n",
      "TRAIN: Batch: 0.12475815598105278 Loss: 1.4561529\n",
      "TRAIN: Batch: 0.1256754953632664 Loss: 1.7177182\n",
      "TRAIN: Batch: 0.12659283474548003 Loss: 1.029685\n",
      "TRAIN: Batch: 0.12751017412769364 Loss: 2.8748398\n",
      "TRAIN: Batch: 0.12842751350990728 Loss: 1.9378036\n",
      "TRAIN: Batch: 0.1293448528921209 Loss: 2.7876585\n",
      "TRAIN: Batch: 0.1302621922743345 Loss: 0.73218787\n",
      "TRAIN: Batch: 0.13117953165654814 Loss: 1.2417526\n",
      "TRAIN: Batch: 0.13209687103876175 Loss: 0.7706305\n",
      "TRAIN: Batch: 0.1330142104209754 Loss: 2.7725282\n",
      "TRAIN: Batch: 0.133931549803189 Loss: 0.78312135\n",
      "TRAIN: Batch: 0.13484888918540264 Loss: 0.86959714\n",
      "TRAIN: Batch: 0.13576622856761625 Loss: 1.3030813\n",
      "TRAIN: Batch: 0.1366835679498299 Loss: 1.9276378\n",
      "TRAIN: Batch: 0.1376009073320435 Loss: 1.494987\n",
      "TRAIN: Batch: 0.1385182467142571 Loss: 1.6088126\n",
      "TRAIN: Batch: 0.13943558609647075 Loss: 2.31781\n",
      "TRAIN: Batch: 0.14035292547868436 Loss: 0.48760396\n",
      "TRAIN: Batch: 0.141270264860898 Loss: 0.4817614\n",
      "TRAIN: Batch: 0.1421876042431116 Loss: 1.3303154\n",
      "TRAIN: Batch: 0.14310494362532525 Loss: 1.6212757\n",
      "TRAIN: Batch: 0.14402228300753886 Loss: 1.2522137\n",
      "TRAIN: Batch: 0.1449396223897525 Loss: 2.05319\n",
      "TRAIN: Batch: 0.1458569617719661 Loss: 1.8653337\n",
      "TRAIN: Batch: 0.14677430115417972 Loss: 0.5512039\n",
      "TRAIN: Batch: 0.14769164053639336 Loss: 1.1282582\n",
      "TRAIN: Batch: 0.14860897991860697 Loss: 1.1186929\n",
      "TRAIN: Batch: 0.1495263193008206 Loss: 0.9109694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.15044365868303422 Loss: 4.7724175\n",
      "TRAIN: Batch: 0.15136099806524786 Loss: 0.5798253\n",
      "TRAIN: Batch: 0.15227833744746147 Loss: 0.9098537\n",
      "TRAIN: Batch: 0.15319567682967508 Loss: 1.2182647\n",
      "TRAIN: Batch: 0.15411301621188872 Loss: 0.7455193\n",
      "TRAIN: Batch: 0.15503035559410233 Loss: 3.8802328\n",
      "TRAIN: Batch: 0.15594769497631597 Loss: 1.0898046\n",
      "TRAIN: Batch: 0.15686503435852958 Loss: 3.2343068\n",
      "TRAIN: Batch: 0.15778237374074322 Loss: 1.4491501\n",
      "TRAIN: Batch: 0.15869971312295683 Loss: 0.47665668\n",
      "TRAIN: Batch: 0.15961705250517047 Loss: 1.1528026\n",
      "TRAIN: Batch: 0.16053439188738408 Loss: 3.00843\n",
      "TRAIN: Batch: 0.1614517312695977 Loss: 2.8027906\n",
      "TRAIN: Batch: 0.16236907065181133 Loss: 1.8330207\n",
      "TRAIN: Batch: 0.16328641003402494 Loss: 1.0153961\n",
      "TRAIN: Batch: 0.16420374941623858 Loss: 0.5017904\n",
      "TRAIN: Batch: 0.1651210887984522 Loss: 2.8015783\n",
      "TRAIN: Batch: 0.16603842818066583 Loss: 0.9506736\n",
      "TRAIN: Batch: 0.16695576756287944 Loss: 1.6421645\n",
      "TRAIN: Batch: 0.16787310694509308 Loss: 0.68921846\n",
      "TRAIN: Batch: 0.1687904463273067 Loss: 0.57006145\n",
      "TRAIN: Batch: 0.1697077857095203 Loss: 1.1784348\n",
      "TRAIN: Batch: 0.17062512509173394 Loss: 0.6493392\n",
      "TRAIN: Batch: 0.17154246447394755 Loss: 1.1712594\n",
      "TRAIN: Batch: 0.1724598038561612 Loss: 1.0262753\n",
      "TRAIN: Batch: 0.1733771432383748 Loss: 2.4208474\n",
      "TRAIN: Batch: 0.17429448262058844 Loss: 0.46922788\n",
      "TRAIN: Batch: 0.17521182200280205 Loss: 4.599432\n",
      "TRAIN: Batch: 0.17612916138501566 Loss: 1.2666439\n",
      "TRAIN: Batch: 0.1770465007672293 Loss: 0.55283463\n",
      "TRAIN: Batch: 0.17796384014944291 Loss: 2.3860035\n",
      "TRAIN: Batch: 0.17888117953165655 Loss: 1.4184663\n",
      "TRAIN: Batch: 0.17979851891387016 Loss: 1.0121216\n",
      "TRAIN: Batch: 0.1807158582960838 Loss: 1.2463733\n",
      "TRAIN: Batch: 0.18163319767829741 Loss: 0.7167877\n",
      "TRAIN: Batch: 0.18255053706051105 Loss: 1.9768038\n",
      "TRAIN: Batch: 0.18346787644272466 Loss: 0.68394935\n",
      "TRAIN: Batch: 0.18438521582493828 Loss: 1.2539368\n",
      "TRAIN: Batch: 0.18530255520715191 Loss: 5.9388266\n",
      "TRAIN: Batch: 0.18621989458936553 Loss: 1.1328502\n",
      "TRAIN: Batch: 0.18713723397157916 Loss: 1.2244625\n",
      "TRAIN: Batch: 0.18805457335379278 Loss: 0.7620854\n",
      "TRAIN: Batch: 0.18897191273600641 Loss: 0.784081\n",
      "TRAIN: Batch: 0.18988925211822003 Loss: 0.73324656\n",
      "TRAIN: Batch: 0.19080659150043366 Loss: 0.5954054\n",
      "TRAIN: Batch: 0.19172393088264728 Loss: 0.89066935\n",
      "TRAIN: Batch: 0.1926412702648609 Loss: 2.5295734\n",
      "TRAIN: Batch: 0.19355860964707453 Loss: 1.6588154\n",
      "TRAIN: Batch: 0.19447594902928814 Loss: 1.0045269\n",
      "TRAIN: Batch: 0.19539328841150178 Loss: 1.2475566\n",
      "TRAIN: Batch: 0.1963106277937154 Loss: 2.1353607\n",
      "TRAIN: Batch: 0.19722796717592903 Loss: 0.88759285\n",
      "TRAIN: Batch: 0.19814530655814264 Loss: 0.8641999\n",
      "TRAIN: Batch: 0.19906264594035625 Loss: 3.226465\n",
      "TRAIN: Batch: 0.1999799853225699 Loss: 0.7423618\n",
      "TRAIN: Batch: 0.2008973247047835 Loss: 1.4035223\n",
      "TRAIN: Batch: 0.20181466408699714 Loss: 1.0462031\n",
      "TRAIN: Batch: 0.20273200346921075 Loss: 0.6459044\n",
      "TRAIN: Batch: 0.2036493428514244 Loss: 3.3790927\n",
      "TRAIN: Batch: 0.204566682233638 Loss: 1.09002\n",
      "TRAIN: Batch: 0.20548402161585164 Loss: 1.022726\n",
      "TRAIN: Batch: 0.20640136099806525 Loss: 0.8801296\n",
      "TRAIN: Batch: 0.20731870038027886 Loss: 2.952432\n",
      "TRAIN: Batch: 0.2082360397624925 Loss: 2.6214006\n",
      "TRAIN: Batch: 0.2091533791447061 Loss: 0.64371777\n",
      "TRAIN: Batch: 0.21007071852691975 Loss: 2.5027432\n",
      "TRAIN: Batch: 0.21098805790913336 Loss: 3.532286\n",
      "TRAIN: Batch: 0.211905397291347 Loss: 0.8347304\n",
      "TRAIN: Batch: 0.2128227366735606 Loss: 1.524341\n",
      "TRAIN: Batch: 0.21374007605577425 Loss: 1.8496361\n",
      "TRAIN: Batch: 0.21465741543798786 Loss: 1.6313031\n",
      "TRAIN: Batch: 0.21557475482020147 Loss: 0.8001474\n",
      "TRAIN: Batch: 0.2164920942024151 Loss: 2.7930253\n",
      "TRAIN: Batch: 0.21740943358462872 Loss: 1.4435697\n",
      "TRAIN: Batch: 0.21832677296684236 Loss: 0.7428131\n",
      "TRAIN: Batch: 0.21924411234905597 Loss: 1.1074351\n",
      "TRAIN: Batch: 0.2201614517312696 Loss: 0.49744284\n",
      "TRAIN: Batch: 0.22107879111348322 Loss: 3.5127702\n",
      "TRAIN: Batch: 0.22199613049569683 Loss: 4.6305313\n",
      "TRAIN: Batch: 0.22291346987791047 Loss: 0.871218\n",
      "TRAIN: Batch: 0.22383080926012408 Loss: 1.4591637\n",
      "TRAIN: Batch: 0.22474814864233772 Loss: 1.9093449\n",
      "TRAIN: Batch: 0.22566548802455133 Loss: 1.8742466\n",
      "TRAIN: Batch: 0.22658282740676497 Loss: 1.558871\n",
      "TRAIN: Batch: 0.22750016678897858 Loss: 1.7099909\n",
      "TRAIN: Batch: 0.22841750617119222 Loss: 0.47841558\n",
      "TRAIN: Batch: 0.22933484555340583 Loss: 1.2135023\n",
      "TRAIN: Batch: 0.23025218493561944 Loss: 0.7565154\n",
      "TRAIN: Batch: 0.23116952431783308 Loss: 1.2272159\n",
      "TRAIN: Batch: 0.2320868637000467 Loss: 2.397905\n",
      "TRAIN: Batch: 0.23300420308226033 Loss: 0.47668597\n",
      "TRAIN: Batch: 0.23392154246447394 Loss: 0.6363774\n",
      "TRAIN: Batch: 0.23483888184668758 Loss: 1.5097231\n",
      "TRAIN: Batch: 0.2357562212289012 Loss: 1.767415\n",
      "TRAIN: Batch: 0.23667356061111483 Loss: 2.339407\n",
      "TRAIN: Batch: 0.23759089999332844 Loss: 1.4204754\n",
      "TRAIN: Batch: 0.23850823937554205 Loss: 1.9172955\n",
      "TRAIN: Batch: 0.2394255787577557 Loss: 0.5659293\n",
      "TRAIN: Batch: 0.2403429181399693 Loss: 1.1479082\n",
      "TRAIN: Batch: 0.24126025752218294 Loss: 0.7072072\n",
      "TRAIN: Batch: 0.24217759690439655 Loss: 0.64377016\n",
      "TRAIN: Batch: 0.2430949362866102 Loss: 1.5933315\n",
      "TRAIN: Batch: 0.2440122756688238 Loss: 0.5798943\n",
      "TRAIN: Batch: 0.2449296150510374 Loss: 2.4276538\n",
      "TRAIN: Batch: 0.24584695443325105 Loss: 3.2036946\n",
      "TRAIN: Batch: 0.24676429381546466 Loss: 1.7158984\n",
      "TRAIN: Batch: 0.2476816331976783 Loss: 1.1285172\n",
      "TRAIN: Batch: 0.2485989725798919 Loss: 0.8818327\n",
      "TRAIN: Batch: 0.24951631196210555 Loss: 3.189301\n",
      "TRAIN: Batch: 0.25043365134431916 Loss: 0.48100597\n",
      "TRAIN: Batch: 0.2513509907265328 Loss: 3.3889937\n",
      "TRAIN: Batch: 0.25226833010874644 Loss: 1.4832695\n",
      "TRAIN: Batch: 0.25318566949096005 Loss: 0.9986875\n",
      "TRAIN: Batch: 0.25410300887317366 Loss: 0.9403074\n",
      "TRAIN: Batch: 0.2550203482553873 Loss: 3.2372725\n",
      "TRAIN: Batch: 0.2559376876376009 Loss: 0.50420994\n",
      "TRAIN: Batch: 0.25685502701981455 Loss: 0.706413\n",
      "TRAIN: Batch: 0.25777236640202816 Loss: 1.5325022\n",
      "TRAIN: Batch: 0.2586897057842418 Loss: 1.1040933\n",
      "TRAIN: Batch: 0.2596070451664554 Loss: 0.64756405\n",
      "TRAIN: Batch: 0.260524384548669 Loss: 0.48293158\n",
      "TRAIN: Batch: 0.26144172393088266 Loss: 4.715082\n",
      "TRAIN: Batch: 0.2623590633130963 Loss: 1.4033713\n",
      "TRAIN: Batch: 0.2632764026953099 Loss: 2.0159938\n",
      "TRAIN: Batch: 0.2641937420775235 Loss: 1.3061512\n",
      "TRAIN: Batch: 0.26511108145973716 Loss: 2.2860615\n",
      "TRAIN: Batch: 0.2660284208419508 Loss: 3.1859345\n",
      "TRAIN: Batch: 0.2669457602241644 Loss: 4.8939443\n",
      "TRAIN: Batch: 0.267863099606378 Loss: 0.6027718\n",
      "TRAIN: Batch: 0.2687804389885916 Loss: 1.0589784\n",
      "TRAIN: Batch: 0.2696977783708053 Loss: 3.600547\n",
      "TRAIN: Batch: 0.2706151177530189 Loss: 1.7067977\n",
      "TRAIN: Batch: 0.2715324571352325 Loss: 0.7610722\n",
      "TRAIN: Batch: 0.2724497965174461 Loss: 0.7404795\n",
      "TRAIN: Batch: 0.2733671358996598 Loss: 1.1132895\n",
      "TRAIN: Batch: 0.2742844752818734 Loss: 0.70670426\n",
      "TRAIN: Batch: 0.275201814664087 Loss: 2.0590024\n",
      "TRAIN: Batch: 0.2761191540463006 Loss: 1.1731066\n",
      "TRAIN: Batch: 0.2770364934285142 Loss: 1.6946516\n",
      "TRAIN: Batch: 0.2779538328107279 Loss: 1.16598\n",
      "TRAIN: Batch: 0.2788711721929415 Loss: 2.2969499\n",
      "TRAIN: Batch: 0.2797885115751551 Loss: 1.1462162\n",
      "TRAIN: Batch: 0.2807058509573687 Loss: 1.8073478\n",
      "TRAIN: Batch: 0.2816231903395824 Loss: 2.9114306\n",
      "TRAIN: Batch: 0.282540529721796 Loss: 2.8136516\n",
      "TRAIN: Batch: 0.2834578691040096 Loss: 1.9236202\n",
      "TRAIN: Batch: 0.2843752084862232 Loss: 2.1091332\n",
      "TRAIN: Batch: 0.28529254786843683 Loss: 3.187507\n",
      "TRAIN: Batch: 0.2862098872506505 Loss: 3.5778975\n",
      "TRAIN: Batch: 0.2871272266328641 Loss: 1.3113824\n",
      "TRAIN: Batch: 0.2880445660150777 Loss: 1.9910066\n",
      "TRAIN: Batch: 0.28896190539729133 Loss: 0.6822772\n",
      "TRAIN: Batch: 0.289879244779505 Loss: 0.5056097\n",
      "TRAIN: Batch: 0.2907965841617186 Loss: 2.2258127\n",
      "TRAIN: Batch: 0.2917139235439322 Loss: 2.2898412\n",
      "TRAIN: Batch: 0.29263126292614583 Loss: 0.46173263\n",
      "TRAIN: Batch: 0.29354860230835944 Loss: 3.803937\n",
      "TRAIN: Batch: 0.2944659416905731 Loss: 1.1662501\n",
      "TRAIN: Batch: 0.2953832810727867 Loss: 1.2431207\n",
      "TRAIN: Batch: 0.29630062045500033 Loss: 1.8500633\n",
      "TRAIN: Batch: 0.29721795983721394 Loss: 1.9963121\n",
      "TRAIN: Batch: 0.2981352992194276 Loss: 0.9980788\n",
      "TRAIN: Batch: 0.2990526386016412 Loss: 0.8767239\n",
      "TRAIN: Batch: 0.29996997798385483 Loss: 0.9528462\n",
      "TRAIN: Batch: 0.30088731736606844 Loss: 1.3307408\n",
      "TRAIN: Batch: 0.30180465674828205 Loss: 0.49180087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.3027219961304957 Loss: 1.062738\n",
      "TRAIN: Batch: 0.30363933551270933 Loss: 1.0744829\n",
      "TRAIN: Batch: 0.30455667489492294 Loss: 1.2953341\n",
      "TRAIN: Batch: 0.30547401427713655 Loss: 1.1150167\n",
      "TRAIN: Batch: 0.30639135365935016 Loss: 0.48002663\n",
      "TRAIN: Batch: 0.30730869304156383 Loss: 0.52751654\n",
      "TRAIN: Batch: 0.30822603242377744 Loss: 1.4721098\n",
      "TRAIN: Batch: 0.30914337180599105 Loss: 0.7619801\n",
      "TRAIN: Batch: 0.31006071118820466 Loss: 0.5716314\n",
      "TRAIN: Batch: 0.31097805057041833 Loss: 1.843098\n",
      "TRAIN: Batch: 0.31189538995263194 Loss: 0.69161814\n",
      "TRAIN: Batch: 0.31281272933484555 Loss: 0.7570654\n",
      "TRAIN: Batch: 0.31373006871705916 Loss: 1.3328911\n",
      "TRAIN: Batch: 0.3146474080992728 Loss: 2.529555\n",
      "TRAIN: Batch: 0.31556474748148644 Loss: 3.3103032\n",
      "TRAIN: Batch: 0.31648208686370005 Loss: 0.7810255\n",
      "TRAIN: Batch: 0.31739942624591366 Loss: 1.2106922\n",
      "TRAIN: Batch: 0.3183167656281273 Loss: 0.5128359\n",
      "TRAIN: Batch: 0.31923410501034094 Loss: 1.9259284\n",
      "TRAIN: Batch: 0.32015144439255455 Loss: 3.4646726\n",
      "TRAIN: Batch: 0.32106878377476816 Loss: 0.8200177\n",
      "TRAIN: Batch: 0.3219861231569818 Loss: 3.8421342\n",
      "TRAIN: Batch: 0.3229034625391954 Loss: 1.0623057\n",
      "TRAIN: Batch: 0.32382080192140905 Loss: 0.5793918\n",
      "TRAIN: Batch: 0.32473814130362266 Loss: 2.1384275\n",
      "TRAIN: Batch: 0.3256554806858363 Loss: 1.3746629\n",
      "TRAIN: Batch: 0.3265728200680499 Loss: 1.1590555\n",
      "TRAIN: Batch: 0.32749015945026355 Loss: 0.8638398\n",
      "TRAIN: Batch: 0.32840749883247716 Loss: 3.2026446\n",
      "TRAIN: Batch: 0.3293248382146908 Loss: 0.655618\n",
      "TRAIN: Batch: 0.3302421775969044 Loss: 3.1780975\n",
      "TRAIN: Batch: 0.331159516979118 Loss: 1.1920702\n",
      "TRAIN: Batch: 0.33207685636133166 Loss: 1.2134159\n",
      "TRAIN: Batch: 0.3329941957435453 Loss: 1.0330288\n",
      "TRAIN: Batch: 0.3339115351257589 Loss: 0.62418914\n",
      "TRAIN: Batch: 0.3348288745079725 Loss: 0.6366068\n",
      "TRAIN: Batch: 0.33574621389018616 Loss: 0.65933764\n",
      "TRAIN: Batch: 0.3366635532723998 Loss: 1.0173478\n",
      "TRAIN: Batch: 0.3375808926546134 Loss: 6.076368\n",
      "TRAIN: Batch: 0.338498232036827 Loss: 1.2336476\n",
      "TRAIN: Batch: 0.3394155714190406 Loss: 1.9036133\n",
      "TRAIN: Batch: 0.3403329108012543 Loss: 0.8506316\n",
      "TRAIN: Batch: 0.3412502501834679 Loss: 3.4624798\n",
      "TRAIN: Batch: 0.3421675895656815 Loss: 0.71699005\n",
      "TRAIN: Batch: 0.3430849289478951 Loss: 2.8054378\n",
      "TRAIN: Batch: 0.3440022683301088 Loss: 1.2849805\n",
      "TRAIN: Batch: 0.3449196077123224 Loss: 0.6844481\n",
      "TRAIN: Batch: 0.345836947094536 Loss: 2.0425045\n",
      "TRAIN: Batch: 0.3467542864767496 Loss: 1.2025934\n",
      "TRAIN: Batch: 0.3476716258589632 Loss: 1.6434312\n",
      "TRAIN: Batch: 0.3485889652411769 Loss: 1.0956635\n",
      "TRAIN: Batch: 0.3495063046233905 Loss: 0.49195424\n",
      "TRAIN: Batch: 0.3504236440056041 Loss: 1.6664817\n",
      "TRAIN: Batch: 0.3513409833878177 Loss: 1.13023\n",
      "TRAIN: Batch: 0.35225832277003133 Loss: 1.4564369\n",
      "TRAIN: Batch: 0.353175662152245 Loss: 0.6888095\n",
      "TRAIN: Batch: 0.3540930015344586 Loss: 2.411109\n",
      "TRAIN: Batch: 0.3550103409166722 Loss: 0.8150421\n",
      "TRAIN: Batch: 0.35592768029888583 Loss: 2.1476915\n",
      "TRAIN: Batch: 0.3568450196810995 Loss: 0.52568185\n",
      "TRAIN: Batch: 0.3577623590633131 Loss: 0.7989984\n",
      "TRAIN: Batch: 0.3586796984455267 Loss: 0.65487105\n",
      "TRAIN: Batch: 0.35959703782774033 Loss: 0.766194\n",
      "TRAIN: Batch: 0.36051437720995394 Loss: 1.0758437\n",
      "TRAIN: Batch: 0.3614317165921676 Loss: 2.0532422\n",
      "TRAIN: Batch: 0.3623490559743812 Loss: 1.8758535\n",
      "TRAIN: Batch: 0.36326639535659483 Loss: 1.1281548\n",
      "TRAIN: Batch: 0.36418373473880844 Loss: 1.7071551\n",
      "TRAIN: Batch: 0.3651010741210221 Loss: 1.0255468\n",
      "TRAIN: Batch: 0.3660184135032357 Loss: 0.7829894\n",
      "TRAIN: Batch: 0.36693575288544933 Loss: 2.22523\n",
      "TRAIN: Batch: 0.36785309226766294 Loss: 0.6546105\n",
      "TRAIN: Batch: 0.36877043164987655 Loss: 0.7811445\n",
      "TRAIN: Batch: 0.3696877710320902 Loss: 0.8967997\n",
      "TRAIN: Batch: 0.37060511041430383 Loss: 0.607434\n",
      "TRAIN: Batch: 0.37152244979651744 Loss: 1.3109158\n",
      "TRAIN: Batch: 0.37243978917873105 Loss: 1.8398471\n",
      "TRAIN: Batch: 0.3733571285609447 Loss: 0.5043218\n",
      "TRAIN: Batch: 0.37427446794315833 Loss: 0.49978998\n",
      "TRAIN: Batch: 0.37519180732537194 Loss: 1.4494137\n",
      "TRAIN: Batch: 0.37610914670758555 Loss: 0.6364596\n",
      "TRAIN: Batch: 0.37702648608979916 Loss: 3.3328736\n",
      "TRAIN: Batch: 0.37794382547201283 Loss: 1.7609892\n",
      "TRAIN: Batch: 0.37886116485422644 Loss: 6.62787\n",
      "TRAIN: Batch: 0.37977850423644005 Loss: 2.118353\n",
      "TRAIN: Batch: 0.38069584361865366 Loss: 0.7696177\n",
      "TRAIN: Batch: 0.38161318300086733 Loss: 1.1085589\n",
      "TRAIN: Batch: 0.38253052238308094 Loss: 0.4837995\n",
      "TRAIN: Batch: 0.38344786176529455 Loss: 0.87915033\n",
      "TRAIN: Batch: 0.38436520114750816 Loss: 1.2119808\n",
      "TRAIN: Batch: 0.3852825405297218 Loss: 2.4144542\n",
      "TRAIN: Batch: 0.38619987991193544 Loss: 2.3888714\n",
      "TRAIN: Batch: 0.38711721929414905 Loss: 0.50921804\n",
      "TRAIN: Batch: 0.38803455867636266 Loss: 1.2726288\n",
      "TRAIN: Batch: 0.3889518980585763 Loss: 0.7934168\n",
      "TRAIN: Batch: 0.38986923744078994 Loss: 4.75291\n",
      "TRAIN: Batch: 0.39078657682300355 Loss: 0.7217845\n",
      "TRAIN: Batch: 0.39170391620521716 Loss: 0.64688975\n",
      "TRAIN: Batch: 0.3926212555874308 Loss: 1.456595\n",
      "TRAIN: Batch: 0.3935385949696444 Loss: 0.89006996\n",
      "TRAIN: Batch: 0.39445593435185805 Loss: 4.0254583\n",
      "TRAIN: Batch: 0.39537327373407166 Loss: 1.9791987\n",
      "TRAIN: Batch: 0.3962906131162853 Loss: 0.78999877\n",
      "TRAIN: Batch: 0.3972079524984989 Loss: 3.4196682\n",
      "TRAIN: Batch: 0.3981252918807125 Loss: 1.5894821\n",
      "TRAIN: Batch: 0.39904263126292616 Loss: 1.2627354\n",
      "TRAIN: Batch: 0.3999599706451398 Loss: 0.7894403\n",
      "TRAIN: Batch: 0.4008773100273534 Loss: 2.8600552\n",
      "TRAIN: Batch: 0.401794649409567 Loss: 0.7362137\n",
      "TRAIN: Batch: 0.40271198879178066 Loss: 0.97765183\n",
      "TRAIN: Batch: 0.4036293281739943 Loss: 1.2340827\n",
      "TRAIN: Batch: 0.4045466675562079 Loss: 0.5138068\n",
      "TRAIN: Batch: 0.4054640069384215 Loss: 0.94040203\n",
      "TRAIN: Batch: 0.4063813463206351 Loss: 1.1961496\n",
      "TRAIN: Batch: 0.4072986857028488 Loss: 1.2459034\n",
      "TRAIN: Batch: 0.4082160250850624 Loss: 1.7744017\n",
      "TRAIN: Batch: 0.409133364467276 Loss: 2.8057027\n",
      "TRAIN: Batch: 0.4100507038494896 Loss: 1.1881078\n",
      "TRAIN: Batch: 0.4109680432317033 Loss: 2.048908\n",
      "TRAIN: Batch: 0.4118853826139169 Loss: 0.49063283\n",
      "TRAIN: Batch: 0.4128027219961305 Loss: 0.63381803\n",
      "TRAIN: Batch: 0.4137200613783441 Loss: 2.1576447\n",
      "TRAIN: Batch: 0.4146374007605577 Loss: 3.7992783\n",
      "TRAIN: Batch: 0.4155547401427714 Loss: 1.3592424\n",
      "TRAIN: Batch: 0.416472079524985 Loss: 1.6162728\n",
      "TRAIN: Batch: 0.4173894189071986 Loss: 0.5308901\n",
      "TRAIN: Batch: 0.4183067582894122 Loss: 1.0201066\n",
      "TRAIN: Batch: 0.4192240976716259 Loss: 1.4275374\n",
      "TRAIN: Batch: 0.4201414370538395 Loss: 1.6317298\n",
      "TRAIN: Batch: 0.4210587764360531 Loss: 0.7467061\n",
      "TRAIN: Batch: 0.4219761158182667 Loss: 0.921203\n",
      "TRAIN: Batch: 0.42289345520048033 Loss: 1.6971605\n",
      "TRAIN: Batch: 0.423810794582694 Loss: 2.4052854\n",
      "TRAIN: Batch: 0.4247281339649076 Loss: 1.9831536\n",
      "TRAIN: Batch: 0.4256454733471212 Loss: 0.8090216\n",
      "TRAIN: Batch: 0.42656281272933483 Loss: 2.5200014\n",
      "TRAIN: Batch: 0.4274801521115485 Loss: 2.455804\n",
      "TRAIN: Batch: 0.4283974914937621 Loss: 1.6385465\n",
      "TRAIN: Batch: 0.4293148308759757 Loss: 1.9730781\n",
      "TRAIN: Batch: 0.43023217025818933 Loss: 0.72241783\n",
      "TRAIN: Batch: 0.43114950964040294 Loss: 0.8895874\n",
      "TRAIN: Batch: 0.4320668490226166 Loss: 3.954609\n",
      "TRAIN: Batch: 0.4329841884048302 Loss: 1.5288334\n",
      "TRAIN: Batch: 0.43390152778704383 Loss: 1.232867\n",
      "TRAIN: Batch: 0.43481886716925744 Loss: 0.5693925\n",
      "TRAIN: Batch: 0.4357362065514711 Loss: 2.812694\n",
      "TRAIN: Batch: 0.4366535459336847 Loss: 3.5821128\n",
      "TRAIN: Batch: 0.43757088531589833 Loss: 2.0466013\n",
      "TRAIN: Batch: 0.43848822469811194 Loss: 2.5553515\n",
      "TRAIN: Batch: 0.43940556408032555 Loss: 2.6154263\n",
      "TRAIN: Batch: 0.4403229034625392 Loss: 2.4079146\n",
      "TRAIN: Batch: 0.44124024284475283 Loss: 0.851349\n",
      "TRAIN: Batch: 0.44215758222696644 Loss: 1.6784848\n",
      "TRAIN: Batch: 0.44307492160918005 Loss: 1.4927213\n",
      "TRAIN: Batch: 0.44399226099139366 Loss: 4.4304132\n",
      "TRAIN: Batch: 0.44490960037360733 Loss: 0.9650681\n",
      "TRAIN: Batch: 0.44582693975582094 Loss: 12.83804\n",
      "TRAIN: Batch: 0.44674427913803455 Loss: 3.5383394\n",
      "TRAIN: Batch: 0.44766161852024816 Loss: 0.88840806\n",
      "TRAIN: Batch: 0.44857895790246183 Loss: 6.2695956\n",
      "TRAIN: Batch: 0.44949629728467544 Loss: 1.3444159\n",
      "TRAIN: Batch: 0.45041363666688905 Loss: 2.9860632\n",
      "TRAIN: Batch: 0.45133097604910266 Loss: 1.0523021\n",
      "TRAIN: Batch: 0.4522483154313163 Loss: 1.1569595\n",
      "TRAIN: Batch: 0.45316565481352994 Loss: 2.3078296\n",
      "TRAIN: Batch: 0.45408299419574355 Loss: 0.4701813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.45500033357795716 Loss: 1.3919566\n",
      "TRAIN: Batch: 0.4559176729601708 Loss: 1.2564015\n",
      "TRAIN: Batch: 0.45683501234238444 Loss: 2.6424305\n",
      "TRAIN: Batch: 0.45775235172459805 Loss: 1.2017713\n",
      "TRAIN: Batch: 0.45866969110681166 Loss: 2.3984568\n",
      "TRAIN: Batch: 0.45958703048902527 Loss: 0.5599254\n",
      "TRAIN: Batch: 0.4605043698712389 Loss: 1.2889494\n",
      "TRAIN: Batch: 0.46142170925345255 Loss: 0.71123993\n",
      "TRAIN: Batch: 0.46233904863566616 Loss: 1.7151115\n",
      "TRAIN: Batch: 0.46325638801787977 Loss: 1.8692648\n",
      "TRAIN: Batch: 0.4641737274000934 Loss: 0.8294904\n",
      "TRAIN: Batch: 0.46509106678230705 Loss: 1.0738181\n",
      "TRAIN: Batch: 0.46600840616452066 Loss: 1.6656735\n",
      "TRAIN: Batch: 0.46692574554673427 Loss: 1.6486428\n",
      "TRAIN: Batch: 0.4678430849289479 Loss: 3.232449\n",
      "TRAIN: Batch: 0.4687604243111615 Loss: 2.3683355\n",
      "TRAIN: Batch: 0.46967776369337516 Loss: 1.1936281\n",
      "TRAIN: Batch: 0.47059510307558877 Loss: 0.53540814\n",
      "TRAIN: Batch: 0.4715124424578024 Loss: 0.7681453\n",
      "TRAIN: Batch: 0.472429781840016 Loss: 2.269984\n",
      "TRAIN: Batch: 0.47334712122222966 Loss: 4.869413\n",
      "TRAIN: Batch: 0.47426446060444327 Loss: 0.54533243\n",
      "TRAIN: Batch: 0.4751817999866569 Loss: 0.48456857\n",
      "TRAIN: Batch: 0.4760991393688705 Loss: 0.4874799\n",
      "TRAIN: Batch: 0.4770164787510841 Loss: 0.7813438\n",
      "TRAIN: Batch: 0.47793381813329777 Loss: 1.9035261\n",
      "TRAIN: Batch: 0.4788511575155114 Loss: 0.741597\n",
      "TRAIN: Batch: 0.479768496897725 Loss: 1.5044563\n",
      "TRAIN: Batch: 0.4806858362799386 Loss: 2.6063385\n",
      "TRAIN: Batch: 0.48160317566215227 Loss: 0.77453935\n",
      "TRAIN: Batch: 0.4825205150443659 Loss: 0.4936459\n",
      "TRAIN: Batch: 0.4834378544265795 Loss: 0.9287\n",
      "TRAIN: Batch: 0.4843551938087931 Loss: 1.5750575\n",
      "TRAIN: Batch: 0.4852725331910067 Loss: 2.0401292\n",
      "TRAIN: Batch: 0.4861898725732204 Loss: 1.9149652\n",
      "TRAIN: Batch: 0.487107211955434 Loss: 1.4335744\n",
      "TRAIN: Batch: 0.4880245513376476 Loss: 2.004977\n",
      "TRAIN: Batch: 0.4889418907198612 Loss: 1.5015858\n",
      "TRAIN: Batch: 0.4898592301020748 Loss: 1.0137215\n",
      "TRAIN: Batch: 0.4907765694842885 Loss: 1.1707253\n",
      "TRAIN: Batch: 0.4916939088665021 Loss: 1.634151\n",
      "TRAIN: Batch: 0.4926112482487157 Loss: 1.6251626\n",
      "TRAIN: Batch: 0.4935285876309293 Loss: 0.674254\n",
      "TRAIN: Batch: 0.494445927013143 Loss: 2.3803043\n",
      "TRAIN: Batch: 0.4953632663953566 Loss: 1.3982394\n",
      "TRAIN: Batch: 0.4962806057775702 Loss: 0.6165453\n",
      "TRAIN: Batch: 0.4971979451597838 Loss: 2.191732\n",
      "TRAIN: Batch: 0.49811528454199744 Loss: 2.4178858\n",
      "TRAIN: Batch: 0.4990326239242111 Loss: 0.5571894\n",
      "TRAIN: Batch: 0.4999499633064247 Loss: 1.794498\n",
      "TRAIN: Batch: 0.5008673026886383 Loss: 1.2667547\n",
      "TRAIN: Batch: 0.5017846420708519 Loss: 4.9542456\n",
      "TRAIN: Batch: 0.5027019814530655 Loss: 2.4387767\n",
      "TRAIN: Batch: 0.5036193208352792 Loss: 0.82746404\n",
      "TRAIN: Batch: 0.5045366602174929 Loss: 1.0516686\n",
      "TRAIN: Batch: 0.5054539995997065 Loss: 1.3972334\n",
      "TRAIN: Batch: 0.5063713389819201 Loss: 0.64761794\n",
      "TRAIN: Batch: 0.5072886783641337 Loss: 1.7828678\n",
      "TRAIN: Batch: 0.5082060177463473 Loss: 1.488259\n",
      "TRAIN: Batch: 0.5091233571285609 Loss: 2.8575718\n",
      "TRAIN: Batch: 0.5100406965107745 Loss: 1.5137217\n",
      "TRAIN: Batch: 0.5109580358929882 Loss: 3.047264\n",
      "TRAIN: Batch: 0.5118753752752018 Loss: 1.4970906\n",
      "TRAIN: Batch: 0.5127927146574154 Loss: 0.50514805\n",
      "TRAIN: Batch: 0.5137100540396291 Loss: 0.75750303\n",
      "TRAIN: Batch: 0.5146273934218427 Loss: 1.1931055\n",
      "TRAIN: Batch: 0.5155447328040563 Loss: 0.67956305\n",
      "TRAIN: Batch: 0.5164620721862699 Loss: 0.635627\n",
      "TRAIN: Batch: 0.5173794115684835 Loss: 1.3863254\n",
      "TRAIN: Batch: 0.5182967509506972 Loss: 1.0771585\n",
      "TRAIN: Batch: 0.5192140903329108 Loss: 5.563679\n",
      "TRAIN: Batch: 0.5201314297151244 Loss: 1.8057561\n",
      "TRAIN: Batch: 0.521048769097338 Loss: 0.893655\n",
      "TRAIN: Batch: 0.5219661084795517 Loss: 2.961899\n",
      "TRAIN: Batch: 0.5228834478617653 Loss: 0.811753\n",
      "TRAIN: Batch: 0.5238007872439789 Loss: 1.3788809\n",
      "TRAIN: Batch: 0.5247181266261925 Loss: 0.90380216\n",
      "TRAIN: Batch: 0.5256354660084062 Loss: 6.275712\n",
      "TRAIN: Batch: 0.5265528053906198 Loss: 1.4321549\n",
      "TRAIN: Batch: 0.5274701447728334 Loss: 1.987992\n",
      "TRAIN: Batch: 0.528387484155047 Loss: 2.0203478\n",
      "TRAIN: Batch: 0.5293048235372606 Loss: 0.9420421\n",
      "TRAIN: Batch: 0.5302221629194743 Loss: 1.6956949\n",
      "TRAIN: Batch: 0.5311395023016879 Loss: 1.8395767\n",
      "TRAIN: Batch: 0.5320568416839015 Loss: 1.9677094\n",
      "TRAIN: Batch: 0.5329741810661152 Loss: 6.293312\n",
      "TRAIN: Batch: 0.5338915204483288 Loss: 2.8285594\n",
      "TRAIN: Batch: 0.5348088598305424 Loss: 1.6327227\n",
      "TRAIN: Batch: 0.535726199212756 Loss: 2.4513156\n",
      "TRAIN: Batch: 0.5366435385949696 Loss: 1.6441698\n",
      "TRAIN: Batch: 0.5375608779771832 Loss: 0.75948447\n",
      "TRAIN: Batch: 0.5384782173593969 Loss: 1.516203\n",
      "TRAIN: Batch: 0.5393955567416105 Loss: 1.3855183\n",
      "TRAIN: Batch: 0.5403128961238242 Loss: 0.6195188\n",
      "TRAIN: Batch: 0.5412302355060378 Loss: 0.61399406\n",
      "TRAIN: Batch: 0.5421475748882514 Loss: 1.1740137\n",
      "TRAIN: Batch: 0.543064914270465 Loss: 0.7008119\n",
      "TRAIN: Batch: 0.5439822536526786 Loss: 2.5921104\n",
      "TRAIN: Batch: 0.5448995930348922 Loss: 0.75885665\n",
      "TRAIN: Batch: 0.5458169324171058 Loss: 0.8322018\n",
      "TRAIN: Batch: 0.5467342717993195 Loss: 0.5051261\n",
      "TRAIN: Batch: 0.5476516111815332 Loss: 0.6272454\n",
      "TRAIN: Batch: 0.5485689505637468 Loss: 1.0877275\n",
      "TRAIN: Batch: 0.5494862899459604 Loss: 3.485769\n",
      "TRAIN: Batch: 0.550403629328174 Loss: 1.020998\n",
      "TRAIN: Batch: 0.5513209687103876 Loss: 0.5143213\n",
      "TRAIN: Batch: 0.5522383080926012 Loss: 1.524142\n",
      "TRAIN: Batch: 0.5531556474748148 Loss: 1.6504459\n",
      "TRAIN: Batch: 0.5540729868570284 Loss: 1.1814402\n",
      "TRAIN: Batch: 0.5549903262392422 Loss: 1.1788352\n",
      "TRAIN: Batch: 0.5559076656214558 Loss: 2.520215\n",
      "TRAIN: Batch: 0.5568250050036694 Loss: 11.148857\n",
      "TRAIN: Batch: 0.557742344385883 Loss: 0.5986774\n",
      "TRAIN: Batch: 0.5586596837680966 Loss: 1.6621177\n",
      "TRAIN: Batch: 0.5595770231503102 Loss: 1.3240246\n",
      "TRAIN: Batch: 0.5604943625325238 Loss: 0.5135963\n",
      "TRAIN: Batch: 0.5614117019147374 Loss: 2.3440092\n",
      "TRAIN: Batch: 0.562329041296951 Loss: 1.2221794\n",
      "TRAIN: Batch: 0.5632463806791648 Loss: 0.6722052\n",
      "TRAIN: Batch: 0.5641637200613784 Loss: 3.9309478\n",
      "TRAIN: Batch: 0.565081059443592 Loss: 0.87029386\n",
      "TRAIN: Batch: 0.5659983988258056 Loss: 2.1497552\n",
      "TRAIN: Batch: 0.5669157382080192 Loss: 0.7014586\n",
      "TRAIN: Batch: 0.5678330775902328 Loss: 1.2377005\n",
      "TRAIN: Batch: 0.5687504169724464 Loss: 1.0795336\n",
      "TRAIN: Batch: 0.56966775635466 Loss: 1.364986\n",
      "TRAIN: Batch: 0.5705850957368737 Loss: 0.85809106\n",
      "TRAIN: Batch: 0.5715024351190874 Loss: 0.5106889\n",
      "TRAIN: Batch: 0.572419774501301 Loss: 1.6556149\n",
      "TRAIN: Batch: 0.5733371138835146 Loss: 1.6635388\n",
      "TRAIN: Batch: 0.5742544532657282 Loss: 0.8077335\n",
      "TRAIN: Batch: 0.5751717926479418 Loss: 0.5993519\n",
      "TRAIN: Batch: 0.5760891320301554 Loss: 1.534355\n",
      "TRAIN: Batch: 0.577006471412369 Loss: 1.9310832\n",
      "TRAIN: Batch: 0.5779238107945827 Loss: 1.0268247\n",
      "TRAIN: Batch: 0.5788411501767963 Loss: 2.6405551\n",
      "TRAIN: Batch: 0.57975848955901 Loss: 0.48561588\n",
      "TRAIN: Batch: 0.5806758289412236 Loss: 2.2331328\n",
      "TRAIN: Batch: 0.5815931683234372 Loss: 4.19078\n",
      "TRAIN: Batch: 0.5825105077056508 Loss: 1.0178995\n",
      "TRAIN: Batch: 0.5834278470878644 Loss: 0.7875638\n",
      "TRAIN: Batch: 0.584345186470078 Loss: 1.9062485\n",
      "TRAIN: Batch: 0.5852625258522917 Loss: 1.1074619\n",
      "TRAIN: Batch: 0.5861798652345053 Loss: 2.5916085\n",
      "TRAIN: Batch: 0.5870972046167189 Loss: 0.8779822\n",
      "TRAIN: Batch: 0.5880145439989326 Loss: 2.0273373\n",
      "TRAIN: Batch: 0.5889318833811462 Loss: 1.4024521\n",
      "TRAIN: Batch: 0.5898492227633598 Loss: 1.0490566\n",
      "TRAIN: Batch: 0.5907665621455734 Loss: 0.5198158\n",
      "TRAIN: Batch: 0.591683901527787 Loss: 2.8317032\n",
      "TRAIN: Batch: 0.5926012409100007 Loss: 0.6647038\n",
      "TRAIN: Batch: 0.5935185802922143 Loss: 1.4629388\n",
      "TRAIN: Batch: 0.5944359196744279 Loss: 1.6122246\n",
      "TRAIN: Batch: 0.5953532590566415 Loss: 7.9399824\n",
      "TRAIN: Batch: 0.5962705984388552 Loss: 1.3577561\n",
      "TRAIN: Batch: 0.5971879378210688 Loss: 0.62145543\n",
      "TRAIN: Batch: 0.5981052772032824 Loss: 2.4858909\n",
      "TRAIN: Batch: 0.599022616585496 Loss: 0.62335193\n",
      "TRAIN: Batch: 0.5999399559677097 Loss: 1.1212163\n",
      "TRAIN: Batch: 0.6008572953499233 Loss: 1.1427643\n",
      "TRAIN: Batch: 0.6017746347321369 Loss: 1.1326333\n",
      "TRAIN: Batch: 0.6026919741143505 Loss: 1.2898631\n",
      "TRAIN: Batch: 0.6036093134965641 Loss: 1.2731702\n",
      "TRAIN: Batch: 0.6045266528787777 Loss: 0.48938042\n",
      "TRAIN: Batch: 0.6054439922609914 Loss: 1.555636\n",
      "TRAIN: Batch: 0.606361331643205 Loss: 1.7268523\n",
      "TRAIN: Batch: 0.6072786710254187 Loss: 1.2241434\n",
      "TRAIN: Batch: 0.6081960104076323 Loss: 2.5002878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.6091133497898459 Loss: 0.46981758\n",
      "TRAIN: Batch: 0.6100306891720595 Loss: 1.7293801\n",
      "TRAIN: Batch: 0.6109480285542731 Loss: 1.0846925\n",
      "TRAIN: Batch: 0.6118653679364867 Loss: 0.8005476\n",
      "TRAIN: Batch: 0.6127827073187003 Loss: 1.7202665\n",
      "TRAIN: Batch: 0.613700046700914 Loss: 1.4248798\n",
      "TRAIN: Batch: 0.6146173860831277 Loss: 1.4566619\n",
      "TRAIN: Batch: 0.6155347254653413 Loss: 1.0082055\n",
      "TRAIN: Batch: 0.6164520648475549 Loss: 1.0211358\n",
      "TRAIN: Batch: 0.6173694042297685 Loss: 0.96374136\n",
      "TRAIN: Batch: 0.6182867436119821 Loss: 2.3392107\n",
      "TRAIN: Batch: 0.6192040829941957 Loss: 2.3495\n",
      "TRAIN: Batch: 0.6201214223764093 Loss: 1.0220116\n",
      "TRAIN: Batch: 0.6210387617586229 Loss: 1.1633067\n",
      "TRAIN: Batch: 0.6219561011408367 Loss: 0.7208565\n",
      "TRAIN: Batch: 0.6228734405230503 Loss: 5.5385942\n",
      "TRAIN: Batch: 0.6237907799052639 Loss: 1.5250404\n",
      "TRAIN: Batch: 0.6247081192874775 Loss: 1.5016896\n",
      "TRAIN: Batch: 0.6256254586696911 Loss: 2.054995\n",
      "TRAIN: Batch: 0.6265427980519047 Loss: 0.923653\n",
      "TRAIN: Batch: 0.6274601374341183 Loss: 1.606917\n",
      "TRAIN: Batch: 0.6283774768163319 Loss: 0.58316797\n",
      "TRAIN: Batch: 0.6292948161985455 Loss: 0.5547683\n",
      "TRAIN: Batch: 0.6302121555807593 Loss: 4.542317\n",
      "TRAIN: Batch: 0.6311294949629729 Loss: 1.0799088\n",
      "TRAIN: Batch: 0.6320468343451865 Loss: 3.0888667\n",
      "TRAIN: Batch: 0.6329641737274001 Loss: 0.63875043\n",
      "TRAIN: Batch: 0.6338815131096137 Loss: 1.8559009\n",
      "TRAIN: Batch: 0.6347988524918273 Loss: 0.50990653\n",
      "TRAIN: Batch: 0.6357161918740409 Loss: 0.90963423\n",
      "TRAIN: Batch: 0.6366335312562545 Loss: 1.1892171\n",
      "TRAIN: Batch: 0.6375508706384682 Loss: 0.67135507\n",
      "TRAIN: Batch: 0.6384682100206819 Loss: 4.9953904\n",
      "TRAIN: Batch: 0.6393855494028955 Loss: 3.6103053\n",
      "TRAIN: Batch: 0.6403028887851091 Loss: 0.98193\n",
      "TRAIN: Batch: 0.6412202281673227 Loss: 1.9044772\n",
      "TRAIN: Batch: 0.6421375675495363 Loss: 1.2402405\n",
      "TRAIN: Batch: 0.6430549069317499 Loss: 0.9433563\n",
      "TRAIN: Batch: 0.6439722463139635 Loss: 2.4684377\n",
      "TRAIN: Batch: 0.6448895856961772 Loss: 0.67932045\n",
      "TRAIN: Batch: 0.6458069250783908 Loss: 2.9012134\n",
      "TRAIN: Batch: 0.6467242644606045 Loss: 1.6307305\n"
     ]
    }
   ],
   "source": [
    "decoderType = DecoderType.BestPath\n",
    "if args.beamsearch:\n",
    "    decoderType = DecoderType.BeamSearch\n",
    "elif args.wordbeamsearch:\n",
    "    decoderType = DecoderType.WordBeamSearch\n",
    "if args.train or args.validate:\n",
    "\n",
    "    # load training data, create TF model\n",
    "    # loader = DataLoader(FilePaths.fnTrain, args.batchsize, args.imgsize, Model.maxTextLen, args)\n",
    "    # testloader = DataLoader(FilePaths.fnTest, args.batchsize, args.imgsize, Model.maxTextLen, args, is_test=True)\n",
    "\n",
    "    # tnansforms#\n",
    "    transform_train = transforms.Compose([\n",
    "        # lambda img: (np.zeros([args.imgsize[1], args.imgsize[0]]) if img is None or np.min(img.shape) <= 1 else cv2.resize(img, (args.imgsize[1],args.imgsize[0]), interpolation=cv2.INTER_CUBIC)),\n",
    "        # lambda img: np.zeros([args.imgsize[1], args.imgsize[0]]) if (img is None or np.min(img.shape) <= 1) else cv2.resize(img, (args.imgsize[1],args.imgsize[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        transforms.Lambda(\n",
    "            lambda img: cv2.resize(img, (args.imgsize[0], args.imgsize[1]), interpolation=cv2.INTER_CUBIC)),\n",
    "        # (img, (32,128), interpolation=cv2.INTER_CUBIC),\n",
    "        #transforms.Lambda(lambda img: add_artifacts(img, args)),\n",
    "        # transforms.Lambda(lambda img: img_normalize(img)),\n",
    "        transforms.Lambda(lambda img: cv2.transpose(img))\n",
    "        # lambda img: cv2.resize(img, (args.imgsize[1], args.imgsize[0]), interpolation=cv2.INTER_CUBIC),\n",
    "    ])\n",
    "\n",
    "    # yike: to use torch.transform.classes, please convert numpy to PIL first. i.e.transforms.Resize(size=(args.imgsize[1], args.imgsize[0]), interpolation=PIL.Image.BICUBIC)\n",
    "\n",
    "    # instantiate datasets\n",
    "    iam = IAM(args.dataroot, transform=transform_train)\n",
    "    eydigits = EyDigitStrings(args.dataroot, transform=transform_train)\n",
    "    #printed = PRT(args.dataroot, transform=transform_train)  # yike todo\n",
    "    printed =PRT_WORD(args.dataroot,transform=transform_train)\n",
    "    irs = IRS(args.dataroot,transform=transform_train) #yike todo\n",
    "    freal=REAL(args.dataroot,transform=transform_train)\n",
    "\n",
    "    # tst=irs.__getitem__(1)\n",
    "    # print(type(tst[0]))\n",
    "    # print(tst[0].shape)\n",
    "    # cv2.imshow('tst',tst[0])\n",
    "    # cv2.imwrite('/root/Engagements/test/tst1.jpg', tst[0])\n",
    "    # concatenate datasets\n",
    "    concat =  ConcatDataset([iam, eydigits,irs,printed,freal]) # concatenate the multiple datasets yike notice!!!!\n",
    "    print(len(concat))\n",
    "    # concat= printed\n",
    "    # concat=eydigits\n",
    "    idxTrain = int(.95 * len(concat))\n",
    "    trainset, testset = random_split(concat, [idxTrain, len(concat) - idxTrain])\n",
    "    print(str(len(trainset) / 50))\n",
    "    print(str(len(testset) / 50))\n",
    "    trainloader = DataLoader(trainset, batch_size=args.batchsize_recg, shuffle=True, drop_last=True, num_workers=4)\n",
    "    validateloader = DataLoader(trainset, batch_size=args.batchsize_recg * 8, shuffle=False, drop_last=False, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=args.batchsize_recg * 8, shuffle=False, drop_last=False,\n",
    "                            num_workers=2)  # yike: feel not right\n",
    "    # testloader=DataLoader(testset, batch_size=args.batchsize,shuffle=False, num_workers=2) # yike: all test data included, no sampling. validation is not training. , sampler=SequentialSampler\n",
    "\n",
    "    # save characters of model for inference mode\n",
    "    charlist = list(set.union(set(iam.charList),set(eydigits.charList),set(irs.charList),set(printed.charList),set(freal.charList))) # yike notice !!!!\n",
    "    #charlist = list(set(printed.charList))\n",
    "    charlist.sort()\n",
    "    # charlist=eydigits.charList\n",
    "    open(join(args.ckptpath_recg, 'charList.txt'), 'w').write(str().join(charlist))\n",
    "\n",
    "    # # save words contained in dataset into file\n",
    "    # open(FilePaths.fnCorpus, 'w').write(str(' ').join(loader.trainWords + loader.validationWords))\n",
    "\n",
    "    # execute training or validation\n",
    "    if args.train:\n",
    "        model = Model(args, charList=charlist, loss_beta=0.6,loss_weight=[.5,.5], decoderType=DecoderType.BeamSearch,experiment=experiment,mustRestore_seg=False,mustRestore_recg=False, joint=False)  # yike: pay notice here, add mustRestore if wanna continue with pretrained model !!!!!!!!!\n",
    "        model.trainRecg(trainloader, validateloader=testloader, testloader=None)  # yike added validateloader !!!!!!!!!!\n",
    "    elif args.validate:\n",
    "        #model = Model(args, charlist, decoderType, mustRestore=True)\n",
    "        model = Model(args, charList=charlist, loss_beta=0.6,loss_weight=[.5,.5], decoderType=DecoderType.BeamSearch,experiment=experiment,mustRestore_seg=False,mustRestore_recg=True, joint=False)\n",
    "        model.validateRecg(testloader)\n",
    "\n",
    "# infer text on test image\n",
    "else:\n",
    "    print(open(join(args.ckptpath_recg, 'accuracy.txt')).read())\n",
    "    #model = Model(args, open(join(args.ckptpath, 'charList.txt')).read(), decoderType, mustRestore=True)\n",
    "    model = Model(args, charList=open(join(args.ckptpath_recg, 'charList.txt')).read(), loss_beta=0.6,loss_weight=[.5,.5], decoderType=DecoderType.BeamSearch,experiment=experiment,mustRestore_seg=False,mustRestore_recg=True, joint=False)\n",
    "    infer(model, FilePaths.fnInfer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_name('debug')\n",
    "experiment.log_parameters(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets_seg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkArgs():\n",
    "    if args.test:\n",
    "        print('The model is set to Testing')\n",
    "        print(\"Check point file: %s\"%args.ckptpath_seg)\n",
    "        #print(\"CamVid testing dir: %s\"%FLAGS.test_dir)\n",
    "    elif args.transfer:\n",
    "        print('The model is set to transfer learn from ckpt')\n",
    "        print(\"Check point file: %s\"%args.ckptpath_seg)\n",
    "        #print(\"CamVid Image dir: %s\"%FLAGS.image_dir)\n",
    "        #print(\"CamVid Val dir: %s\"%FLAGS.val_dir)\n",
    "    else:\n",
    "        print('The model is set to Training')\n",
    "        print(\"Max training Iteration: %d\"%args.max_epoch)\n",
    "        print(\"Initial lr: %f\"%args.lrInit)\n",
    "        print(\"First Drop Steps: %i\"%args.lrDrop1)\n",
    "        print(\"Second Drop Steps: %i\"%args.lrDrop2)\n",
    "        print(\"Data root: %s\"%args.data_root)\n",
    "        print(\"Check point file: %s\"%args.ckptpath_seg)\n",
    "        #print(\"CamVid Val dir: %s\"%FLAGS.val_dir)\n",
    "\n",
    "    print(\"Batch Size: %d\"%args.batch_size_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is set to Training\n",
      "Max training Iteration: 100\n",
      "Initial lr: 0.001000\n",
      "First Drop Steps: 10\n",
      "Second Drop Steps: 1000\n",
      "Data root: /root/datasets\n",
      "Check point file: /root/ckpt/debug_seg\n",
      "Batch Size: 10\n",
      "/root/datasets/artifact_images_no_intersect already exists, skipping download\n",
      "[0.1 0.9]\n",
      "GGG\n",
      "[None, 32, 128, 1]\n",
      "0 conv1: (?, ?, ?, 32)\n",
      "0 conv2: (?, ?, ?, 32)\n",
      "1 conv1: (?, ?, ?, 64)\n",
      "1 conv2: (?, ?, ?, 64)\n",
      "2 conv1: (?, ?, ?, 128)\n",
      "2 conv2: (?, ?, ?, 128)\n",
      "1 h_deconv: (?, ?, ?, 64)\n",
      "1 h_deconv_concat: (?, ?, ?, ?)\n",
      "1 h_conv1_post_deconv: (?, ?, ?, 64)\n",
      "1 h_conv2_post_deconv: (?, ?, ?, 64)\n",
      "0 h_deconv: (?, ?, ?, 32)\n",
      "0 h_deconv_concat: (?, ?, ?, ?)\n",
      "0 h_conv1_post_deconv: (?, ?, ?, 32)\n",
      "0 h_conv2_post_deconv: (?, ?, ?, 32)\n",
      "0 outmap: (?, ?, ?, 2)\n",
      "(?, ?, ?, 2)\n",
      "loss_seg: ()\n",
      "clean output from seg: (?, 32, 128)\n",
      "recg input: (?, 128, 32)\n",
      "shape of cnn input: [None, 128, 32]\n",
      "Build Densenet4htr model with 5 blocks, 9 bottleneck layers and 9 composite layers each.\n",
      "Depth: 96\n",
      "Reduction at transition layers: 0.4\n",
      "densenet feature extractor graph built in (sec): 6.272006273269653\n",
      "Total training params: 1.0M\n",
      "shape of cnn output: [None, 32, 1, 178]\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "INFO:tensorflow:Summary name graph_segmentation/loss/cross_entropy (raw) is illegal; using graph_segmentation/loss/cross_entropy__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name graph_segmentation/loss/cross_entropy (raw) is illegal; using graph_segmentation/loss/cross_entropy__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name add (raw) is illegal; using add__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name add (raw) is illegal; using add__raw_ instead.\n",
      "COMET ERROR: Failed to extract parameters from Estimator.init()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toto_loss_shape: Tensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET ERROR: Failed to extract parameters from Estimator.init()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.5.2 (default, Nov 12 2018, 13:43:14) \n",
      "[GCC 5.4.0 20160609]\n",
      "Tensorflow: 1.12.0-rc0\n",
      "Ran global_variables_initializer first\n",
      "Ran initializers.variables on segnet trainable variables\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1557: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1557: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/ckpt/debug_recg/model-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/ckpt/debug_recg/model-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init with stored values from /root/ckpt/debug_recg/model-1\n",
      "Epoch: 1  Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.0 Loss_Total: 69.04067\n",
      "TRAIN: Batch: 0.0 Loss_Seg: 119.83568\n",
      "accuracy = 0.790549\n",
      "mean IU  = 0.429998\n",
      "    class # 0 capture rate = 0.835037 \n",
      "    class # 1 capture rate = 0.225753 \n",
      "TRAIN: Batch: 0.003908998514580564 Loss_Total: 21.510185\n",
      "TRAIN: Batch: 0.003908998514580564 Loss_Seg: 27.185595\n",
      "accuracy = 0.861262\n",
      "mean IU  = 0.549289\n",
      "    class # 0 capture rate = 0.866055 \n",
      "    class # 1 capture rate = 0.782237 \n",
      "TRAIN: Batch: 0.007817997029161129 Loss_Total: 17.741007\n",
      "TRAIN: Batch: 0.007817997029161129 Loss_Seg: 22.68701\n",
      "accuracy = 0.901792\n",
      "mean IU  = 0.630465\n",
      "    class # 0 capture rate = 0.906639 \n",
      "    class # 1 capture rate = 0.834958 \n",
      "TRAIN: Batch: 0.011726995543741693 Loss_Total: 18.125402\n",
      "TRAIN: Batch: 0.011726995543741693 Loss_Seg: 22.558826\n",
      "accuracy = 0.859930\n",
      "mean IU  = 0.575830\n",
      "    class # 0 capture rate = 0.854794 \n",
      "    class # 1 capture rate = 0.934470 \n",
      "TRAIN: Batch: 0.015635994058322257 Loss_Total: 12.916971\n",
      "TRAIN: Batch: 0.015635994058322257 Loss_Seg: 13.756218\n",
      "accuracy = 0.929607\n",
      "mean IU  = 0.633690\n",
      "    class # 0 capture rate = 0.930230 \n",
      "    class # 1 capture rate = 0.914567 \n",
      "TRAIN: Batch: 0.019544992572902823 Loss_Total: 16.809547\n",
      "TRAIN: Batch: 0.019544992572902823 Loss_Seg: 13.622722\n",
      "accuracy = 0.929948\n",
      "mean IU  = 0.666396\n",
      "    class # 0 capture rate = 0.930740 \n",
      "    class # 1 capture rate = 0.915618 \n",
      "TRAIN: Batch: 0.023453991087483386 Loss_Total: 14.697517\n",
      "TRAIN: Batch: 0.023453991087483386 Loss_Seg: 14.835626\n",
      "accuracy = 0.935734\n",
      "mean IU  = 0.695401\n",
      "    class # 0 capture rate = 0.937942 \n",
      "    class # 1 capture rate = 0.901414 \n",
      "TRAIN: Batch: 0.02736298960206395 Loss_Total: 13.507683\n",
      "TRAIN: Batch: 0.02736298960206395 Loss_Seg: 13.844747\n",
      "accuracy = 0.924183\n",
      "mean IU  = 0.667831\n",
      "    class # 0 capture rate = 0.923218 \n",
      "    class # 1 capture rate = 0.940026 \n",
      "TRAIN: Batch: 0.031271988116644514 Loss_Total: 13.111177\n",
      "TRAIN: Batch: 0.031271988116644514 Loss_Seg: 15.270302\n",
      "accuracy = 0.943647\n",
      "mean IU  = 0.672515\n",
      "    class # 0 capture rate = 0.948273 \n",
      "    class # 1 capture rate = 0.845696 \n",
      "TRAIN: Batch: 0.035180986631225084 Loss_Total: 12.527312\n",
      "TRAIN: Batch: 0.035180986631225084 Loss_Seg: 14.036578\n",
      "accuracy = 0.912811\n",
      "mean IU  = 0.651500\n",
      "    class # 0 capture rate = 0.910413 \n",
      "    class # 1 capture rate = 0.950366 \n",
      "TRAIN: Batch: 0.039089985145805646 Loss_Total: 11.313669\n",
      "TRAIN: Batch: 0.039089985145805646 Loss_Seg: 11.249839\n",
      "accuracy = 0.956219\n",
      "mean IU  = 0.733175\n",
      "    class # 0 capture rate = 0.959240 \n",
      "    class # 1 capture rate = 0.900096 \n",
      "TRAIN: Batch: 0.04299898366038621 Loss_Total: 10.824783\n",
      "TRAIN: Batch: 0.04299898366038621 Loss_Seg: 10.823709\n",
      "accuracy = 0.959074\n",
      "mean IU  = 0.734023\n",
      "    class # 0 capture rate = 0.961589 \n",
      "    class # 1 capture rate = 0.908147 \n",
      "TRAIN: Batch: 0.04690798217496677 Loss_Total: 14.508962\n",
      "TRAIN: Batch: 0.04690798217496677 Loss_Seg: 15.587601\n",
      "accuracy = 0.932461\n",
      "mean IU  = 0.675451\n",
      "    class # 0 capture rate = 0.936487 \n",
      "    class # 1 capture rate = 0.865780 \n",
      "TRAIN: Batch: 0.05081698068954734 Loss_Total: 11.45956\n",
      "TRAIN: Batch: 0.05081698068954734 Loss_Seg: 12.039717\n",
      "accuracy = 0.945792\n",
      "mean IU  = 0.706735\n",
      "    class # 0 capture rate = 0.946521 \n",
      "    class # 1 capture rate = 0.932388 \n",
      "TRAIN: Batch: 0.0547259792041279 Loss_Total: 13.30666\n",
      "TRAIN: Batch: 0.0547259792041279 Loss_Seg: 13.41785\n",
      "accuracy = 0.929902\n",
      "mean IU  = 0.691892\n",
      "    class # 0 capture rate = 0.929003 \n",
      "    class # 1 capture rate = 0.943301 \n",
      "TRAIN: Batch: 0.058634977718708466 Loss_Total: 16.020103\n",
      "TRAIN: Batch: 0.058634977718708466 Loss_Seg: 19.058723\n",
      "accuracy = 0.916909\n",
      "mean IU  = 0.680628\n",
      "    class # 0 capture rate = 0.918902 \n",
      "    class # 1 capture rate = 0.892766 \n",
      "TRAIN: Batch: 0.06254397623328903 Loss_Total: 13.803257\n",
      "TRAIN: Batch: 0.06254397623328903 Loss_Seg: 13.990331\n",
      "accuracy = 0.938413\n",
      "mean IU  = 0.705543\n",
      "    class # 0 capture rate = 0.940157 \n",
      "    class # 1 capture rate = 0.911765 \n",
      "TRAIN: Batch: 0.0664529747478696 Loss_Total: 12.423037\n",
      "TRAIN: Batch: 0.0664529747478696 Loss_Seg: 12.762872\n",
      "accuracy = 0.942651\n",
      "mean IU  = 0.707645\n",
      "    class # 0 capture rate = 0.944513 \n",
      "    class # 1 capture rate = 0.911891 \n",
      "TRAIN: Batch: 0.07036197326245017 Loss_Total: 11.2107525\n",
      "TRAIN: Batch: 0.07036197326245017 Loss_Seg: 12.321178\n",
      "accuracy = 0.944131\n",
      "mean IU  = 0.689850\n",
      "    class # 0 capture rate = 0.944819 \n",
      "    class # 1 capture rate = 0.930136 \n",
      "TRAIN: Batch: 0.07427097177703072 Loss_Total: 10.802042\n",
      "TRAIN: Batch: 0.07427097177703072 Loss_Seg: 13.388422\n",
      "accuracy = 0.934178\n",
      "mean IU  = 0.676650\n",
      "    class # 0 capture rate = 0.935649 \n",
      "    class # 1 capture rate = 0.907919 \n",
      "TRAIN: Batch: 0.07817997029161129 Loss_Total: 10.555035\n",
      "TRAIN: Batch: 0.07817997029161129 Loss_Seg: 11.495695\n",
      "accuracy = 0.940866\n",
      "mean IU  = 0.686345\n",
      "    class # 0 capture rate = 0.941639 \n",
      "    class # 1 capture rate = 0.925908 \n",
      "TRAIN: Batch: 0.08208896880619185 Loss_Total: 15.520438\n",
      "TRAIN: Batch: 0.08208896880619185 Loss_Seg: 15.607566\n",
      "accuracy = 0.930256\n",
      "mean IU  = 0.706953\n",
      "    class # 0 capture rate = 0.930413 \n",
      "    class # 1 capture rate = 0.928231 \n",
      "TRAIN: Batch: 0.08599796732077242 Loss_Total: 13.5295105\n",
      "TRAIN: Batch: 0.08599796732077242 Loss_Seg: 15.622163\n",
      "accuracy = 0.937959\n",
      "mean IU  = 0.677649\n",
      "    class # 0 capture rate = 0.944211 \n",
      "    class # 1 capture rate = 0.828983 \n",
      "TRAIN: Batch: 0.08990696583535299 Loss_Total: 12.695314\n",
      "TRAIN: Batch: 0.08990696583535299 Loss_Seg: 12.244729\n",
      "accuracy = 0.941352\n",
      "mean IU  = 0.694987\n",
      "    class # 0 capture rate = 0.941588 \n",
      "    class # 1 capture rate = 0.936997 \n",
      "TRAIN: Batch: 0.09381596434993354 Loss_Total: 15.429348\n",
      "TRAIN: Batch: 0.09381596434993354 Loss_Seg: 15.805546\n",
      "accuracy = 0.940514\n",
      "mean IU  = 0.702790\n",
      "    class # 0 capture rate = 0.943914 \n",
      "    class # 1 capture rate = 0.886411 \n",
      "TRAIN: Batch: 0.09772496286451411 Loss_Total: 14.271118\n",
      "TRAIN: Batch: 0.09772496286451411 Loss_Seg: 15.002007\n",
      "accuracy = 0.932661\n",
      "mean IU  = 0.650315\n",
      "    class # 0 capture rate = 0.935444 \n",
      "    class # 1 capture rate = 0.874126 \n",
      "TRAIN: Batch: 0.10163396137909468 Loss_Total: 15.05077\n",
      "TRAIN: Batch: 0.10163396137909468 Loss_Seg: 11.665795\n",
      "accuracy = 0.941135\n",
      "mean IU  = 0.735888\n",
      "    class # 0 capture rate = 0.939888 \n",
      "    class # 1 capture rate = 0.957528 \n",
      "TRAIN: Batch: 0.10554295989367524 Loss_Total: 11.483133\n",
      "TRAIN: Batch: 0.10554295989367524 Loss_Seg: 13.958735\n",
      "accuracy = 0.938286\n",
      "mean IU  = 0.688212\n",
      "    class # 0 capture rate = 0.939877 \n",
      "    class # 1 capture rate = 0.910169 \n",
      "TRAIN: Batch: 0.1094519584082558 Loss_Total: 9.744625\n",
      "TRAIN: Batch: 0.1094519584082558 Loss_Seg: 12.471877\n",
      "accuracy = 0.935687\n",
      "mean IU  = 0.675934\n",
      "    class # 0 capture rate = 0.936560 \n",
      "    class # 1 capture rate = 0.919285 \n",
      "TRAIN: Batch: 0.11336095692283638 Loss_Total: 15.502916\n",
      "TRAIN: Batch: 0.11336095692283638 Loss_Seg: 18.856297\n",
      "accuracy = 0.915079\n",
      "mean IU  = 0.658835\n",
      "    class # 0 capture rate = 0.919029 \n",
      "    class # 1 capture rate = 0.860891 \n",
      "TRAIN: Batch: 0.11726995543741693 Loss_Total: 12.389325\n",
      "TRAIN: Batch: 0.11726995543741693 Loss_Seg: 11.545678\n",
      "accuracy = 0.948188\n",
      "mean IU  = 0.731378\n",
      "    class # 0 capture rate = 0.948301 \n",
      "    class # 1 capture rate = 0.946384 \n",
      "TRAIN: Batch: 0.1211789539519975 Loss_Total: 16.107105\n",
      "TRAIN: Batch: 0.1211789539519975 Loss_Seg: 15.915318\n",
      "accuracy = 0.915830\n",
      "mean IU  = 0.698229\n",
      "    class # 0 capture rate = 0.913047 \n",
      "    class # 1 capture rate = 0.945868 \n",
      "TRAIN: Batch: 0.12508795246657806 Loss_Total: 18.203133\n",
      "TRAIN: Batch: 0.12508795246657806 Loss_Seg: 17.977179\n",
      "accuracy = 0.942037\n",
      "mean IU  = 0.733439\n",
      "    class # 0 capture rate = 0.950036 \n",
      "    class # 1 capture rate = 0.846032 \n",
      "TRAIN: Batch: 0.12899695098115863 Loss_Total: 15.4699745\n",
      "TRAIN: Batch: 0.12899695098115863 Loss_Seg: 16.22386\n",
      "accuracy = 0.934292\n",
      "mean IU  = 0.683314\n",
      "    class # 0 capture rate = 0.937749 \n",
      "    class # 1 capture rate = 0.877956 \n",
      "TRAIN: Batch: 0.1329059494957392 Loss_Total: 15.863392\n",
      "TRAIN: Batch: 0.1329059494957392 Loss_Seg: 19.3914\n",
      "accuracy = 0.927438\n",
      "mean IU  = 0.646942\n",
      "    class # 0 capture rate = 0.935038 \n",
      "    class # 1 capture rate = 0.793355 \n",
      "TRAIN: Batch: 0.13681494801031976 Loss_Total: 12.344643\n",
      "TRAIN: Batch: 0.13681494801031976 Loss_Seg: 11.115068\n",
      "accuracy = 0.954358\n",
      "mean IU  = 0.732540\n",
      "    class # 0 capture rate = 0.955705 \n",
      "    class # 1 capture rate = 0.929651 \n",
      "TRAIN: Batch: 0.14072394652490033 Loss_Total: 12.37229\n",
      "TRAIN: Batch: 0.14072394652490033 Loss_Seg: 11.136979\n",
      "accuracy = 0.937790\n",
      "mean IU  = 0.698136\n",
      "    class # 0 capture rate = 0.936913 \n",
      "    class # 1 capture rate = 0.952547 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.14463294503948088 Loss_Total: 14.03543\n",
      "TRAIN: Batch: 0.14463294503948088 Loss_Seg: 12.1186\n",
      "accuracy = 0.947939\n",
      "mean IU  = 0.705166\n",
      "    class # 0 capture rate = 0.950188 \n",
      "    class # 1 capture rate = 0.905181 \n",
      "TRAIN: Batch: 0.14854194355406145 Loss_Total: 17.54755\n",
      "TRAIN: Batch: 0.14854194355406145 Loss_Seg: 18.19168\n",
      "accuracy = 0.917833\n",
      "mean IU  = 0.665792\n",
      "    class # 0 capture rate = 0.923202 \n",
      "    class # 1 capture rate = 0.846476 \n",
      "TRAIN: Batch: 0.15245094206864201 Loss_Total: 17.145653\n",
      "TRAIN: Batch: 0.15245094206864201 Loss_Seg: 17.494654\n",
      "accuracy = 0.926140\n",
      "mean IU  = 0.704982\n",
      "    class # 0 capture rate = 0.929475 \n",
      "    class # 1 capture rate = 0.887665 \n",
      "TRAIN: Batch: 0.15635994058322258 Loss_Total: 12.789807\n",
      "TRAIN: Batch: 0.15635994058322258 Loss_Seg: 16.069374\n",
      "accuracy = 0.934038\n",
      "mean IU  = 0.714372\n",
      "    class # 0 capture rate = 0.935679 \n",
      "    class # 1 capture rate = 0.912911 \n",
      "TRAIN: Batch: 0.16026893909780315 Loss_Total: 15.265148\n",
      "TRAIN: Batch: 0.16026893909780315 Loss_Seg: 12.110039\n",
      "accuracy = 0.941840\n",
      "mean IU  = 0.739949\n",
      "    class # 0 capture rate = 0.941063 \n",
      "    class # 1 capture rate = 0.951787 \n",
      "TRAIN: Batch: 0.1641779376123837 Loss_Total: 12.631971\n",
      "TRAIN: Batch: 0.1641779376123837 Loss_Seg: 14.018041\n",
      "accuracy = 0.934263\n",
      "mean IU  = 0.708752\n",
      "    class # 0 capture rate = 0.934476 \n",
      "    class # 1 capture rate = 0.931298 \n",
      "TRAIN: Batch: 0.16808693612696426 Loss_Total: 15.261749\n",
      "TRAIN: Batch: 0.16808693612696426 Loss_Seg: 12.026118\n",
      "accuracy = 0.933973\n",
      "mean IU  = 0.681149\n",
      "    class # 0 capture rate = 0.933148 \n",
      "    class # 1 capture rate = 0.948753 \n",
      "TRAIN: Batch: 0.17199593464154483 Loss_Total: 14.322638\n",
      "TRAIN: Batch: 0.17199593464154483 Loss_Seg: 14.2443495\n",
      "accuracy = 0.934105\n",
      "mean IU  = 0.691967\n",
      "    class # 0 capture rate = 0.935534 \n",
      "    class # 1 capture rate = 0.911717 \n",
      "TRAIN: Batch: 0.1759049331561254 Loss_Total: 10.447521\n",
      "TRAIN: Batch: 0.1759049331561254 Loss_Seg: 11.346286\n",
      "accuracy = 0.940819\n",
      "mean IU  = 0.697353\n",
      "    class # 0 capture rate = 0.940763 \n",
      "    class # 1 capture rate = 0.941828 \n",
      "TRAIN: Batch: 0.17981393167070597 Loss_Total: 15.1854\n",
      "TRAIN: Batch: 0.17981393167070597 Loss_Seg: 16.733007\n",
      "accuracy = 0.935696\n",
      "mean IU  = 0.718127\n",
      "    class # 0 capture rate = 0.937551 \n",
      "    class # 1 capture rate = 0.911775 \n",
      "TRAIN: Batch: 0.18372293018528654 Loss_Total: 10.913844\n",
      "TRAIN: Batch: 0.18372293018528654 Loss_Seg: 11.4720745\n",
      "accuracy = 0.945987\n",
      "mean IU  = 0.719484\n",
      "    class # 0 capture rate = 0.947009 \n",
      "    class # 1 capture rate = 0.929151 \n",
      "TRAIN: Batch: 0.18763192869986708 Loss_Total: 13.570374\n",
      "TRAIN: Batch: 0.18763192869986708 Loss_Seg: 14.205524\n",
      "accuracy = 0.936102\n",
      "mean IU  = 0.662240\n",
      "    class # 0 capture rate = 0.939130 \n",
      "    class # 1 capture rate = 0.874610 \n",
      "TRAIN: Batch: 0.19154092721444765 Loss_Total: 15.16357\n",
      "TRAIN: Batch: 0.19154092721444765 Loss_Seg: 12.356281\n",
      "accuracy = 0.945428\n",
      "mean IU  = 0.726376\n",
      "    class # 0 capture rate = 0.946692 \n",
      "    class # 1 capture rate = 0.926132 \n",
      "TRAIN: Batch: 0.19544992572902822 Loss_Total: 10.33518\n",
      "TRAIN: Batch: 0.19544992572902822 Loss_Seg: 10.037127\n",
      "accuracy = 0.954270\n",
      "mean IU  = 0.755527\n",
      "    class # 0 capture rate = 0.954269 \n",
      "    class # 1 capture rate = 0.954290 \n",
      "TRAIN: Batch: 0.1993589242436088 Loss_Total: 11.756004\n",
      "TRAIN: Batch: 0.1993589242436088 Loss_Seg: 9.454655\n",
      "accuracy = 0.963129\n",
      "mean IU  = 0.775425\n",
      "    class # 0 capture rate = 0.964822 \n",
      "    class # 1 capture rate = 0.934970 \n",
      "TRAIN: Batch: 0.20326792275818936 Loss_Total: 11.374859\n",
      "TRAIN: Batch: 0.20326792275818936 Loss_Seg: 9.613844\n",
      "accuracy = 0.946261\n",
      "mean IU  = 0.687331\n",
      "    class # 0 capture rate = 0.946609 \n",
      "    class # 1 capture rate = 0.938557 \n",
      "TRAIN: Batch: 0.2071769212727699 Loss_Total: 11.859783\n",
      "TRAIN: Batch: 0.2071769212727699 Loss_Seg: 14.597792\n",
      "accuracy = 0.937936\n",
      "mean IU  = 0.705425\n",
      "    class # 0 capture rate = 0.940106 \n",
      "    class # 1 capture rate = 0.905358 \n",
      "TRAIN: Batch: 0.21108591978735047 Loss_Total: 12.445778\n",
      "TRAIN: Batch: 0.21108591978735047 Loss_Seg: 15.195233\n",
      "accuracy = 0.943694\n",
      "mean IU  = 0.703919\n",
      "    class # 0 capture rate = 0.948887 \n",
      "    class # 1 capture rate = 0.858539 \n",
      "TRAIN: Batch: 0.21499491830193104 Loss_Total: 12.212589\n",
      "TRAIN: Batch: 0.21499491830193104 Loss_Seg: 12.161196\n",
      "accuracy = 0.946843\n",
      "mean IU  = 0.715469\n",
      "    class # 0 capture rate = 0.948482 \n",
      "    class # 1 capture rate = 0.918631 \n",
      "TRAIN: Batch: 0.2189039168165116 Loss_Total: 16.874119\n",
      "TRAIN: Batch: 0.2189039168165116 Loss_Seg: 11.265894\n",
      "accuracy = 0.939813\n",
      "mean IU  = 0.722579\n",
      "    class # 0 capture rate = 0.939399 \n",
      "    class # 1 capture rate = 0.945676 \n",
      "TRAIN: Batch: 0.22281291533109218 Loss_Total: 12.581415\n",
      "TRAIN: Batch: 0.22281291533109218 Loss_Seg: 14.724387\n",
      "accuracy = 0.929536\n",
      "mean IU  = 0.687701\n",
      "    class # 0 capture rate = 0.929315 \n",
      "    class # 1 capture rate = 0.932886 \n",
      "TRAIN: Batch: 0.22672191384567275 Loss_Total: 12.394133\n",
      "TRAIN: Batch: 0.22672191384567275 Loss_Seg: 13.755363\n",
      "accuracy = 0.932389\n",
      "mean IU  = 0.701576\n",
      "    class # 0 capture rate = 0.931226 \n",
      "    class # 1 capture rate = 0.949262 \n",
      "TRAIN: Batch: 0.2306309123602533 Loss_Total: 13.00169\n",
      "TRAIN: Batch: 0.2306309123602533 Loss_Seg: 17.232265\n",
      "accuracy = 0.935049\n",
      "mean IU  = 0.685313\n",
      "    class # 0 capture rate = 0.941280 \n",
      "    class # 1 capture rate = 0.838502 \n",
      "TRAIN: Batch: 0.23453991087483386 Loss_Total: 10.964634\n",
      "TRAIN: Batch: 0.23453991087483386 Loss_Seg: 8.651698\n",
      "accuracy = 0.959907\n",
      "mean IU  = 0.763460\n",
      "    class # 0 capture rate = 0.959693 \n",
      "    class # 1 capture rate = 0.963588 \n",
      "TRAIN: Batch: 0.23844890938941443 Loss_Total: 13.102877\n",
      "TRAIN: Batch: 0.23844890938941443 Loss_Seg: 11.881603\n",
      "accuracy = 0.945040\n",
      "mean IU  = 0.712652\n",
      "    class # 0 capture rate = 0.946741 \n",
      "    class # 1 capture rate = 0.916413 \n",
      "TRAIN: Batch: 0.242357907903995 Loss_Total: 10.075125\n",
      "TRAIN: Batch: 0.242357907903995 Loss_Seg: 10.416106\n",
      "accuracy = 0.955341\n",
      "mean IU  = 0.752880\n",
      "    class # 0 capture rate = 0.956636 \n",
      "    class # 1 capture rate = 0.934738 \n",
      "TRAIN: Batch: 0.24626690641857557 Loss_Total: 10.122164\n",
      "TRAIN: Batch: 0.24626690641857557 Loss_Seg: 14.090876\n",
      "accuracy = 0.929386\n",
      "mean IU  = 0.703617\n",
      "    class # 0 capture rate = 0.928943 \n",
      "    class # 1 capture rate = 0.935227 \n",
      "TRAIN: Batch: 0.2501759049331561 Loss_Total: 10.772703\n",
      "TRAIN: Batch: 0.2501759049331561 Loss_Seg: 10.477332\n",
      "accuracy = 0.956907\n",
      "mean IU  = 0.754991\n",
      "    class # 0 capture rate = 0.958560 \n",
      "    class # 1 capture rate = 0.930017 \n",
      "TRAIN: Batch: 0.2540849034477367 Loss_Total: 15.217247\n",
      "TRAIN: Batch: 0.2540849034477367 Loss_Seg: 17.614422\n",
      "accuracy = 0.946302\n",
      "mean IU  = 0.731480\n",
      "    class # 0 capture rate = 0.952768 \n",
      "    class # 1 capture rate = 0.857451 \n",
      "TRAIN: Batch: 0.25799390196231725 Loss_Total: 11.486933\n",
      "TRAIN: Batch: 0.25799390196231725 Loss_Seg: 11.297552\n",
      "accuracy = 0.936639\n",
      "mean IU  = 0.684139\n",
      "    class # 0 capture rate = 0.935649 \n",
      "    class # 1 capture rate = 0.955045 \n",
      "TRAIN: Batch: 0.2619029004768978 Loss_Total: 13.554228\n",
      "TRAIN: Batch: 0.2619029004768978 Loss_Seg: 16.744831\n",
      "accuracy = 0.926660\n",
      "mean IU  = 0.706130\n",
      "    class # 0 capture rate = 0.928342 \n",
      "    class # 1 capture rate = 0.906788 \n",
      "TRAIN: Batch: 0.2658118989914784 Loss_Total: 12.323319\n",
      "TRAIN: Batch: 0.2658118989914784 Loss_Seg: 13.632186\n",
      "accuracy = 0.944636\n",
      "mean IU  = 0.719932\n",
      "    class # 0 capture rate = 0.947509 \n",
      "    class # 1 capture rate = 0.900480 \n",
      "TRAIN: Batch: 0.26972089750605893 Loss_Total: 12.99631\n",
      "TRAIN: Batch: 0.26972089750605893 Loss_Seg: 12.194377\n",
      "accuracy = 0.949242\n",
      "mean IU  = 0.732141\n",
      "    class # 0 capture rate = 0.951301 \n",
      "    class # 1 capture rate = 0.916701 \n",
      "TRAIN: Batch: 0.27362989602063953 Loss_Total: 12.725039\n",
      "TRAIN: Batch: 0.27362989602063953 Loss_Seg: 15.218625\n",
      "accuracy = 0.936620\n",
      "mean IU  = 0.708105\n",
      "    class # 0 capture rate = 0.937756 \n",
      "    class # 1 capture rate = 0.920136 \n",
      "TRAIN: Batch: 0.27753889453522007 Loss_Total: 12.818572\n",
      "TRAIN: Batch: 0.27753889453522007 Loss_Seg: 14.094648\n",
      "accuracy = 0.933874\n",
      "mean IU  = 0.667862\n",
      "    class # 0 capture rate = 0.935398 \n",
      "    class # 1 capture rate = 0.904762 \n",
      "TRAIN: Batch: 0.28144789304980067 Loss_Total: 12.009129\n",
      "TRAIN: Batch: 0.28144789304980067 Loss_Seg: 13.26889\n",
      "accuracy = 0.935855\n",
      "mean IU  = 0.706920\n",
      "    class # 0 capture rate = 0.935792 \n",
      "    class # 1 capture rate = 0.936782 \n",
      "TRAIN: Batch: 0.2853568915643812 Loss_Total: 11.871309\n",
      "TRAIN: Batch: 0.2853568915643812 Loss_Seg: 12.306395\n",
      "accuracy = 0.939519\n",
      "mean IU  = 0.710817\n",
      "    class # 0 capture rate = 0.939894 \n",
      "    class # 1 capture rate = 0.933759 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.28926589007896175 Loss_Total: 14.657756\n",
      "TRAIN: Batch: 0.28926589007896175 Loss_Seg: 16.152006\n",
      "accuracy = 0.928060\n",
      "mean IU  = 0.681733\n",
      "    class # 0 capture rate = 0.930984 \n",
      "    class # 1 capture rate = 0.885145 \n",
      "TRAIN: Batch: 0.29317488859354235 Loss_Total: 12.882719\n",
      "TRAIN: Batch: 0.29317488859354235 Loss_Seg: 15.386385\n",
      "accuracy = 0.937289\n",
      "mean IU  = 0.700756\n",
      "    class # 0 capture rate = 0.939995 \n",
      "    class # 1 capture rate = 0.896032 \n",
      "TRAIN: Batch: 0.2970838871081229 Loss_Total: 10.805511\n",
      "TRAIN: Batch: 0.2970838871081229 Loss_Seg: 12.468171\n",
      "accuracy = 0.946091\n",
      "mean IU  = 0.716433\n",
      "    class # 0 capture rate = 0.947615 \n",
      "    class # 1 capture rate = 0.920504 \n",
      "TRAIN: Batch: 0.3009928856227035 Loss_Total: 13.404684\n",
      "TRAIN: Batch: 0.3009928856227035 Loss_Seg: 14.055597\n",
      "accuracy = 0.931420\n",
      "mean IU  = 0.692918\n",
      "    class # 0 capture rate = 0.932680 \n",
      "    class # 1 capture rate = 0.912893 \n",
      "TRAIN: Batch: 0.30490188413728403 Loss_Total: 12.158969\n",
      "TRAIN: Batch: 0.30490188413728403 Loss_Seg: 14.425596\n",
      "accuracy = 0.926165\n",
      "mean IU  = 0.709331\n",
      "    class # 0 capture rate = 0.925049 \n",
      "    class # 1 capture rate = 0.939337 \n",
      "TRAIN: Batch: 0.30881088265186457 Loss_Total: 12.592354\n",
      "TRAIN: Batch: 0.30881088265186457 Loss_Seg: 11.44251\n",
      "accuracy = 0.944959\n",
      "mean IU  = 0.711797\n",
      "    class # 0 capture rate = 0.946425 \n",
      "    class # 1 capture rate = 0.920035 \n",
      "TRAIN: Batch: 0.31271988116644517 Loss_Total: 14.826992\n",
      "TRAIN: Batch: 0.31271988116644517 Loss_Seg: 15.042008\n",
      "accuracy = 0.929290\n",
      "mean IU  = 0.690441\n",
      "    class # 0 capture rate = 0.930437 \n",
      "    class # 1 capture rate = 0.912782 \n",
      "TRAIN: Batch: 0.3166288796810257 Loss_Total: 12.75359\n",
      "TRAIN: Batch: 0.3166288796810257 Loss_Seg: 12.122032\n",
      "accuracy = 0.930614\n",
      "mean IU  = 0.696066\n",
      "    class # 0 capture rate = 0.929475 \n",
      "    class # 1 capture rate = 0.947288 \n",
      "TRAIN: Batch: 0.3205378781956063 Loss_Total: 12.810575\n",
      "TRAIN: Batch: 0.3205378781956063 Loss_Seg: 10.7941475\n",
      "accuracy = 0.950680\n",
      "mean IU  = 0.740183\n",
      "    class # 0 capture rate = 0.951484 \n",
      "    class # 1 capture rate = 0.938060 \n",
      "TRAIN: Batch: 0.32444687671018685 Loss_Total: 12.706646\n",
      "TRAIN: Batch: 0.32444687671018685 Loss_Seg: 10.463043\n",
      "accuracy = 0.950357\n",
      "mean IU  = 0.735136\n",
      "    class # 0 capture rate = 0.950718 \n",
      "    class # 1 capture rate = 0.944444 \n",
      "TRAIN: Batch: 0.3283558752247674 Loss_Total: 11.016581\n",
      "TRAIN: Batch: 0.3283558752247674 Loss_Seg: 12.25953\n",
      "accuracy = 0.939542\n",
      "mean IU  = 0.703354\n",
      "    class # 0 capture rate = 0.939405 \n",
      "    class # 1 capture rate = 0.941806 \n",
      "TRAIN: Batch: 0.332264873739348 Loss_Total: 11.694708\n",
      "TRAIN: Batch: 0.332264873739348 Loss_Seg: 11.323074\n",
      "accuracy = 0.952147\n",
      "mean IU  = 0.742122\n",
      "    class # 0 capture rate = 0.952881 \n",
      "    class # 1 capture rate = 0.940342 \n",
      "TRAIN: Batch: 0.33617387225392853 Loss_Total: 8.663042\n",
      "TRAIN: Batch: 0.33617387225392853 Loss_Seg: 9.116441\n",
      "accuracy = 0.950922\n",
      "mean IU  = 0.706584\n",
      "    class # 0 capture rate = 0.950461 \n",
      "    class # 1 capture rate = 0.960860 \n",
      "TRAIN: Batch: 0.3400828707685091 Loss_Total: 10.565762\n",
      "TRAIN: Batch: 0.3400828707685091 Loss_Seg: 11.563172\n",
      "accuracy = 0.938342\n",
      "mean IU  = 0.708215\n",
      "    class # 0 capture rate = 0.938165 \n",
      "    class # 1 capture rate = 0.941082 \n",
      "TRAIN: Batch: 0.34399186928308967 Loss_Total: 13.151674\n",
      "TRAIN: Batch: 0.34399186928308967 Loss_Seg: 11.2811365\n",
      "accuracy = 0.938646\n",
      "mean IU  = 0.717513\n",
      "    class # 0 capture rate = 0.937555 \n",
      "    class # 1 capture rate = 0.954494 \n",
      "TRAIN: Batch: 0.3479008677976702 Loss_Total: 13.504065\n",
      "TRAIN: Batch: 0.3479008677976702 Loss_Seg: 12.191048\n",
      "accuracy = 0.934457\n",
      "mean IU  = 0.706224\n",
      "    class # 0 capture rate = 0.933497 \n",
      "    class # 1 capture rate = 0.948407 \n",
      "TRAIN: Batch: 0.3518098663122508 Loss_Total: 12.545227\n",
      "TRAIN: Batch: 0.3518098663122508 Loss_Seg: 13.617873\n",
      "accuracy = 0.935222\n",
      "mean IU  = 0.713739\n",
      "    class # 0 capture rate = 0.935728 \n",
      "    class # 1 capture rate = 0.928369 \n",
      "TRAIN: Batch: 0.35571886482683135 Loss_Total: 12.454511\n",
      "TRAIN: Batch: 0.35571886482683135 Loss_Seg: 12.532657\n",
      "accuracy = 0.940697\n",
      "mean IU  = 0.711711\n",
      "    class # 0 capture rate = 0.941016 \n",
      "    class # 1 capture rate = 0.935694 \n",
      "TRAIN: Batch: 0.35962786334141195 Loss_Total: 12.732819\n",
      "TRAIN: Batch: 0.35962786334141195 Loss_Seg: 10.631586\n",
      "accuracy = 0.938060\n",
      "mean IU  = 0.701750\n",
      "    class # 0 capture rate = 0.936386 \n",
      "    class # 1 capture rate = 0.965948 \n",
      "TRAIN: Batch: 0.3635368618559925 Loss_Total: 15.759346\n",
      "TRAIN: Batch: 0.3635368618559925 Loss_Seg: 15.588062\n",
      "accuracy = 0.939206\n",
      "mean IU  = 0.736009\n",
      "    class # 0 capture rate = 0.941611 \n",
      "    class # 1 capture rate = 0.910607 \n",
      "TRAIN: Batch: 0.3674458603705731 Loss_Total: 18.704477\n",
      "TRAIN: Batch: 0.3674458603705731 Loss_Seg: 20.272806\n",
      "accuracy = 0.931266\n",
      "mean IU  = 0.706490\n",
      "    class # 0 capture rate = 0.938067 \n",
      "    class # 1 capture rate = 0.849363 \n",
      "TRAIN: Batch: 0.3713548588851536 Loss_Total: 15.160987\n",
      "TRAIN: Batch: 0.3713548588851536 Loss_Seg: 16.362041\n",
      "accuracy = 0.933503\n",
      "mean IU  = 0.708437\n",
      "    class # 0 capture rate = 0.936550 \n",
      "    class # 1 capture rate = 0.893558 \n",
      "TRAIN: Batch: 0.37526385739973417 Loss_Total: 13.382025\n",
      "TRAIN: Batch: 0.37526385739973417 Loss_Seg: 14.459449\n",
      "accuracy = 0.938739\n",
      "mean IU  = 0.714446\n",
      "    class # 0 capture rate = 0.940631 \n",
      "    class # 1 capture rate = 0.911798 \n",
      "TRAIN: Batch: 0.37917285591431477 Loss_Total: 14.092209\n",
      "TRAIN: Batch: 0.37917285591431477 Loss_Seg: 11.532337\n",
      "accuracy = 0.942768\n",
      "mean IU  = 0.705229\n",
      "    class # 0 capture rate = 0.942486 \n",
      "    class # 1 capture rate = 0.947727 \n",
      "TRAIN: Batch: 0.3830818544288953 Loss_Total: 11.411718\n",
      "TRAIN: Batch: 0.3830818544288953 Loss_Seg: 13.294485\n",
      "accuracy = 0.927759\n",
      "mean IU  = 0.669080\n",
      "    class # 0 capture rate = 0.926610 \n",
      "    class # 1 capture rate = 0.947917 \n",
      "TRAIN: Batch: 0.3869908529434759 Loss_Total: 10.511551\n",
      "TRAIN: Batch: 0.3869908529434759 Loss_Seg: 10.24119\n",
      "accuracy = 0.955497\n",
      "mean IU  = 0.750036\n",
      "    class # 0 capture rate = 0.955831 \n",
      "    class # 1 capture rate = 0.949935 \n",
      "TRAIN: Batch: 0.39089985145805645 Loss_Total: 11.444076\n",
      "TRAIN: Batch: 0.39089985145805645 Loss_Seg: 11.404271\n",
      "accuracy = 0.947424\n",
      "mean IU  = 0.747025\n",
      "    class # 0 capture rate = 0.947246 \n",
      "    class # 1 capture rate = 0.949874 \n",
      "TRAIN: Batch: 0.394808849972637 Loss_Total: 12.366951\n",
      "TRAIN: Batch: 0.394808849972637 Loss_Seg: 11.617069\n",
      "accuracy = 0.948853\n",
      "mean IU  = 0.697009\n",
      "    class # 0 capture rate = 0.950954 \n",
      "    class # 1 capture rate = 0.905016 \n",
      "TRAIN: Batch: 0.3987178484872176 Loss_Total: 10.881131\n",
      "TRAIN: Batch: 0.3987178484872176 Loss_Seg: 12.025453\n",
      "accuracy = 0.943351\n",
      "mean IU  = 0.694122\n",
      "    class # 0 capture rate = 0.945149 \n",
      "    class # 1 capture rate = 0.909531 \n",
      "TRAIN: Batch: 0.4026268470017981 Loss_Total: 14.679033\n",
      "TRAIN: Batch: 0.4026268470017981 Loss_Seg: 11.915159\n",
      "accuracy = 0.937175\n",
      "mean IU  = 0.689242\n",
      "    class # 0 capture rate = 0.936158 \n",
      "    class # 1 capture rate = 0.955473 \n",
      "TRAIN: Batch: 0.4065358455163787 Loss_Total: 12.227331\n",
      "TRAIN: Batch: 0.4065358455163787 Loss_Seg: 13.305021\n",
      "accuracy = 0.931515\n",
      "mean IU  = 0.686858\n",
      "    class # 0 capture rate = 0.932189 \n",
      "    class # 1 capture rate = 0.920945 \n",
      "TRAIN: Batch: 0.41044484403095927 Loss_Total: 12.572439\n",
      "TRAIN: Batch: 0.41044484403095927 Loss_Seg: 13.014957\n",
      "accuracy = 0.938831\n",
      "mean IU  = 0.717717\n",
      "    class # 0 capture rate = 0.939798 \n",
      "    class # 1 capture rate = 0.925203 \n",
      "TRAIN: Batch: 0.4143538425455398 Loss_Total: 16.457714\n",
      "TRAIN: Batch: 0.4143538425455398 Loss_Seg: 11.860002\n",
      "accuracy = 0.929391\n",
      "mean IU  = 0.676116\n",
      "    class # 0 capture rate = 0.928008 \n",
      "    class # 1 capture rate = 0.953077 \n",
      "TRAIN: Batch: 0.4182628410601204 Loss_Total: 9.9854145\n",
      "TRAIN: Batch: 0.4182628410601204 Loss_Seg: 10.33545\n",
      "accuracy = 0.946987\n",
      "mean IU  = 0.722066\n",
      "    class # 0 capture rate = 0.946678 \n",
      "    class # 1 capture rate = 0.952214 \n",
      "TRAIN: Batch: 0.42217183957470095 Loss_Total: 11.905102\n",
      "TRAIN: Batch: 0.42217183957470095 Loss_Seg: 12.36867\n",
      "accuracy = 0.940517\n",
      "mean IU  = 0.719283\n",
      "    class # 0 capture rate = 0.940242 \n",
      "    class # 1 capture rate = 0.944573 \n",
      "TRAIN: Batch: 0.42608083808928154 Loss_Total: 13.959835\n",
      "TRAIN: Batch: 0.42608083808928154 Loss_Seg: 13.395734\n",
      "accuracy = 0.935341\n",
      "mean IU  = 0.696881\n",
      "    class # 0 capture rate = 0.936572 \n",
      "    class # 1 capture rate = 0.916264 \n",
      "TRAIN: Batch: 0.4299898366038621 Loss_Total: 11.169878\n",
      "TRAIN: Batch: 0.4299898366038621 Loss_Seg: 12.409081\n",
      "accuracy = 0.939937\n",
      "mean IU  = 0.705814\n",
      "    class # 0 capture rate = 0.941931 \n",
      "    class # 1 capture rate = 0.908609 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.4338988351184426 Loss_Total: 12.755939\n",
      "TRAIN: Batch: 0.4338988351184426 Loss_Seg: 9.208458\n",
      "accuracy = 0.944530\n",
      "mean IU  = 0.697197\n",
      "    class # 0 capture rate = 0.944159 \n",
      "    class # 1 capture rate = 0.951850 \n",
      "TRAIN: Batch: 0.4378078336330232 Loss_Total: 8.581817\n",
      "TRAIN: Batch: 0.4378078336330232 Loss_Seg: 11.114575\n",
      "accuracy = 0.949683\n",
      "mean IU  = 0.743543\n",
      "    class # 0 capture rate = 0.949893 \n",
      "    class # 1 capture rate = 0.946526 \n",
      "TRAIN: Batch: 0.44171683214760377 Loss_Total: 14.113697\n",
      "TRAIN: Batch: 0.44171683214760377 Loss_Seg: 14.384409\n",
      "accuracy = 0.934632\n",
      "mean IU  = 0.695688\n",
      "    class # 0 capture rate = 0.935242 \n",
      "    class # 1 capture rate = 0.925132 \n",
      "TRAIN: Batch: 0.44562583066218436 Loss_Total: 13.892193\n",
      "TRAIN: Batch: 0.44562583066218436 Loss_Seg: 18.299128\n",
      "accuracy = 0.949676\n",
      "mean IU  = 0.738689\n",
      "    class # 0 capture rate = 0.957623 \n",
      "    class # 1 capture rate = 0.840303 \n",
      "TRAIN: Batch: 0.4495348291767649 Loss_Total: 14.425501\n",
      "TRAIN: Batch: 0.4495348291767649 Loss_Seg: 12.104461\n",
      "accuracy = 0.955077\n",
      "mean IU  = 0.769497\n",
      "    class # 0 capture rate = 0.958110 \n",
      "    class # 1 capture rate = 0.914655 \n",
      "TRAIN: Batch: 0.4534438276913455 Loss_Total: 13.317753\n",
      "TRAIN: Batch: 0.4534438276913455 Loss_Seg: 14.397361\n",
      "accuracy = 0.920457\n",
      "mean IU  = 0.681064\n",
      "    class # 0 capture rate = 0.920585 \n",
      "    class # 1 capture rate = 0.918759 \n",
      "TRAIN: Batch: 0.45735282620592604 Loss_Total: 12.837008\n",
      "TRAIN: Batch: 0.45735282620592604 Loss_Seg: 10.229025\n",
      "accuracy = 0.954341\n",
      "mean IU  = 0.710062\n",
      "    class # 0 capture rate = 0.957230 \n",
      "    class # 1 capture rate = 0.892935 \n",
      "TRAIN: Batch: 0.4612618247205066 Loss_Total: 10.801678\n",
      "TRAIN: Batch: 0.4612618247205066 Loss_Seg: 12.135422\n",
      "accuracy = 0.946871\n",
      "mean IU  = 0.730324\n",
      "    class # 0 capture rate = 0.948332 \n",
      "    class # 1 capture rate = 0.924573 \n",
      "TRAIN: Batch: 0.4651708232350872 Loss_Total: 16.922184\n",
      "TRAIN: Batch: 0.4651708232350872 Loss_Seg: 18.201626\n",
      "accuracy = 0.924313\n",
      "mean IU  = 0.711042\n",
      "    class # 0 capture rate = 0.926830 \n",
      "    class # 1 capture rate = 0.897465 \n",
      "TRAIN: Batch: 0.4690798217496677 Loss_Total: 12.169823\n",
      "TRAIN: Batch: 0.4690798217496677 Loss_Seg: 13.89732\n",
      "accuracy = 0.928069\n",
      "mean IU  = 0.689459\n",
      "    class # 0 capture rate = 0.928142 \n",
      "    class # 1 capture rate = 0.927013 \n",
      "TRAIN: Batch: 0.4729888202642483 Loss_Total: 13.641189\n",
      "TRAIN: Batch: 0.4729888202642483 Loss_Seg: 9.799225\n",
      "accuracy = 0.940738\n",
      "mean IU  = 0.700183\n",
      "    class # 0 capture rate = 0.939266 \n",
      "    class # 1 capture rate = 0.967160 \n",
      "TRAIN: Batch: 0.47689781877882886 Loss_Total: 11.841552\n",
      "TRAIN: Batch: 0.47689781877882886 Loss_Seg: 10.324585\n",
      "accuracy = 0.942958\n",
      "mean IU  = 0.716223\n",
      "    class # 0 capture rate = 0.941850 \n",
      "    class # 1 capture rate = 0.961050 \n",
      "TRAIN: Batch: 0.4808068172934094 Loss_Total: 14.459801\n",
      "TRAIN: Batch: 0.4808068172934094 Loss_Seg: 13.797837\n",
      "accuracy = 0.947066\n",
      "mean IU  = 0.721769\n",
      "    class # 0 capture rate = 0.949918 \n",
      "    class # 1 capture rate = 0.901250 \n",
      "TRAIN: Batch: 0.48471581580799 Loss_Total: 12.884867\n",
      "TRAIN: Batch: 0.48471581580799 Loss_Seg: 11.81079\n",
      "accuracy = 0.943235\n",
      "mean IU  = 0.735198\n",
      "    class # 0 capture rate = 0.943026 \n",
      "    class # 1 capture rate = 0.946101 \n",
      "TRAIN: Batch: 0.48862481432257054 Loss_Total: 11.260712\n",
      "TRAIN: Batch: 0.48862481432257054 Loss_Seg: 9.739828\n",
      "accuracy = 0.950577\n",
      "mean IU  = 0.727437\n",
      "    class # 0 capture rate = 0.949985 \n",
      "    class # 1 capture rate = 0.961201 \n",
      "TRAIN: Batch: 0.49253381283715114 Loss_Total: 13.4127865\n",
      "TRAIN: Batch: 0.49253381283715114 Loss_Seg: 11.322225\n",
      "accuracy = 0.949927\n",
      "mean IU  = 0.739374\n",
      "    class # 0 capture rate = 0.952206 \n",
      "    class # 1 capture rate = 0.915521 \n",
      "TRAIN: Batch: 0.4964428113517317 Loss_Total: 15.475048\n",
      "TRAIN: Batch: 0.4964428113517317 Loss_Seg: 15.631351\n",
      "accuracy = 0.931144\n",
      "mean IU  = 0.709908\n",
      "    class # 0 capture rate = 0.932069 \n",
      "    class # 1 capture rate = 0.919398 \n",
      "TRAIN: Batch: 0.5003518098663122 Loss_Total: 24.864956\n",
      "TRAIN: Batch: 0.5003518098663122 Loss_Seg: 33.175728\n",
      "accuracy = 0.899311\n",
      "mean IU  = 0.672219\n",
      "    class # 0 capture rate = 0.912549 \n",
      "    class # 1 capture rate = 0.788108 \n",
      "TRAIN: Batch: 0.5042608083808928 Loss_Total: 14.5437\n",
      "TRAIN: Batch: 0.5042608083808928 Loss_Seg: 12.341215\n",
      "accuracy = 0.941520\n",
      "mean IU  = 0.687804\n",
      "    class # 0 capture rate = 0.943252 \n",
      "    class # 1 capture rate = 0.908557 \n",
      "TRAIN: Batch: 0.5081698068954734 Loss_Total: 14.270704\n",
      "TRAIN: Batch: 0.5081698068954734 Loss_Seg: 10.402609\n",
      "accuracy = 0.951969\n",
      "mean IU  = 0.735191\n",
      "    class # 0 capture rate = 0.952191 \n",
      "    class # 1 capture rate = 0.948161 \n",
      "TRAIN: Batch: 0.5120788054100539 Loss_Total: 15.745489\n",
      "TRAIN: Batch: 0.5120788054100539 Loss_Seg: 12.87095\n",
      "accuracy = 0.942475\n",
      "mean IU  = 0.702897\n",
      "    class # 0 capture rate = 0.943627 \n",
      "    class # 1 capture rate = 0.922491 \n",
      "TRAIN: Batch: 0.5159878039246345 Loss_Total: 11.540047\n",
      "TRAIN: Batch: 0.5159878039246345 Loss_Seg: 13.379556\n",
      "accuracy = 0.953416\n",
      "mean IU  = 0.753199\n",
      "    class # 0 capture rate = 0.957382 \n",
      "    class # 1 capture rate = 0.896396 \n",
      "TRAIN: Batch: 0.5198968024392151 Loss_Total: 16.231522\n",
      "TRAIN: Batch: 0.5198968024392151 Loss_Seg: 9.943118\n",
      "accuracy = 0.954098\n",
      "mean IU  = 0.777916\n",
      "    class # 0 capture rate = 0.953479 \n",
      "    class # 1 capture rate = 0.961949 \n",
      "TRAIN: Batch: 0.5238058009537956 Loss_Total: 9.510815\n",
      "TRAIN: Batch: 0.5238058009537956 Loss_Seg: 8.648032\n",
      "accuracy = 0.952902\n",
      "mean IU  = 0.733174\n",
      "    class # 0 capture rate = 0.952347 \n",
      "    class # 1 capture rate = 0.962998 \n",
      "TRAIN: Batch: 0.5277147994683762 Loss_Total: 14.690704\n",
      "TRAIN: Batch: 0.5277147994683762 Loss_Seg: 17.738544\n",
      "accuracy = 0.949582\n",
      "mean IU  = 0.709803\n",
      "    class # 0 capture rate = 0.958200 \n",
      "    class # 1 capture rate = 0.804613 \n",
      "TRAIN: Batch: 0.5316237979829568 Loss_Total: 10.672964\n",
      "TRAIN: Batch: 0.5316237979829568 Loss_Seg: 9.560925\n",
      "accuracy = 0.955343\n",
      "mean IU  = 0.733605\n",
      "    class # 0 capture rate = 0.955750 \n",
      "    class # 1 capture rate = 0.947600 \n",
      "TRAIN: Batch: 0.5355327964975374 Loss_Total: 17.072668\n",
      "TRAIN: Batch: 0.5355327964975374 Loss_Seg: 18.51734\n",
      "accuracy = 0.917177\n",
      "mean IU  = 0.710518\n",
      "    class # 0 capture rate = 0.916337 \n",
      "    class # 1 capture rate = 0.925299 \n",
      "TRAIN: Batch: 0.5394417950121179 Loss_Total: 12.585939\n",
      "TRAIN: Batch: 0.5394417950121179 Loss_Seg: 11.949284\n",
      "accuracy = 0.940955\n",
      "mean IU  = 0.701169\n",
      "    class # 0 capture rate = 0.942473 \n",
      "    class # 1 capture rate = 0.915358 \n",
      "TRAIN: Batch: 0.5433507935266985 Loss_Total: 12.096472\n",
      "TRAIN: Batch: 0.5433507935266985 Loss_Seg: 11.010353\n",
      "accuracy = 0.943917\n",
      "mean IU  = 0.697846\n",
      "    class # 0 capture rate = 0.944215 \n",
      "    class # 1 capture rate = 0.938235 \n",
      "TRAIN: Batch: 0.5472597920412791 Loss_Total: 13.016415\n",
      "TRAIN: Batch: 0.5472597920412791 Loss_Seg: 11.663358\n",
      "accuracy = 0.940276\n",
      "mean IU  = 0.745011\n",
      "    class # 0 capture rate = 0.938946 \n",
      "    class # 1 capture rate = 0.956044 \n",
      "TRAIN: Batch: 0.5511687905558595 Loss_Total: 8.943476\n",
      "TRAIN: Batch: 0.5511687905558595 Loss_Seg: 9.566131\n",
      "accuracy = 0.957687\n",
      "mean IU  = 0.752452\n",
      "    class # 0 capture rate = 0.959758 \n",
      "    class # 1 capture rate = 0.922741 \n",
      "TRAIN: Batch: 0.5550777890704401 Loss_Total: 10.644749\n",
      "TRAIN: Batch: 0.5550777890704401 Loss_Seg: 12.336838\n",
      "accuracy = 0.939278\n",
      "mean IU  = 0.681026\n",
      "    class # 0 capture rate = 0.942707 \n",
      "    class # 1 capture rate = 0.875951 \n",
      "TRAIN: Batch: 0.5589867875850207 Loss_Total: 10.937361\n",
      "TRAIN: Batch: 0.5589867875850207 Loss_Seg: 11.907263\n",
      "accuracy = 0.935830\n",
      "mean IU  = 0.704684\n",
      "    class # 0 capture rate = 0.935034 \n",
      "    class # 1 capture rate = 0.947908 \n",
      "TRAIN: Batch: 0.5628957860996013 Loss_Total: 13.759079\n",
      "TRAIN: Batch: 0.5628957860996013 Loss_Seg: 18.43071\n",
      "accuracy = 0.926852\n",
      "mean IU  = 0.681861\n",
      "    class # 0 capture rate = 0.933297 \n",
      "    class # 1 capture rate = 0.839659 \n",
      "TRAIN: Batch: 0.5668047846141818 Loss_Total: 10.541897\n",
      "TRAIN: Batch: 0.5668047846141818 Loss_Seg: 8.952852\n",
      "accuracy = 0.954822\n",
      "mean IU  = 0.731135\n",
      "    class # 0 capture rate = 0.954700 \n",
      "    class # 1 capture rate = 0.957193 \n",
      "TRAIN: Batch: 0.5707137831287624 Loss_Total: 13.826235\n",
      "TRAIN: Batch: 0.5707137831287624 Loss_Seg: 11.935426\n",
      "accuracy = 0.949039\n",
      "mean IU  = 0.706520\n",
      "    class # 0 capture rate = 0.952432 \n",
      "    class # 1 capture rate = 0.885007 \n",
      "TRAIN: Batch: 0.574622781643343 Loss_Total: 12.353267\n",
      "TRAIN: Batch: 0.574622781643343 Loss_Seg: 11.525543\n",
      "accuracy = 0.939447\n",
      "mean IU  = 0.692537\n",
      "    class # 0 capture rate = 0.939856 \n",
      "    class # 1 capture rate = 0.932133 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.5785317801579235 Loss_Total: 11.119186\n",
      "TRAIN: Batch: 0.5785317801579235 Loss_Seg: 9.585261\n",
      "accuracy = 0.944842\n",
      "mean IU  = 0.700848\n",
      "    class # 0 capture rate = 0.943893 \n",
      "    class # 1 capture rate = 0.963390 \n",
      "TRAIN: Batch: 0.5824407786725041 Loss_Total: 10.931178\n",
      "TRAIN: Batch: 0.5824407786725041 Loss_Seg: 10.924797\n",
      "accuracy = 0.945281\n",
      "mean IU  = 0.720671\n",
      "    class # 0 capture rate = 0.945519 \n",
      "    class # 1 capture rate = 0.941424 \n",
      "TRAIN: Batch: 0.5863497771870847 Loss_Total: 10.619135\n",
      "TRAIN: Batch: 0.5863497771870847 Loss_Seg: 9.252837\n",
      "accuracy = 0.957785\n",
      "mean IU  = 0.741646\n",
      "    class # 0 capture rate = 0.958299 \n",
      "    class # 1 capture rate = 0.947963 \n",
      "TRAIN: Batch: 0.5902587757016652 Loss_Total: 9.350042\n",
      "TRAIN: Batch: 0.5902587757016652 Loss_Seg: 9.694891\n",
      "accuracy = 0.961081\n",
      "mean IU  = 0.746389\n",
      "    class # 0 capture rate = 0.963503 \n",
      "    class # 1 capture rate = 0.913784 \n",
      "TRAIN: Batch: 0.5941677742162458 Loss_Total: 14.107313\n",
      "TRAIN: Batch: 0.5941677742162458 Loss_Seg: 12.020708\n",
      "accuracy = 0.946230\n",
      "mean IU  = 0.733471\n",
      "    class # 0 capture rate = 0.946777 \n",
      "    class # 1 capture rate = 0.938104 \n",
      "TRAIN: Batch: 0.5980767727308264 Loss_Total: 14.892303\n",
      "TRAIN: Batch: 0.5980767727308264 Loss_Seg: 11.758718\n",
      "accuracy = 0.950411\n",
      "mean IU  = 0.750442\n",
      "    class # 0 capture rate = 0.950548 \n",
      "    class # 1 capture rate = 0.948438 \n",
      "TRAIN: Batch: 0.601985771245407 Loss_Total: 17.136833\n",
      "TRAIN: Batch: 0.601985771245407 Loss_Seg: 17.292503\n",
      "accuracy = 0.925083\n",
      "mean IU  = 0.723502\n",
      "    class # 0 capture rate = 0.924690 \n",
      "    class # 1 capture rate = 0.929034 \n",
      "TRAIN: Batch: 0.6058947697599875 Loss_Total: 10.953746\n",
      "TRAIN: Batch: 0.6058947697599875 Loss_Seg: 10.9644375\n",
      "accuracy = 0.944599\n",
      "mean IU  = 0.736566\n",
      "    class # 0 capture rate = 0.944564 \n",
      "    class # 1 capture rate = 0.945095 \n",
      "TRAIN: Batch: 0.6098037682745681 Loss_Total: 12.826627\n",
      "TRAIN: Batch: 0.6098037682745681 Loss_Seg: 11.593599\n",
      "accuracy = 0.939595\n",
      "mean IU  = 0.698015\n",
      "    class # 0 capture rate = 0.939005 \n",
      "    class # 1 capture rate = 0.949910 \n",
      "TRAIN: Batch: 0.6137127667891487 Loss_Total: 14.125088\n",
      "TRAIN: Batch: 0.6137127667891487 Loss_Seg: 13.144583\n",
      "accuracy = 0.935759\n",
      "mean IU  = 0.726873\n",
      "    class # 0 capture rate = 0.935810 \n",
      "    class # 1 capture rate = 0.935128 \n",
      "TRAIN: Batch: 0.6176217653037291 Loss_Total: 12.527454\n",
      "TRAIN: Batch: 0.6176217653037291 Loss_Seg: 12.360057\n",
      "accuracy = 0.947233\n",
      "mean IU  = 0.730903\n",
      "    class # 0 capture rate = 0.948928 \n",
      "    class # 1 capture rate = 0.921366 \n",
      "TRAIN: Batch: 0.6215307638183097 Loss_Total: 13.83666\n",
      "TRAIN: Batch: 0.6215307638183097 Loss_Seg: 16.425257\n",
      "accuracy = 0.921732\n",
      "mean IU  = 0.694426\n",
      "    class # 0 capture rate = 0.923947 \n",
      "    class # 1 capture rate = 0.895704 \n",
      "TRAIN: Batch: 0.6254397623328903 Loss_Total: 11.37775\n",
      "TRAIN: Batch: 0.6254397623328903 Loss_Seg: 10.082426\n",
      "accuracy = 0.951958\n",
      "mean IU  = 0.731733\n",
      "    class # 0 capture rate = 0.952252 \n",
      "    class # 1 capture rate = 0.946770 \n",
      "TRAIN: Batch: 0.6293487608474708 Loss_Total: 12.664211\n",
      "TRAIN: Batch: 0.6293487608474708 Loss_Seg: 11.931642\n",
      "accuracy = 0.946116\n",
      "mean IU  = 0.722303\n",
      "    class # 0 capture rate = 0.948240 \n",
      "    class # 1 capture rate = 0.912490 \n",
      "TRAIN: Batch: 0.6332577593620514 Loss_Total: 9.803886\n",
      "TRAIN: Batch: 0.6332577593620514 Loss_Seg: 10.067029\n",
      "accuracy = 0.957079\n",
      "mean IU  = 0.736022\n",
      "    class # 0 capture rate = 0.958889 \n",
      "    class # 1 capture rate = 0.922549 \n",
      "TRAIN: Batch: 0.637166757876632 Loss_Total: 17.913347\n",
      "TRAIN: Batch: 0.637166757876632 Loss_Seg: 25.821571\n",
      "accuracy = 0.907119\n",
      "mean IU  = 0.707729\n",
      "    class # 0 capture rate = 0.909601 \n",
      "    class # 1 capture rate = 0.887589 \n",
      "TRAIN: Batch: 0.6410757563912126 Loss_Total: 10.288013\n",
      "TRAIN: Batch: 0.6410757563912126 Loss_Seg: 12.632171\n",
      "accuracy = 0.951947\n",
      "mean IU  = 0.733899\n",
      "    class # 0 capture rate = 0.954412 \n",
      "    class # 1 capture rate = 0.911006 \n",
      "TRAIN: Batch: 0.6449847549057931 Loss_Total: 10.8030815\n",
      "TRAIN: Batch: 0.6449847549057931 Loss_Seg: 10.891035\n",
      "accuracy = 0.952929\n",
      "mean IU  = 0.741209\n",
      "    class # 0 capture rate = 0.955144 \n",
      "    class # 1 capture rate = 0.917189 \n",
      "TRAIN: Batch: 0.6488937534203737 Loss_Total: 13.431438\n",
      "TRAIN: Batch: 0.6488937534203737 Loss_Seg: 15.116794\n",
      "accuracy = 0.940129\n",
      "mean IU  = 0.737026\n",
      "    class # 0 capture rate = 0.942465 \n",
      "    class # 1 capture rate = 0.911962 \n",
      "TRAIN: Batch: 0.6528027519349543 Loss_Total: 13.175999\n",
      "TRAIN: Batch: 0.6528027519349543 Loss_Seg: 11.350618\n",
      "accuracy = 0.946178\n",
      "mean IU  = 0.740120\n",
      "    class # 0 capture rate = 0.944973 \n",
      "    class # 1 capture rate = 0.963534 \n",
      "TRAIN: Batch: 0.6567117504495348 Loss_Total: 11.647058\n",
      "TRAIN: Batch: 0.6567117504495348 Loss_Seg: 9.918618\n",
      "accuracy = 0.950217\n",
      "mean IU  = 0.677690\n",
      "    class # 0 capture rate = 0.951343 \n",
      "    class # 1 capture rate = 0.921001 \n",
      "TRAIN: Batch: 0.6606207489641154 Loss_Total: 11.543528\n",
      "TRAIN: Batch: 0.6606207489641154 Loss_Seg: 12.731033\n",
      "accuracy = 0.933932\n",
      "mean IU  = 0.694725\n",
      "    class # 0 capture rate = 0.933164 \n",
      "    class # 1 capture rate = 0.946113 \n",
      "TRAIN: Batch: 0.664529747478696 Loss_Total: 12.498787\n",
      "TRAIN: Batch: 0.664529747478696 Loss_Seg: 12.793633\n",
      "accuracy = 0.942917\n",
      "mean IU  = 0.693717\n",
      "    class # 0 capture rate = 0.944772 \n",
      "    class # 1 capture rate = 0.908305 \n",
      "TRAIN: Batch: 0.6684387459932766 Loss_Total: 16.09446\n",
      "TRAIN: Batch: 0.6684387459932766 Loss_Seg: 16.71047\n",
      "accuracy = 0.925932\n",
      "mean IU  = 0.721357\n",
      "    class # 0 capture rate = 0.926940 \n",
      "    class # 1 capture rate = 0.915590 \n",
      "TRAIN: Batch: 0.6723477445078571 Loss_Total: 14.651032\n",
      "TRAIN: Batch: 0.6723477445078571 Loss_Seg: 15.458483\n",
      "accuracy = 0.921005\n",
      "mean IU  = 0.642315\n",
      "    class # 0 capture rate = 0.921688 \n",
      "    class # 1 capture rate = 0.908168 \n",
      "TRAIN: Batch: 0.6762567430224377 Loss_Total: 13.811757\n",
      "TRAIN: Batch: 0.6762567430224377 Loss_Seg: 15.07088\n",
      "accuracy = 0.932337\n",
      "mean IU  = 0.740238\n",
      "    class # 0 capture rate = 0.930959 \n",
      "    class # 1 capture rate = 0.946458 \n",
      "TRAIN: Batch: 0.6801657415370183 Loss_Total: 10.271848\n",
      "TRAIN: Batch: 0.6801657415370183 Loss_Seg: 9.434292\n",
      "accuracy = 0.956763\n",
      "mean IU  = 0.730241\n",
      "    class # 0 capture rate = 0.958532 \n",
      "    class # 1 capture rate = 0.921668 \n",
      "TRAIN: Batch: 0.6840747400515987 Loss_Total: 10.548637\n",
      "TRAIN: Batch: 0.6840747400515987 Loss_Seg: 10.660091\n",
      "accuracy = 0.948921\n",
      "mean IU  = 0.736270\n",
      "    class # 0 capture rate = 0.949579 \n",
      "    class # 1 capture rate = 0.938710 \n",
      "TRAIN: Batch: 0.6879837385661793 Loss_Total: 11.139591\n",
      "TRAIN: Batch: 0.6879837385661793 Loss_Seg: 10.619961\n",
      "accuracy = 0.944864\n",
      "mean IU  = 0.713163\n",
      "    class # 0 capture rate = 0.944599 \n",
      "    class # 1 capture rate = 0.949441 \n",
      "TRAIN: Batch: 0.6918927370807599 Loss_Total: 13.292971\n",
      "TRAIN: Batch: 0.6918927370807599 Loss_Seg: 12.686583\n",
      "accuracy = 0.936309\n",
      "mean IU  = 0.706248\n",
      "    class # 0 capture rate = 0.936337 \n",
      "    class # 1 capture rate = 0.935897 \n",
      "TRAIN: Batch: 0.6958017355953404 Loss_Total: 15.113707\n",
      "TRAIN: Batch: 0.6958017355953404 Loss_Seg: 10.654923\n",
      "accuracy = 0.946475\n",
      "mean IU  = 0.726363\n",
      "    class # 0 capture rate = 0.946605 \n",
      "    class # 1 capture rate = 0.944398 \n",
      "TRAIN: Batch: 0.699710734109921 Loss_Total: 11.493469\n",
      "TRAIN: Batch: 0.699710734109921 Loss_Seg: 9.922221\n",
      "accuracy = 0.946160\n",
      "mean IU  = 0.697983\n",
      "    class # 0 capture rate = 0.946461 \n",
      "    class # 1 capture rate = 0.940114 \n",
      "TRAIN: Batch: 0.7036197326245016 Loss_Total: 10.266691\n",
      "TRAIN: Batch: 0.7036197326245016 Loss_Seg: 9.226223\n",
      "accuracy = 0.957925\n",
      "mean IU  = 0.744808\n",
      "    class # 0 capture rate = 0.958057 \n",
      "    class # 1 capture rate = 0.955448 \n",
      "TRAIN: Batch: 0.7075287311390822 Loss_Total: 12.061926\n",
      "TRAIN: Batch: 0.7075287311390822 Loss_Seg: 9.714757\n",
      "accuracy = 0.949088\n",
      "mean IU  = 0.717167\n",
      "    class # 0 capture rate = 0.949564 \n",
      "    class # 1 capture rate = 0.940341 \n",
      "TRAIN: Batch: 0.7114377296536627 Loss_Total: 13.6038\n",
      "TRAIN: Batch: 0.7114377296536627 Loss_Seg: 10.750209\n",
      "accuracy = 0.952587\n",
      "mean IU  = 0.758367\n",
      "    class # 0 capture rate = 0.952469 \n",
      "    class # 1 capture rate = 0.954273 \n",
      "TRAIN: Batch: 0.7153467281682433 Loss_Total: 12.3346195\n",
      "TRAIN: Batch: 0.7153467281682433 Loss_Seg: 10.951897\n",
      "accuracy = 0.948143\n",
      "mean IU  = 0.713169\n",
      "    class # 0 capture rate = 0.950402 \n",
      "    class # 1 capture rate = 0.907707 \n",
      "TRAIN: Batch: 0.7192557266828239 Loss_Total: 14.80515\n",
      "TRAIN: Batch: 0.7192557266828239 Loss_Seg: 13.238144\n",
      "accuracy = 0.928341\n",
      "mean IU  = 0.676462\n",
      "    class # 0 capture rate = 0.927933 \n",
      "    class # 1 capture rate = 0.935032 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.7231647251974044 Loss_Total: 10.605475\n",
      "TRAIN: Batch: 0.7231647251974044 Loss_Seg: 9.972358\n",
      "accuracy = 0.953951\n",
      "mean IU  = 0.759894\n",
      "    class # 0 capture rate = 0.954023 \n",
      "    class # 1 capture rate = 0.952892 \n",
      "TRAIN: Batch: 0.727073723711985 Loss_Total: 15.870716\n",
      "TRAIN: Batch: 0.727073723711985 Loss_Seg: 14.149777\n",
      "accuracy = 0.937738\n",
      "mean IU  = 0.702897\n",
      "    class # 0 capture rate = 0.940370 \n",
      "    class # 1 capture rate = 0.897871 \n",
      "TRAIN: Batch: 0.7309827222265656 Loss_Total: 11.334179\n",
      "TRAIN: Batch: 0.7309827222265656 Loss_Seg: 10.445247\n",
      "accuracy = 0.946258\n",
      "mean IU  = 0.710709\n",
      "    class # 0 capture rate = 0.947516 \n",
      "    class # 1 capture rate = 0.923888 \n",
      "TRAIN: Batch: 0.7348917207411462 Loss_Total: 10.320846\n",
      "TRAIN: Batch: 0.7348917207411462 Loss_Seg: 9.1461935\n",
      "accuracy = 0.951361\n",
      "mean IU  = 0.748708\n",
      "    class # 0 capture rate = 0.951212 \n",
      "    class # 1 capture rate = 0.953616 \n",
      "TRAIN: Batch: 0.7388007192557267 Loss_Total: 12.343693\n",
      "TRAIN: Batch: 0.7388007192557267 Loss_Seg: 13.858395\n",
      "accuracy = 0.934618\n",
      "mean IU  = 0.709449\n",
      "    class # 0 capture rate = 0.935719 \n",
      "    class # 1 capture rate = 0.919511 \n",
      "TRAIN: Batch: 0.7427097177703073 Loss_Total: 10.337962\n",
      "TRAIN: Batch: 0.7427097177703073 Loss_Seg: 10.240255\n",
      "accuracy = 0.946352\n",
      "mean IU  = 0.731981\n",
      "    class # 0 capture rate = 0.945136 \n",
      "    class # 1 capture rate = 0.965253 \n",
      "TRAIN: Batch: 0.7466187162848879 Loss_Total: 11.361453\n",
      "TRAIN: Batch: 0.7466187162848879 Loss_Seg: 10.050798\n",
      "accuracy = 0.952534\n",
      "mean IU  = 0.756148\n",
      "    class # 0 capture rate = 0.952442 \n",
      "    class # 1 capture rate = 0.953887 \n",
      "TRAIN: Batch: 0.7505277147994683 Loss_Total: 9.688945\n",
      "TRAIN: Batch: 0.7505277147994683 Loss_Seg: 6.552239\n",
      "accuracy = 0.965325\n",
      "mean IU  = 0.796964\n",
      "    class # 0 capture rate = 0.964293 \n",
      "    class # 1 capture rate = 0.981392 \n",
      "TRAIN: Batch: 0.7544367133140489 Loss_Total: 12.382311\n",
      "TRAIN: Batch: 0.7544367133140489 Loss_Seg: 10.292949\n",
      "accuracy = 0.938354\n",
      "mean IU  = 0.667692\n",
      "    class # 0 capture rate = 0.938440 \n",
      "    class # 1 capture rate = 0.936490 \n",
      "TRAIN: Batch: 0.7583457118286295 Loss_Total: 13.473333\n",
      "TRAIN: Batch: 0.7583457118286295 Loss_Seg: 13.530356\n",
      "accuracy = 0.936862\n",
      "mean IU  = 0.685772\n",
      "    class # 0 capture rate = 0.937689 \n",
      "    class # 1 capture rate = 0.922197 \n",
      "TRAIN: Batch: 0.76225471034321 Loss_Total: 9.885561\n",
      "TRAIN: Batch: 0.76225471034321 Loss_Seg: 11.207652\n",
      "accuracy = 0.945725\n",
      "mean IU  = 0.724859\n",
      "    class # 0 capture rate = 0.946282 \n",
      "    class # 1 capture rate = 0.936937 \n",
      "TRAIN: Batch: 0.7661637088577906 Loss_Total: 10.018224\n",
      "TRAIN: Batch: 0.7661637088577906 Loss_Seg: 10.371233\n",
      "accuracy = 0.958056\n",
      "mean IU  = 0.765973\n",
      "    class # 0 capture rate = 0.960891 \n",
      "    class # 1 capture rate = 0.915427 \n",
      "TRAIN: Batch: 0.7700727073723712 Loss_Total: 14.87433\n",
      "TRAIN: Batch: 0.7700727073723712 Loss_Seg: 16.163063\n",
      "accuracy = 0.923933\n",
      "mean IU  = 0.700969\n",
      "    class # 0 capture rate = 0.925884 \n",
      "    class # 1 capture rate = 0.901235 \n",
      "TRAIN: Batch: 0.7739817058869518 Loss_Total: 9.934982\n",
      "TRAIN: Batch: 0.7739817058869518 Loss_Seg: 9.705635\n",
      "accuracy = 0.956567\n",
      "mean IU  = 0.706022\n",
      "    class # 0 capture rate = 0.958221 \n",
      "    class # 1 capture rate = 0.916769 \n",
      "TRAIN: Batch: 0.7778907044015323 Loss_Total: 11.502128\n",
      "TRAIN: Batch: 0.7778907044015323 Loss_Seg: 10.420409\n",
      "accuracy = 0.942090\n",
      "mean IU  = 0.686512\n",
      "    class # 0 capture rate = 0.942533 \n",
      "    class # 1 capture rate = 0.933196 \n",
      "TRAIN: Batch: 0.7817997029161129 Loss_Total: 20.482965\n",
      "TRAIN: Batch: 0.7817997029161129 Loss_Seg: 24.305723\n",
      "accuracy = 0.909735\n",
      "mean IU  = 0.687885\n",
      "    class # 0 capture rate = 0.914727 \n",
      "    class # 1 capture rate = 0.861837 \n",
      "TRAIN: Batch: 0.7857087014306935 Loss_Total: 16.369493\n",
      "TRAIN: Batch: 0.7857087014306935 Loss_Seg: 18.530882\n",
      "accuracy = 0.908800\n",
      "mean IU  = 0.671576\n",
      "    class # 0 capture rate = 0.908700 \n",
      "    class # 1 capture rate = 0.909960 \n",
      "TRAIN: Batch: 0.789617699945274 Loss_Total: 14.534148\n",
      "TRAIN: Batch: 0.789617699945274 Loss_Seg: 14.56598\n",
      "accuracy = 0.934512\n",
      "mean IU  = 0.688368\n",
      "    class # 0 capture rate = 0.936566 \n",
      "    class # 1 capture rate = 0.901420 \n",
      "TRAIN: Batch: 0.7935266984598546 Loss_Total: 11.769428\n",
      "TRAIN: Batch: 0.7935266984598546 Loss_Seg: 10.572324\n",
      "accuracy = 0.946637\n",
      "mean IU  = 0.716328\n",
      "    class # 0 capture rate = 0.947351 \n",
      "    class # 1 capture rate = 0.934287 \n",
      "TRAIN: Batch: 0.7974356969744352 Loss_Total: 9.816406\n",
      "TRAIN: Batch: 0.7974356969744352 Loss_Seg: 9.668611\n",
      "accuracy = 0.948851\n",
      "mean IU  = 0.738097\n",
      "    class # 0 capture rate = 0.948146 \n",
      "    class # 1 capture rate = 0.959854 \n",
      "TRAIN: Batch: 0.8013446954890158 Loss_Total: 11.211092\n",
      "TRAIN: Batch: 0.8013446954890158 Loss_Seg: 10.412213\n",
      "accuracy = 0.944709\n",
      "mean IU  = 0.719147\n",
      "    class # 0 capture rate = 0.944004 \n",
      "    class # 1 capture rate = 0.956354 \n",
      "TRAIN: Batch: 0.8052536940035963 Loss_Total: 13.679897\n",
      "TRAIN: Batch: 0.8052536940035963 Loss_Seg: 14.191119\n",
      "accuracy = 0.945159\n",
      "mean IU  = 0.703321\n",
      "    class # 0 capture rate = 0.950415 \n",
      "    class # 1 capture rate = 0.855761 \n",
      "TRAIN: Batch: 0.8091626925181769 Loss_Total: 13.552687\n",
      "TRAIN: Batch: 0.8091626925181769 Loss_Seg: 10.884521\n",
      "accuracy = 0.948106\n",
      "mean IU  = 0.725107\n",
      "    class # 0 capture rate = 0.948041 \n",
      "    class # 1 capture rate = 0.949212 \n",
      "TRAIN: Batch: 0.8130716910327574 Loss_Total: 11.933996\n",
      "TRAIN: Batch: 0.8130716910327574 Loss_Seg: 12.723851\n",
      "accuracy = 0.951242\n",
      "mean IU  = 0.728452\n",
      "    class # 0 capture rate = 0.956152 \n",
      "    class # 1 capture rate = 0.871254 \n",
      "TRAIN: Batch: 0.8169806895473379 Loss_Total: 12.305872\n",
      "TRAIN: Batch: 0.8169806895473379 Loss_Seg: 12.079246\n",
      "accuracy = 0.945865\n",
      "mean IU  = 0.744173\n",
      "    class # 0 capture rate = 0.946467 \n",
      "    class # 1 capture rate = 0.937807 \n",
      "TRAIN: Batch: 0.8208896880619185 Loss_Total: 10.041811\n",
      "TRAIN: Batch: 0.8208896880619185 Loss_Seg: 10.69508\n",
      "accuracy = 0.948165\n",
      "mean IU  = 0.724856\n",
      "    class # 0 capture rate = 0.948505 \n",
      "    class # 1 capture rate = 0.942433 \n",
      "TRAIN: Batch: 0.8247986865764991 Loss_Total: 12.643032\n",
      "TRAIN: Batch: 0.8247986865764991 Loss_Seg: 14.176244\n",
      "accuracy = 0.942624\n",
      "mean IU  = 0.710712\n",
      "    class # 0 capture rate = 0.943696 \n",
      "    class # 1 capture rate = 0.925127 \n",
      "TRAIN: Batch: 0.8287076850910796 Loss_Total: 11.77877\n",
      "TRAIN: Batch: 0.8287076850910796 Loss_Seg: 11.769043\n",
      "accuracy = 0.949819\n",
      "mean IU  = 0.738280\n",
      "    class # 0 capture rate = 0.950379 \n",
      "    class # 1 capture rate = 0.941057 \n",
      "TRAIN: Batch: 0.8326166836056602 Loss_Total: 14.284863\n",
      "TRAIN: Batch: 0.8326166836056602 Loss_Seg: 17.200243\n",
      "accuracy = 0.910989\n",
      "mean IU  = 0.674428\n",
      "    class # 0 capture rate = 0.909081 \n",
      "    class # 1 capture rate = 0.934080 \n",
      "TRAIN: Batch: 0.8365256821202408 Loss_Total: 12.578421\n",
      "TRAIN: Batch: 0.8365256821202408 Loss_Seg: 8.096585\n",
      "accuracy = 0.958469\n",
      "mean IU  = 0.750015\n",
      "    class # 0 capture rate = 0.957832 \n",
      "    class # 1 capture rate = 0.970321 \n",
      "TRAIN: Batch: 0.8404346806348214 Loss_Total: 10.460171\n",
      "TRAIN: Batch: 0.8404346806348214 Loss_Seg: 9.881439\n",
      "accuracy = 0.940839\n",
      "mean IU  = 0.686866\n",
      "    class # 0 capture rate = 0.940148 \n",
      "    class # 1 capture rate = 0.954592 \n",
      "TRAIN: Batch: 0.8443436791494019 Loss_Total: 14.105257\n",
      "TRAIN: Batch: 0.8443436791494019 Loss_Seg: 10.050992\n",
      "accuracy = 0.951459\n",
      "mean IU  = 0.743539\n",
      "    class # 0 capture rate = 0.951178 \n",
      "    class # 1 capture rate = 0.955913 \n",
      "TRAIN: Batch: 0.8482526776639825 Loss_Total: 8.87287\n",
      "TRAIN: Batch: 0.8482526776639825 Loss_Seg: 8.806092\n",
      "accuracy = 0.956222\n",
      "mean IU  = 0.734810\n",
      "    class # 0 capture rate = 0.957664 \n",
      "    class # 1 capture rate = 0.928919 \n",
      "TRAIN: Batch: 0.8521616761785631 Loss_Total: 9.697231\n",
      "TRAIN: Batch: 0.8521616761785631 Loss_Seg: 9.490365\n",
      "accuracy = 0.955951\n",
      "mean IU  = 0.751673\n",
      "    class # 0 capture rate = 0.956382 \n",
      "    class # 1 capture rate = 0.948773 \n",
      "TRAIN: Batch: 0.8560706746931436 Loss_Total: 11.211842\n",
      "TRAIN: Batch: 0.8560706746931436 Loss_Seg: 10.594569\n",
      "accuracy = 0.944892\n",
      "mean IU  = 0.705445\n",
      "    class # 0 capture rate = 0.945813 \n",
      "    class # 1 capture rate = 0.928239 \n",
      "TRAIN: Batch: 0.8599796732077242 Loss_Total: 16.312948\n",
      "TRAIN: Batch: 0.8599796732077242 Loss_Seg: 22.434172\n",
      "accuracy = 0.919904\n",
      "mean IU  = 0.692028\n",
      "    class # 0 capture rate = 0.925894 \n",
      "    class # 1 capture rate = 0.853959 \n",
      "TRAIN: Batch: 0.8638886717223048 Loss_Total: 12.653962\n",
      "TRAIN: Batch: 0.8638886717223048 Loss_Seg: 10.810833\n",
      "accuracy = 0.950284\n",
      "mean IU  = 0.753310\n",
      "    class # 0 capture rate = 0.949927 \n",
      "    class # 1 capture rate = 0.955334 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.8677976702368853 Loss_Total: 13.642321\n",
      "TRAIN: Batch: 0.8677976702368853 Loss_Seg: 10.900052\n",
      "accuracy = 0.944227\n",
      "mean IU  = 0.737523\n",
      "    class # 0 capture rate = 0.943956 \n",
      "    class # 1 capture rate = 0.947977 \n",
      "TRAIN: Batch: 0.8717066687514659 Loss_Total: 10.99448\n",
      "TRAIN: Batch: 0.8717066687514659 Loss_Seg: 9.139123\n",
      "accuracy = 0.952851\n",
      "mean IU  = 0.723321\n",
      "    class # 0 capture rate = 0.953167 \n",
      "    class # 1 capture rate = 0.946713 \n",
      "TRAIN: Batch: 0.8756156672660464 Loss_Total: 13.558284\n",
      "TRAIN: Batch: 0.8756156672660464 Loss_Seg: 9.402849\n",
      "accuracy = 0.947969\n",
      "mean IU  = 0.730136\n",
      "    class # 0 capture rate = 0.946805 \n",
      "    class # 1 capture rate = 0.967136 \n",
      "TRAIN: Batch: 0.879524665780627 Loss_Total: 11.202413\n",
      "TRAIN: Batch: 0.879524665780627 Loss_Seg: 13.267897\n",
      "accuracy = 0.932645\n",
      "mean IU  = 0.703664\n",
      "    class # 0 capture rate = 0.932688 \n",
      "    class # 1 capture rate = 0.932035 \n",
      "TRAIN: Batch: 0.8834336642952075 Loss_Total: 13.245\n",
      "TRAIN: Batch: 0.8834336642952075 Loss_Seg: 18.847576\n",
      "accuracy = 0.932229\n",
      "mean IU  = 0.685241\n",
      "    class # 0 capture rate = 0.938875 \n",
      "    class # 1 capture rate = 0.835486 \n",
      "TRAIN: Batch: 0.8873426628097881 Loss_Total: 11.771017\n",
      "TRAIN: Batch: 0.8873426628097881 Loss_Seg: 12.742167\n",
      "accuracy = 0.930753\n",
      "mean IU  = 0.676285\n",
      "    class # 0 capture rate = 0.930217 \n",
      "    class # 1 capture rate = 0.940045 \n",
      "TRAIN: Batch: 0.8912516613243687 Loss_Total: 15.743223\n",
      "TRAIN: Batch: 0.8912516613243687 Loss_Seg: 16.074993\n",
      "accuracy = 0.923458\n",
      "mean IU  = 0.710365\n",
      "    class # 0 capture rate = 0.923563 \n",
      "    class # 1 capture rate = 0.922319 \n",
      "TRAIN: Batch: 0.8951606598389492 Loss_Total: 10.061672\n",
      "TRAIN: Batch: 0.8951606598389492 Loss_Seg: 7.9408464\n",
      "accuracy = 0.957686\n",
      "mean IU  = 0.716840\n",
      "    class # 0 capture rate = 0.958198 \n",
      "    class # 1 capture rate = 0.945704 \n",
      "TRAIN: Batch: 0.8990696583535298 Loss_Total: 10.161032\n",
      "TRAIN: Batch: 0.8990696583535298 Loss_Seg: 8.824251\n",
      "accuracy = 0.960837\n",
      "mean IU  = 0.743293\n",
      "    class # 0 capture rate = 0.961920 \n",
      "    class # 1 capture rate = 0.938679 \n",
      "TRAIN: Batch: 0.9029786568681104 Loss_Total: 13.246386\n",
      "TRAIN: Batch: 0.9029786568681104 Loss_Seg: 12.7474985\n",
      "accuracy = 0.928221\n",
      "mean IU  = 0.714851\n",
      "    class # 0 capture rate = 0.926599 \n",
      "    class # 1 capture rate = 0.947401 \n",
      "TRAIN: Batch: 0.906887655382691 Loss_Total: 11.533176\n",
      "TRAIN: Batch: 0.906887655382691 Loss_Seg: 12.172625\n",
      "accuracy = 0.940169\n",
      "mean IU  = 0.729746\n",
      "    class # 0 capture rate = 0.940582 \n",
      "    class # 1 capture rate = 0.934722 \n",
      "TRAIN: Batch: 0.9107966538972715 Loss_Total: 13.286992\n",
      "TRAIN: Batch: 0.9107966538972715 Loss_Seg: 14.903182\n",
      "accuracy = 0.949948\n",
      "mean IU  = 0.713872\n",
      "    class # 0 capture rate = 0.955988 \n",
      "    class # 1 capture rate = 0.845536 \n",
      "TRAIN: Batch: 0.9147056524118521 Loss_Total: 12.40074\n",
      "TRAIN: Batch: 0.9147056524118521 Loss_Seg: 12.61771\n",
      "accuracy = 0.938001\n",
      "mean IU  = 0.734568\n",
      "    class # 0 capture rate = 0.937882 \n",
      "    class # 1 capture rate = 0.939442 \n",
      "TRAIN: Batch: 0.9186146509264327 Loss_Total: 11.308874\n",
      "TRAIN: Batch: 0.9186146509264327 Loss_Seg: 8.675359\n",
      "accuracy = 0.959807\n",
      "mean IU  = 0.763443\n",
      "    class # 0 capture rate = 0.960375 \n",
      "    class # 1 capture rate = 0.950218 \n",
      "TRAIN: Batch: 0.9225236494410132 Loss_Total: 11.543017\n",
      "TRAIN: Batch: 0.9225236494410132 Loss_Seg: 10.449703\n",
      "accuracy = 0.957908\n",
      "mean IU  = 0.767718\n",
      "    class # 0 capture rate = 0.958016 \n",
      "    class # 1 capture rate = 0.956242 \n",
      "TRAIN: Batch: 0.9264326479555938 Loss_Total: 14.665491\n",
      "TRAIN: Batch: 0.9264326479555938 Loss_Seg: 15.219606\n",
      "accuracy = 0.924796\n",
      "mean IU  = 0.698081\n",
      "    class # 0 capture rate = 0.925593 \n",
      "    class # 1 capture rate = 0.914956 \n",
      "TRAIN: Batch: 0.9303416464701744 Loss_Total: 9.551842\n",
      "TRAIN: Batch: 0.9303416464701744 Loss_Seg: 10.930972\n",
      "accuracy = 0.942058\n",
      "mean IU  = 0.700345\n",
      "    class # 0 capture rate = 0.941787 \n",
      "    class # 1 capture rate = 0.946952 \n",
      "TRAIN: Batch: 0.9342506449847549 Loss_Total: 11.925406\n",
      "TRAIN: Batch: 0.9342506449847549 Loss_Seg: 12.031239\n",
      "accuracy = 0.951387\n",
      "mean IU  = 0.745201\n",
      "    class # 0 capture rate = 0.954503 \n",
      "    class # 1 capture rate = 0.905457 \n",
      "TRAIN: Batch: 0.9381596434993354 Loss_Total: 10.887158\n",
      "TRAIN: Batch: 0.9381596434993354 Loss_Seg: 10.79159\n",
      "accuracy = 0.956855\n",
      "mean IU  = 0.765913\n",
      "    class # 0 capture rate = 0.957678 \n",
      "    class # 1 capture rate = 0.944510 \n",
      "TRAIN: Batch: 0.942068642013916 Loss_Total: 12.742773\n",
      "TRAIN: Batch: 0.942068642013916 Loss_Seg: 13.038655\n",
      "accuracy = 0.940438\n",
      "mean IU  = 0.716657\n",
      "    class # 0 capture rate = 0.940404 \n",
      "    class # 1 capture rate = 0.940946 \n",
      "TRAIN: Batch: 0.9459776405284966 Loss_Total: 18.140312\n",
      "TRAIN: Batch: 0.9459776405284966 Loss_Seg: 16.29892\n",
      "accuracy = 0.906735\n",
      "mean IU  = 0.649402\n",
      "    class # 0 capture rate = 0.905646 \n",
      "    class # 1 capture rate = 0.921926 \n",
      "TRAIN: Batch: 0.9498866390430771 Loss_Total: 17.657013\n",
      "TRAIN: Batch: 0.9498866390430771 Loss_Seg: 21.508926\n",
      "accuracy = 0.946182\n",
      "mean IU  = 0.749016\n",
      "    class # 0 capture rate = 0.952195 \n",
      "    class # 1 capture rate = 0.874128 \n",
      "TRAIN: Batch: 0.9537956375576577 Loss_Total: 11.709659\n",
      "TRAIN: Batch: 0.9537956375576577 Loss_Seg: 10.001499\n",
      "accuracy = 0.943746\n",
      "mean IU  = 0.692358\n",
      "    class # 0 capture rate = 0.942864 \n",
      "    class # 1 capture rate = 0.961780 \n",
      "TRAIN: Batch: 0.9577046360722383 Loss_Total: 13.381563\n",
      "TRAIN: Batch: 0.9577046360722383 Loss_Seg: 15.543224\n",
      "accuracy = 0.939928\n",
      "mean IU  = 0.691473\n",
      "    class # 0 capture rate = 0.944338 \n",
      "    class # 1 capture rate = 0.865443 \n",
      "TRAIN: Batch: 0.9616136345868188 Loss_Total: 9.195647\n",
      "TRAIN: Batch: 0.9616136345868188 Loss_Seg: 9.171744\n",
      "accuracy = 0.962816\n",
      "mean IU  = 0.762956\n",
      "    class # 0 capture rate = 0.964095 \n",
      "    class # 1 capture rate = 0.939221 \n",
      "TRAIN: Batch: 0.9655226331013994 Loss_Total: 10.050827\n",
      "TRAIN: Batch: 0.9655226331013994 Loss_Seg: 9.586137\n",
      "accuracy = 0.957584\n",
      "mean IU  = 0.759618\n",
      "    class # 0 capture rate = 0.958290 \n",
      "    class # 1 capture rate = 0.946128 \n",
      "TRAIN: Batch: 0.96943163161598 Loss_Total: 13.291998\n",
      "TRAIN: Batch: 0.96943163161598 Loss_Seg: 10.150893\n",
      "accuracy = 0.946920\n",
      "mean IU  = 0.733801\n",
      "    class # 0 capture rate = 0.946325 \n",
      "    class # 1 capture rate = 0.956053 \n",
      "TRAIN: Batch: 0.9733406301305606 Loss_Total: 12.684206\n",
      "TRAIN: Batch: 0.9733406301305606 Loss_Seg: 10.386963\n",
      "accuracy = 0.945867\n",
      "mean IU  = 0.724516\n",
      "    class # 0 capture rate = 0.945055 \n",
      "    class # 1 capture rate = 0.959089 \n",
      "TRAIN: Batch: 0.9772496286451411 Loss_Total: 10.919821\n",
      "TRAIN: Batch: 0.9772496286451411 Loss_Seg: 10.676934\n",
      "accuracy = 0.953197\n",
      "mean IU  = 0.741670\n",
      "    class # 0 capture rate = 0.954077 \n",
      "    class # 1 capture rate = 0.938600 \n",
      "TRAIN: Batch: 0.9811586271597217 Loss_Total: 13.025188\n",
      "TRAIN: Batch: 0.9811586271597217 Loss_Seg: 9.583023\n",
      "accuracy = 0.942468\n",
      "mean IU  = 0.690060\n",
      "    class # 0 capture rate = 0.941353 \n",
      "    class # 1 capture rate = 0.965122 \n",
      "TRAIN: Batch: 0.9850676256743023 Loss_Total: 10.490307\n",
      "TRAIN: Batch: 0.9850676256743023 Loss_Seg: 9.203007\n",
      "accuracy = 0.950018\n",
      "mean IU  = 0.748875\n",
      "    class # 0 capture rate = 0.949321 \n",
      "    class # 1 capture rate = 0.960245 \n",
      "TRAIN: Batch: 0.9889766241888828 Loss_Total: 10.339746\n",
      "TRAIN: Batch: 0.9889766241888828 Loss_Seg: 12.064668\n",
      "accuracy = 0.929989\n",
      "mean IU  = 0.693160\n",
      "    class # 0 capture rate = 0.927945 \n",
      "    class # 1 capture rate = 0.960784 \n",
      "TRAIN: Batch: 0.9928856227034634 Loss_Total: 15.035762\n",
      "TRAIN: Batch: 0.9928856227034634 Loss_Seg: 17.933125\n",
      "accuracy = 0.937389\n",
      "mean IU  = 0.706474\n",
      "    class # 0 capture rate = 0.943707 \n",
      "    class # 1 capture rate = 0.850504 \n",
      "TRAIN: Batch: 0.996794621218044 Loss_Total: 11.747531\n",
      "TRAIN: Batch: 0.996794621218044 Loss_Seg: 10.505465\n",
      "accuracy = 0.950990\n",
      "mean IU  = 0.736577\n",
      "    class # 0 capture rate = 0.950687 \n",
      "    class # 1 capture rate = 0.956034 \n",
      "Validating NN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID: Total accuracy: 94.820869%. Class 0 capture: 94.930846%. Class 1 capture: 93.088522%\n",
      "Character error rate improved, save model\n",
      "Epoch: 2  Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.0 Loss_Total: 11.771904\n",
      "TRAIN: Batch: 0.0 Loss_Seg: 13.088664\n",
      "accuracy = 0.946306\n",
      "mean IU  = 0.722357\n",
      "    class # 0 capture rate = 0.949046 \n",
      "    class # 1 capture rate = 0.903226 \n",
      "TRAIN: Batch: 0.003908998514580564 Loss_Total: 13.25333\n",
      "TRAIN: Batch: 0.003908998514580564 Loss_Seg: 13.958004\n",
      "accuracy = 0.930657\n",
      "mean IU  = 0.685781\n",
      "    class # 0 capture rate = 0.932021 \n",
      "    class # 1 capture rate = 0.909708 \n",
      "TRAIN: Batch: 0.007817997029161129 Loss_Total: 10.245403\n",
      "TRAIN: Batch: 0.007817997029161129 Loss_Seg: 10.617004\n",
      "accuracy = 0.938364\n",
      "mean IU  = 0.706856\n",
      "    class # 0 capture rate = 0.937232 \n",
      "    class # 1 capture rate = 0.956325 \n",
      "TRAIN: Batch: 0.011726995543741693 Loss_Total: 15.091477\n",
      "TRAIN: Batch: 0.011726995543741693 Loss_Seg: 14.97229\n",
      "accuracy = 0.940566\n",
      "mean IU  = 0.719412\n",
      "    class # 0 capture rate = 0.944901 \n",
      "    class # 1 capture rate = 0.881149 \n",
      "TRAIN: Batch: 0.015635994058322257 Loss_Total: 14.76417\n",
      "TRAIN: Batch: 0.015635994058322257 Loss_Seg: 16.08379\n",
      "accuracy = 0.927026\n",
      "mean IU  = 0.713024\n",
      "    class # 0 capture rate = 0.928385 \n",
      "    class # 1 capture rate = 0.911730 \n",
      "TRAIN: Batch: 0.019544992572902823 Loss_Total: 11.4468\n",
      "TRAIN: Batch: 0.019544992572902823 Loss_Seg: 10.846294\n",
      "accuracy = 0.948800\n",
      "mean IU  = 0.744407\n",
      "    class # 0 capture rate = 0.949329 \n",
      "    class # 1 capture rate = 0.941154 \n",
      "TRAIN: Batch: 0.023453991087483386 Loss_Total: 11.636444\n",
      "TRAIN: Batch: 0.023453991087483386 Loss_Seg: 9.527279\n",
      "accuracy = 0.948963\n",
      "mean IU  = 0.710161\n",
      "    class # 0 capture rate = 0.949177 \n",
      "    class # 1 capture rate = 0.944779 \n",
      "TRAIN: Batch: 0.02736298960206395 Loss_Total: 11.678892\n",
      "TRAIN: Batch: 0.02736298960206395 Loss_Seg: 11.460105\n",
      "accuracy = 0.942236\n",
      "mean IU  = 0.698433\n",
      "    class # 0 capture rate = 0.942373 \n",
      "    class # 1 capture rate = 0.939736 \n",
      "TRAIN: Batch: 0.031271988116644514 Loss_Total: 13.187942\n",
      "TRAIN: Batch: 0.031271988116644514 Loss_Seg: 9.207293\n",
      "accuracy = 0.958129\n",
      "mean IU  = 0.778875\n",
      "    class # 0 capture rate = 0.958215 \n",
      "    class # 1 capture rate = 0.956922 \n",
      "TRAIN: Batch: 0.035180986631225084 Loss_Total: 9.593007\n",
      "TRAIN: Batch: 0.035180986631225084 Loss_Seg: 9.438977\n",
      "accuracy = 0.946504\n",
      "mean IU  = 0.719695\n",
      "    class # 0 capture rate = 0.945531 \n",
      "    class # 1 capture rate = 0.963344 \n",
      "TRAIN: Batch: 0.039089985145805646 Loss_Total: 9.688713\n",
      "TRAIN: Batch: 0.039089985145805646 Loss_Seg: 7.168802\n",
      "accuracy = 0.963110\n",
      "mean IU  = 0.745835\n",
      "    class # 0 capture rate = 0.963447 \n",
      "    class # 1 capture rate = 0.955717 \n",
      "TRAIN: Batch: 0.04299898366038621 Loss_Total: 8.613953\n",
      "TRAIN: Batch: 0.04299898366038621 Loss_Seg: 10.103638\n",
      "accuracy = 0.959077\n",
      "mean IU  = 0.770197\n",
      "    class # 0 capture rate = 0.960366 \n",
      "    class # 1 capture rate = 0.939273 \n",
      "TRAIN: Batch: 0.04690798217496677 Loss_Total: 11.56797\n",
      "TRAIN: Batch: 0.04690798217496677 Loss_Seg: 11.396579\n",
      "accuracy = 0.952875\n",
      "mean IU  = 0.723072\n",
      "    class # 0 capture rate = 0.953786 \n",
      "    class # 1 capture rate = 0.935372 \n",
      "TRAIN: Batch: 0.05081698068954734 Loss_Total: 13.654899\n",
      "TRAIN: Batch: 0.05081698068954734 Loss_Seg: 13.012779\n",
      "accuracy = 0.938588\n",
      "mean IU  = 0.724947\n",
      "    class # 0 capture rate = 0.940139 \n",
      "    class # 1 capture rate = 0.918353 \n",
      "TRAIN: Batch: 0.0547259792041279 Loss_Total: 10.762151\n",
      "TRAIN: Batch: 0.0547259792041279 Loss_Seg: 9.379547\n",
      "accuracy = 0.954479\n",
      "mean IU  = 0.736588\n",
      "    class # 0 capture rate = 0.954043 \n",
      "    class # 1 capture rate = 0.962506 \n",
      "TRAIN: Batch: 0.058634977718708466 Loss_Total: 15.61125\n",
      "TRAIN: Batch: 0.058634977718708466 Loss_Seg: 17.036705\n",
      "accuracy = 0.929309\n",
      "mean IU  = 0.700192\n",
      "    class # 0 capture rate = 0.931680 \n",
      "    class # 1 capture rate = 0.898496 \n",
      "TRAIN: Batch: 0.06254397623328903 Loss_Total: 9.86837\n",
      "TRAIN: Batch: 0.06254397623328903 Loss_Seg: 10.352826\n",
      "accuracy = 0.951090\n",
      "mean IU  = 0.731653\n",
      "    class # 0 capture rate = 0.952090 \n",
      "    class # 1 capture rate = 0.934095 \n",
      "TRAIN: Batch: 0.0664529747478696 Loss_Total: 12.22471\n",
      "TRAIN: Batch: 0.0664529747478696 Loss_Seg: 12.220057\n",
      "accuracy = 0.937486\n",
      "mean IU  = 0.717042\n",
      "    class # 0 capture rate = 0.937113 \n",
      "    class # 1 capture rate = 0.942731 \n",
      "TRAIN: Batch: 0.07036197326245017 Loss_Total: 10.035315\n",
      "TRAIN: Batch: 0.07036197326245017 Loss_Seg: 9.492744\n",
      "accuracy = 0.950411\n",
      "mean IU  = 0.735785\n",
      "    class # 0 capture rate = 0.950272 \n",
      "    class # 1 capture rate = 0.952706 \n",
      "TRAIN: Batch: 0.07427097177703072 Loss_Total: 16.54137\n",
      "TRAIN: Batch: 0.07427097177703072 Loss_Seg: 15.652062\n",
      "accuracy = 0.935946\n",
      "mean IU  = 0.710036\n",
      "    class # 0 capture rate = 0.939375 \n",
      "    class # 1 capture rate = 0.889481 \n",
      "TRAIN: Batch: 0.07817997029161129 Loss_Total: 13.733405\n",
      "TRAIN: Batch: 0.07817997029161129 Loss_Seg: 14.065106\n",
      "accuracy = 0.938477\n",
      "mean IU  = 0.719873\n",
      "    class # 0 capture rate = 0.939022 \n",
      "    class # 1 capture rate = 0.930947 \n",
      "TRAIN: Batch: 0.08208896880619185 Loss_Total: 11.392101\n",
      "TRAIN: Batch: 0.08208896880619185 Loss_Seg: 11.873238\n",
      "accuracy = 0.949165\n",
      "mean IU  = 0.729636\n",
      "    class # 0 capture rate = 0.950605 \n",
      "    class # 1 capture rate = 0.925707 \n",
      "TRAIN: Batch: 0.08599796732077242 Loss_Total: 13.50108\n",
      "TRAIN: Batch: 0.08599796732077242 Loss_Seg: 12.422516\n",
      "accuracy = 0.933348\n",
      "mean IU  = 0.693373\n",
      "    class # 0 capture rate = 0.933486 \n",
      "    class # 1 capture rate = 0.931202 \n",
      "TRAIN: Batch: 0.08990696583535299 Loss_Total: 12.8652935\n",
      "TRAIN: Batch: 0.08990696583535299 Loss_Seg: 11.576009\n",
      "accuracy = 0.936799\n",
      "mean IU  = 0.693651\n",
      "    class # 0 capture rate = 0.935827 \n",
      "    class # 1 capture rate = 0.953478 \n",
      "TRAIN: Batch: 0.09381596434993354 Loss_Total: 9.08034\n",
      "TRAIN: Batch: 0.09381596434993354 Loss_Seg: 8.426389\n",
      "accuracy = 0.963400\n",
      "mean IU  = 0.769667\n",
      "    class # 0 capture rate = 0.964618 \n",
      "    class # 1 capture rate = 0.941716 \n",
      "TRAIN: Batch: 0.09772496286451411 Loss_Total: 12.297274\n",
      "TRAIN: Batch: 0.09772496286451411 Loss_Seg: 13.563141\n",
      "accuracy = 0.932100\n",
      "mean IU  = 0.710412\n",
      "    class # 0 capture rate = 0.931355 \n",
      "    class # 1 capture rate = 0.941953 \n",
      "TRAIN: Batch: 0.10163396137909468 Loss_Total: 14.639616\n",
      "TRAIN: Batch: 0.10163396137909468 Loss_Seg: 15.628064\n",
      "accuracy = 0.929208\n",
      "mean IU  = 0.706788\n",
      "    class # 0 capture rate = 0.930144 \n",
      "    class # 1 capture rate = 0.917492 \n",
      "TRAIN: Batch: 0.10554295989367524 Loss_Total: 14.234616\n",
      "TRAIN: Batch: 0.10554295989367524 Loss_Seg: 12.22142\n",
      "accuracy = 0.944309\n",
      "mean IU  = 0.727696\n",
      "    class # 0 capture rate = 0.945969 \n",
      "    class # 1 capture rate = 0.920091 \n",
      "TRAIN: Batch: 0.1094519584082558 Loss_Total: 11.573032\n",
      "TRAIN: Batch: 0.1094519584082558 Loss_Seg: 9.541053\n",
      "accuracy = 0.959784\n",
      "mean IU  = 0.767621\n",
      "    class # 0 capture rate = 0.961167 \n",
      "    class # 1 capture rate = 0.937604 \n",
      "TRAIN: Batch: 0.11336095692283638 Loss_Total: 10.136732\n",
      "TRAIN: Batch: 0.11336095692283638 Loss_Seg: 9.586246\n",
      "accuracy = 0.952089\n",
      "mean IU  = 0.748685\n",
      "    class # 0 capture rate = 0.951376 \n",
      "    class # 1 capture rate = 0.963188 \n",
      "TRAIN: Batch: 0.11726995543741693 Loss_Total: 15.15656\n",
      "TRAIN: Batch: 0.11726995543741693 Loss_Seg: 15.312029\n",
      "accuracy = 0.937048\n",
      "mean IU  = 0.719186\n",
      "    class # 0 capture rate = 0.939928 \n",
      "    class # 1 capture rate = 0.899693 \n",
      "TRAIN: Batch: 0.1211789539519975 Loss_Total: 13.146229\n",
      "TRAIN: Batch: 0.1211789539519975 Loss_Seg: 17.1354\n",
      "accuracy = 0.913187\n",
      "mean IU  = 0.664952\n",
      "    class # 0 capture rate = 0.914186 \n",
      "    class # 1 capture rate = 0.900035 \n",
      "TRAIN: Batch: 0.12508795246657806 Loss_Total: 13.631344\n",
      "TRAIN: Batch: 0.12508795246657806 Loss_Seg: 14.653749\n",
      "accuracy = 0.915287\n",
      "mean IU  = 0.627363\n",
      "    class # 0 capture rate = 0.914585 \n",
      "    class # 1 capture rate = 0.929303 \n",
      "TRAIN: Batch: 0.12899695098115863 Loss_Total: 11.13505\n",
      "TRAIN: Batch: 0.12899695098115863 Loss_Seg: 9.0336\n",
      "accuracy = 0.953952\n",
      "mean IU  = 0.726911\n",
      "    class # 0 capture rate = 0.953809 \n",
      "    class # 1 capture rate = 0.956740 \n",
      "TRAIN: Batch: 0.1329059494957392 Loss_Total: 12.097126\n",
      "TRAIN: Batch: 0.1329059494957392 Loss_Seg: 11.800388\n",
      "accuracy = 0.942992\n",
      "mean IU  = 0.727929\n",
      "    class # 0 capture rate = 0.942922 \n",
      "    class # 1 capture rate = 0.944003 \n",
      "TRAIN: Batch: 0.13681494801031976 Loss_Total: 13.656237\n",
      "TRAIN: Batch: 0.13681494801031976 Loss_Seg: 10.619039\n",
      "accuracy = 0.957591\n",
      "mean IU  = 0.730400\n",
      "    class # 0 capture rate = 0.960402 \n",
      "    class # 1 capture rate = 0.901831 \n",
      "TRAIN: Batch: 0.14072394652490033 Loss_Total: 12.522333\n",
      "TRAIN: Batch: 0.14072394652490033 Loss_Seg: 10.612873\n",
      "accuracy = 0.956141\n",
      "mean IU  = 0.778870\n",
      "    class # 0 capture rate = 0.957245 \n",
      "    class # 1 capture rate = 0.941741 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.14463294503948088 Loss_Total: 9.385923\n",
      "TRAIN: Batch: 0.14463294503948088 Loss_Seg: 9.031535\n",
      "accuracy = 0.958245\n",
      "mean IU  = 0.768939\n",
      "    class # 0 capture rate = 0.958348 \n",
      "    class # 1 capture rate = 0.956644 \n",
      "TRAIN: Batch: 0.14854194355406145 Loss_Total: 12.50868\n",
      "TRAIN: Batch: 0.14854194355406145 Loss_Seg: 12.24127\n",
      "accuracy = 0.932393\n",
      "mean IU  = 0.703994\n",
      "    class # 0 capture rate = 0.931505 \n",
      "    class # 1 capture rate = 0.944937 \n",
      "TRAIN: Batch: 0.15245094206864201 Loss_Total: 10.736628\n",
      "TRAIN: Batch: 0.15245094206864201 Loss_Seg: 9.615151\n",
      "accuracy = 0.950508\n",
      "mean IU  = 0.725058\n",
      "    class # 0 capture rate = 0.950741 \n",
      "    class # 1 capture rate = 0.946321 \n",
      "TRAIN: Batch: 0.15635994058322258 Loss_Total: 14.529728\n",
      "TRAIN: Batch: 0.15635994058322258 Loss_Seg: 10.392198\n",
      "accuracy = 0.946448\n",
      "mean IU  = 0.709972\n",
      "    class # 0 capture rate = 0.946842 \n",
      "    class # 1 capture rate = 0.939237 \n",
      "TRAIN: Batch: 0.16026893909780315 Loss_Total: 12.95123\n",
      "TRAIN: Batch: 0.16026893909780315 Loss_Seg: 14.602104\n",
      "accuracy = 0.946839\n",
      "mean IU  = 0.719580\n",
      "    class # 0 capture rate = 0.950254 \n",
      "    class # 1 capture rate = 0.891858 \n",
      "TRAIN: Batch: 0.1641779376123837 Loss_Total: 13.177728\n",
      "TRAIN: Batch: 0.1641779376123837 Loss_Seg: 10.245171\n",
      "accuracy = 0.955614\n",
      "mean IU  = 0.754737\n",
      "    class # 0 capture rate = 0.956583 \n",
      "    class # 1 capture rate = 0.940231 \n",
      "TRAIN: Batch: 0.16808693612696426 Loss_Total: 12.448774\n",
      "TRAIN: Batch: 0.16808693612696426 Loss_Seg: 11.880137\n",
      "accuracy = 0.931369\n",
      "mean IU  = 0.691005\n",
      "    class # 0 capture rate = 0.930333 \n",
      "    class # 1 capture rate = 0.947475 \n",
      "TRAIN: Batch: 0.17199593464154483 Loss_Total: 10.9416485\n",
      "TRAIN: Batch: 0.17199593464154483 Loss_Seg: 8.627723\n",
      "accuracy = 0.960712\n",
      "mean IU  = 0.791537\n",
      "    class # 0 capture rate = 0.960921 \n",
      "    class # 1 capture rate = 0.957872 \n",
      "TRAIN: Batch: 0.1759049331561254 Loss_Total: 10.154672\n",
      "TRAIN: Batch: 0.1759049331561254 Loss_Seg: 10.662375\n",
      "accuracy = 0.956567\n",
      "mean IU  = 0.725896\n",
      "    class # 0 capture rate = 0.958997 \n",
      "    class # 1 capture rate = 0.907637 \n",
      "TRAIN: Batch: 0.17981393167070597 Loss_Total: 11.3417425\n",
      "TRAIN: Batch: 0.17981393167070597 Loss_Seg: 10.244945\n",
      "accuracy = 0.955706\n",
      "mean IU  = 0.722729\n",
      "    class # 0 capture rate = 0.956481 \n",
      "    class # 1 capture rate = 0.939507 \n",
      "TRAIN: Batch: 0.18372293018528654 Loss_Total: 11.434405\n",
      "TRAIN: Batch: 0.18372293018528654 Loss_Seg: 7.582061\n",
      "accuracy = 0.964688\n",
      "mean IU  = 0.791004\n",
      "    class # 0 capture rate = 0.964308 \n",
      "    class # 1 capture rate = 0.970722 \n",
      "TRAIN: Batch: 0.18763192869986708 Loss_Total: 13.936195\n",
      "TRAIN: Batch: 0.18763192869986708 Loss_Seg: 15.797\n",
      "accuracy = 0.932606\n",
      "mean IU  = 0.720549\n",
      "    class # 0 capture rate = 0.933379 \n",
      "    class # 1 capture rate = 0.923345 \n",
      "TRAIN: Batch: 0.19154092721444765 Loss_Total: 10.431386\n",
      "TRAIN: Batch: 0.19154092721444765 Loss_Seg: 9.251555\n",
      "accuracy = 0.957149\n",
      "mean IU  = 0.772049\n",
      "    class # 0 capture rate = 0.957025 \n",
      "    class # 1 capture rate = 0.958951 \n",
      "TRAIN: Batch: 0.19544992572902822 Loss_Total: 13.342575\n",
      "TRAIN: Batch: 0.19544992572902822 Loss_Seg: 14.370415\n",
      "accuracy = 0.929262\n",
      "mean IU  = 0.688007\n",
      "    class # 0 capture rate = 0.930298 \n",
      "    class # 1 capture rate = 0.914012 \n",
      "TRAIN: Batch: 0.1993589242436088 Loss_Total: 12.331545\n",
      "TRAIN: Batch: 0.1993589242436088 Loss_Seg: 10.640962\n",
      "accuracy = 0.946924\n",
      "mean IU  = 0.713818\n",
      "    class # 0 capture rate = 0.948291 \n",
      "    class # 1 capture rate = 0.922902 \n",
      "TRAIN: Batch: 0.20326792275818936 Loss_Total: 17.080633\n",
      "TRAIN: Batch: 0.20326792275818936 Loss_Seg: 15.161338\n",
      "accuracy = 0.920778\n",
      "mean IU  = 0.706981\n",
      "    class # 0 capture rate = 0.918419 \n",
      "    class # 1 capture rate = 0.946601 \n",
      "TRAIN: Batch: 0.2071769212727699 Loss_Total: 9.387613\n",
      "TRAIN: Batch: 0.2071769212727699 Loss_Seg: 8.256981\n",
      "accuracy = 0.955196\n",
      "mean IU  = 0.748606\n",
      "    class # 0 capture rate = 0.954803 \n",
      "    class # 1 capture rate = 0.961859 \n",
      "TRAIN: Batch: 0.21108591978735047 Loss_Total: 9.682087\n",
      "TRAIN: Batch: 0.21108591978735047 Loss_Seg: 9.5993805\n",
      "accuracy = 0.950291\n",
      "mean IU  = 0.724753\n",
      "    class # 0 capture rate = 0.949941 \n",
      "    class # 1 capture rate = 0.956623 \n",
      "TRAIN: Batch: 0.21499491830193104 Loss_Total: 12.740597\n",
      "TRAIN: Batch: 0.21499491830193104 Loss_Seg: 10.030464\n",
      "accuracy = 0.949059\n",
      "mean IU  = 0.733703\n",
      "    class # 0 capture rate = 0.948297 \n",
      "    class # 1 capture rate = 0.961490 \n",
      "TRAIN: Batch: 0.2189039168165116 Loss_Total: 10.695341\n",
      "TRAIN: Batch: 0.2189039168165116 Loss_Seg: 7.5235267\n",
      "accuracy = 0.964746\n",
      "mean IU  = 0.768202\n",
      "    class # 0 capture rate = 0.964797 \n",
      "    class # 1 capture rate = 0.963754 \n",
      "TRAIN: Batch: 0.22281291533109218 Loss_Total: 12.124649\n",
      "TRAIN: Batch: 0.22281291533109218 Loss_Seg: 10.748302\n",
      "accuracy = 0.954781\n",
      "mean IU  = 0.778679\n",
      "    class # 0 capture rate = 0.954608 \n",
      "    class # 1 capture rate = 0.956989 \n",
      "TRAIN: Batch: 0.22672191384567275 Loss_Total: 8.951075\n",
      "TRAIN: Batch: 0.22672191384567275 Loss_Seg: 8.9731045\n",
      "accuracy = 0.954634\n",
      "mean IU  = 0.744902\n",
      "    class # 0 capture rate = 0.955457 \n",
      "    class # 1 capture rate = 0.940792 \n",
      "TRAIN: Batch: 0.2306309123602533 Loss_Total: 15.574973\n",
      "TRAIN: Batch: 0.2306309123602533 Loss_Seg: 17.471647\n",
      "accuracy = 0.916559\n",
      "mean IU  = 0.684264\n",
      "    class # 0 capture rate = 0.915876 \n",
      "    class # 1 capture rate = 0.924824 \n",
      "TRAIN: Batch: 0.23453991087483386 Loss_Total: 12.188395\n",
      "TRAIN: Batch: 0.23453991087483386 Loss_Seg: 8.419311\n",
      "accuracy = 0.963793\n",
      "mean IU  = 0.778504\n",
      "    class # 0 capture rate = 0.964917 \n",
      "    class # 1 capture rate = 0.944974 \n",
      "TRAIN: Batch: 0.23844890938941443 Loss_Total: 9.396624\n",
      "TRAIN: Batch: 0.23844890938941443 Loss_Seg: 8.9719515\n",
      "accuracy = 0.951488\n",
      "mean IU  = 0.746592\n",
      "    class # 0 capture rate = 0.950989 \n",
      "    class # 1 capture rate = 0.959241 \n",
      "TRAIN: Batch: 0.242357907903995 Loss_Total: 11.762966\n",
      "TRAIN: Batch: 0.242357907903995 Loss_Seg: 12.011165\n",
      "accuracy = 0.953198\n",
      "mean IU  = 0.746249\n",
      "    class # 0 capture rate = 0.954574 \n",
      "    class # 1 capture rate = 0.931445 \n",
      "TRAIN: Batch: 0.24626690641857557 Loss_Total: 10.8067255\n",
      "TRAIN: Batch: 0.24626690641857557 Loss_Seg: 9.513356\n",
      "accuracy = 0.952532\n",
      "mean IU  = 0.770565\n",
      "    class # 0 capture rate = 0.950415 \n",
      "    class # 1 capture rate = 0.980542 \n",
      "TRAIN: Batch: 0.2501759049331561 Loss_Total: 11.464535\n",
      "TRAIN: Batch: 0.2501759049331561 Loss_Seg: 7.2790174\n",
      "accuracy = 0.960148\n",
      "mean IU  = 0.743912\n",
      "    class # 0 capture rate = 0.959774 \n",
      "    class # 1 capture rate = 0.967861 \n",
      "TRAIN: Batch: 0.2540849034477367 Loss_Total: 12.7033615\n",
      "TRAIN: Batch: 0.2540849034477367 Loss_Seg: 15.399538\n",
      "accuracy = 0.930783\n",
      "mean IU  = 0.677428\n",
      "    class # 0 capture rate = 0.931899 \n",
      "    class # 1 capture rate = 0.912220 \n",
      "TRAIN: Batch: 0.25799390196231725 Loss_Total: 13.430002\n",
      "TRAIN: Batch: 0.25799390196231725 Loss_Seg: 11.210196\n",
      "accuracy = 0.952640\n",
      "mean IU  = 0.743831\n",
      "    class # 0 capture rate = 0.953576 \n",
      "    class # 1 capture rate = 0.937656 \n",
      "TRAIN: Batch: 0.2619029004768978 Loss_Total: 10.7289715\n",
      "TRAIN: Batch: 0.2619029004768978 Loss_Seg: 10.623516\n",
      "accuracy = 0.936546\n",
      "mean IU  = 0.692765\n",
      "    class # 0 capture rate = 0.936001 \n",
      "    class # 1 capture rate = 0.945839 \n",
      "TRAIN: Batch: 0.2658118989914784 Loss_Total: 9.563876\n",
      "TRAIN: Batch: 0.2658118989914784 Loss_Seg: 9.365574\n",
      "accuracy = 0.958762\n",
      "mean IU  = 0.762977\n",
      "    class # 0 capture rate = 0.958709 \n",
      "    class # 1 capture rate = 0.959639 \n",
      "TRAIN: Batch: 0.26972089750605893 Loss_Total: 12.762104\n",
      "TRAIN: Batch: 0.26972089750605893 Loss_Seg: 11.889506\n",
      "accuracy = 0.945478\n",
      "mean IU  = 0.748251\n",
      "    class # 0 capture rate = 0.945438 \n",
      "    class # 1 capture rate = 0.945992 \n",
      "TRAIN: Batch: 0.27362989602063953 Loss_Total: 9.431331\n",
      "TRAIN: Batch: 0.27362989602063953 Loss_Seg: 9.432046\n",
      "accuracy = 0.954149\n",
      "mean IU  = 0.742342\n",
      "    class # 0 capture rate = 0.954149 \n",
      "    class # 1 capture rate = 0.954161 \n",
      "TRAIN: Batch: 0.27753889453522007 Loss_Total: 9.160264\n",
      "TRAIN: Batch: 0.27753889453522007 Loss_Seg: 9.9620905\n",
      "accuracy = 0.954882\n",
      "mean IU  = 0.759078\n",
      "    class # 0 capture rate = 0.954983 \n",
      "    class # 1 capture rate = 0.953341 \n",
      "TRAIN: Batch: 0.28144789304980067 Loss_Total: 14.01203\n",
      "TRAIN: Batch: 0.28144789304980067 Loss_Seg: 14.31721\n",
      "accuracy = 0.927827\n",
      "mean IU  = 0.720002\n",
      "    class # 0 capture rate = 0.926361 \n",
      "    class # 1 capture rate = 0.944181 \n",
      "TRAIN: Batch: 0.2853568915643812 Loss_Total: 11.89177\n",
      "TRAIN: Batch: 0.2853568915643812 Loss_Seg: 11.947562\n",
      "accuracy = 0.940520\n",
      "mean IU  = 0.694345\n",
      "    class # 0 capture rate = 0.941377 \n",
      "    class # 1 capture rate = 0.925173 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.28926589007896175 Loss_Total: 19.248941\n",
      "TRAIN: Batch: 0.28926589007896175 Loss_Seg: 17.190273\n",
      "accuracy = 0.914398\n",
      "mean IU  = 0.683161\n",
      "    class # 0 capture rate = 0.914324 \n",
      "    class # 1 capture rate = 0.915254 \n",
      "TRAIN: Batch: 0.29317488859354235 Loss_Total: 12.779602\n",
      "TRAIN: Batch: 0.29317488859354235 Loss_Seg: 11.170352\n",
      "accuracy = 0.945272\n",
      "mean IU  = 0.736123\n",
      "    class # 0 capture rate = 0.945355 \n",
      "    class # 1 capture rate = 0.944092 \n",
      "TRAIN: Batch: 0.2970838871081229 Loss_Total: 14.875124\n",
      "TRAIN: Batch: 0.2970838871081229 Loss_Seg: 16.40065\n",
      "accuracy = 0.929519\n",
      "mean IU  = 0.703039\n",
      "    class # 0 capture rate = 0.933216 \n",
      "    class # 1 capture rate = 0.883322 \n",
      "TRAIN: Batch: 0.3009928856227035 Loss_Total: 10.620148\n",
      "TRAIN: Batch: 0.3009928856227035 Loss_Seg: 11.338858\n",
      "accuracy = 0.946455\n",
      "mean IU  = 0.707981\n",
      "    class # 0 capture rate = 0.948333 \n",
      "    class # 1 capture rate = 0.912558 \n",
      "TRAIN: Batch: 0.30490188413728403 Loss_Total: 11.552452\n",
      "TRAIN: Batch: 0.30490188413728403 Loss_Seg: 11.527665\n",
      "accuracy = 0.945527\n",
      "mean IU  = 0.719300\n",
      "    class # 0 capture rate = 0.945562 \n",
      "    class # 1 capture rate = 0.944946 \n",
      "TRAIN: Batch: 0.30881088265186457 Loss_Total: 10.85788\n",
      "TRAIN: Batch: 0.30881088265186457 Loss_Seg: 10.091891\n",
      "accuracy = 0.955068\n",
      "mean IU  = 0.789162\n",
      "    class # 0 capture rate = 0.954626 \n",
      "    class # 1 capture rate = 0.960248 \n",
      "TRAIN: Batch: 0.31271988116644517 Loss_Total: 12.58563\n",
      "TRAIN: Batch: 0.31271988116644517 Loss_Seg: 11.529812\n",
      "accuracy = 0.949208\n",
      "mean IU  = 0.721208\n",
      "    class # 0 capture rate = 0.950568 \n",
      "    class # 1 capture rate = 0.925373 \n",
      "TRAIN: Batch: 0.3166288796810257 Loss_Total: 11.39044\n",
      "TRAIN: Batch: 0.3166288796810257 Loss_Seg: 9.512561\n",
      "accuracy = 0.949621\n",
      "mean IU  = 0.733140\n",
      "    class # 0 capture rate = 0.949342 \n",
      "    class # 1 capture rate = 0.954234 \n",
      "TRAIN: Batch: 0.3205378781956063 Loss_Total: 14.234333\n",
      "TRAIN: Batch: 0.3205378781956063 Loss_Seg: 9.905256\n",
      "accuracy = 0.953704\n",
      "mean IU  = 0.753721\n",
      "    class # 0 capture rate = 0.953517 \n",
      "    class # 1 capture rate = 0.956592 \n",
      "TRAIN: Batch: 0.32444687671018685 Loss_Total: 12.884379\n",
      "TRAIN: Batch: 0.32444687671018685 Loss_Seg: 10.868197\n",
      "accuracy = 0.944634\n",
      "mean IU  = 0.738763\n",
      "    class # 0 capture rate = 0.944117 \n",
      "    class # 1 capture rate = 0.951794 \n",
      "TRAIN: Batch: 0.3283558752247674 Loss_Total: 12.17408\n",
      "TRAIN: Batch: 0.3283558752247674 Loss_Seg: 7.3178234\n",
      "accuracy = 0.961711\n",
      "mean IU  = 0.755271\n",
      "    class # 0 capture rate = 0.962044 \n",
      "    class # 1 capture rate = 0.955268 \n",
      "TRAIN: Batch: 0.332264873739348 Loss_Total: 12.890879\n",
      "TRAIN: Batch: 0.332264873739348 Loss_Seg: 17.68062\n",
      "accuracy = 0.914261\n",
      "mean IU  = 0.692776\n",
      "    class # 0 capture rate = 0.912597 \n",
      "    class # 1 capture rate = 0.932311 \n",
      "TRAIN: Batch: 0.33617387225392853 Loss_Total: 12.324219\n",
      "TRAIN: Batch: 0.33617387225392853 Loss_Seg: 9.360169\n",
      "accuracy = 0.949506\n",
      "mean IU  = 0.729641\n",
      "    class # 0 capture rate = 0.949170 \n",
      "    class # 1 capture rate = 0.955204 \n",
      "TRAIN: Batch: 0.3400828707685091 Loss_Total: 15.106307\n",
      "TRAIN: Batch: 0.3400828707685091 Loss_Seg: 13.5792885\n",
      "accuracy = 0.946017\n",
      "mean IU  = 0.738053\n",
      "    class # 0 capture rate = 0.947046 \n",
      "    class # 1 capture rate = 0.931517 \n",
      "TRAIN: Batch: 0.34399186928308967 Loss_Total: 13.521893\n",
      "TRAIN: Batch: 0.34399186928308967 Loss_Seg: 16.305779\n",
      "accuracy = 0.940744\n",
      "mean IU  = 0.702511\n",
      "    class # 0 capture rate = 0.945405 \n",
      "    class # 1 capture rate = 0.867671 \n",
      "TRAIN: Batch: 0.3479008677976702 Loss_Total: 11.150132\n",
      "TRAIN: Batch: 0.3479008677976702 Loss_Seg: 10.459957\n",
      "accuracy = 0.949424\n",
      "mean IU  = 0.720528\n",
      "    class # 0 capture rate = 0.951487 \n",
      "    class # 1 capture rate = 0.913357 \n",
      "TRAIN: Batch: 0.3518098663122508 Loss_Total: 9.985936\n",
      "TRAIN: Batch: 0.3518098663122508 Loss_Seg: 6.9666715\n",
      "accuracy = 0.962789\n",
      "mean IU  = 0.792756\n",
      "    class # 0 capture rate = 0.961879 \n",
      "    class # 1 capture rate = 0.976190 \n",
      "TRAIN: Batch: 0.35571886482683135 Loss_Total: 8.317243\n",
      "TRAIN: Batch: 0.35571886482683135 Loss_Seg: 7.3509502\n",
      "accuracy = 0.958221\n",
      "mean IU  = 0.749521\n",
      "    class # 0 capture rate = 0.957515 \n",
      "    class # 1 capture rate = 0.971333 \n",
      "TRAIN: Batch: 0.35962786334141195 Loss_Total: 10.922131\n",
      "TRAIN: Batch: 0.35962786334141195 Loss_Seg: 10.43092\n",
      "accuracy = 0.948186\n",
      "mean IU  = 0.724468\n",
      "    class # 0 capture rate = 0.947998 \n",
      "    class # 1 capture rate = 0.951392 \n",
      "TRAIN: Batch: 0.3635368618559925 Loss_Total: 11.528812\n",
      "TRAIN: Batch: 0.3635368618559925 Loss_Seg: 10.329867\n",
      "accuracy = 0.952454\n",
      "mean IU  = 0.766995\n",
      "    class # 0 capture rate = 0.952580 \n",
      "    class # 1 capture rate = 0.950797 \n",
      "TRAIN: Batch: 0.3674458603705731 Loss_Total: 9.155677\n",
      "TRAIN: Batch: 0.3674458603705731 Loss_Seg: 8.44184\n",
      "accuracy = 0.954270\n",
      "mean IU  = 0.721870\n",
      "    class # 0 capture rate = 0.953885 \n",
      "    class # 1 capture rate = 0.962274 \n",
      "TRAIN: Batch: 0.3713548588851536 Loss_Total: 10.869348\n",
      "TRAIN: Batch: 0.3713548588851536 Loss_Seg: 9.171729\n",
      "accuracy = 0.951479\n",
      "mean IU  = 0.745187\n",
      "    class # 0 capture rate = 0.951161 \n",
      "    class # 1 capture rate = 0.956451 \n",
      "TRAIN: Batch: 0.37526385739973417 Loss_Total: 12.7056675\n",
      "TRAIN: Batch: 0.37526385739973417 Loss_Seg: 12.373242\n",
      "accuracy = 0.945178\n",
      "mean IU  = 0.720834\n",
      "    class # 0 capture rate = 0.947068 \n",
      "    class # 1 capture rate = 0.915476 \n",
      "TRAIN: Batch: 0.37917285591431477 Loss_Total: 13.315901\n",
      "TRAIN: Batch: 0.37917285591431477 Loss_Seg: 12.357373\n",
      "accuracy = 0.927917\n",
      "mean IU  = 0.661055\n",
      "    class # 0 capture rate = 0.927142 \n",
      "    class # 1 capture rate = 0.942484 \n",
      "TRAIN: Batch: 0.3830818544288953 Loss_Total: 12.112177\n",
      "TRAIN: Batch: 0.3830818544288953 Loss_Seg: 13.247866\n",
      "accuracy = 0.931556\n",
      "mean IU  = 0.700145\n",
      "    class # 0 capture rate = 0.931142 \n",
      "    class # 1 capture rate = 0.937453 \n",
      "TRAIN: Batch: 0.3869908529434759 Loss_Total: 14.230566\n",
      "TRAIN: Batch: 0.3869908529434759 Loss_Seg: 17.555252\n",
      "accuracy = 0.920265\n",
      "mean IU  = 0.702790\n",
      "    class # 0 capture rate = 0.920707 \n",
      "    class # 1 capture rate = 0.915460 \n",
      "TRAIN: Batch: 0.39089985145805645 Loss_Total: 10.12088\n",
      "TRAIN: Batch: 0.39089985145805645 Loss_Seg: 8.329786\n",
      "accuracy = 0.952484\n",
      "mean IU  = 0.742591\n",
      "    class # 0 capture rate = 0.951747 \n",
      "    class # 1 capture rate = 0.964731 \n",
      "TRAIN: Batch: 0.394808849972637 Loss_Total: 13.772188\n",
      "TRAIN: Batch: 0.394808849972637 Loss_Seg: 14.303157\n",
      "accuracy = 0.935879\n",
      "mean IU  = 0.680514\n",
      "    class # 0 capture rate = 0.936703 \n",
      "    class # 1 capture rate = 0.920917 \n",
      "TRAIN: Batch: 0.3987178484872176 Loss_Total: 8.071515\n",
      "TRAIN: Batch: 0.3987178484872176 Loss_Seg: 8.502833\n",
      "accuracy = 0.959393\n",
      "mean IU  = 0.766073\n",
      "    class # 0 capture rate = 0.959461 \n",
      "    class # 1 capture rate = 0.958280 \n",
      "TRAIN: Batch: 0.4026268470017981 Loss_Total: 12.234745\n",
      "TRAIN: Batch: 0.4026268470017981 Loss_Seg: 9.635089\n",
      "accuracy = 0.948015\n",
      "mean IU  = 0.708404\n",
      "    class # 0 capture rate = 0.947332 \n",
      "    class # 1 capture rate = 0.961499 \n",
      "TRAIN: Batch: 0.4065358455163787 Loss_Total: 11.838598\n",
      "TRAIN: Batch: 0.4065358455163787 Loss_Seg: 10.55688\n",
      "accuracy = 0.947262\n",
      "mean IU  = 0.711172\n",
      "    class # 0 capture rate = 0.947042 \n",
      "    class # 1 capture rate = 0.951372 \n",
      "TRAIN: Batch: 0.41044484403095927 Loss_Total: 16.179554\n",
      "TRAIN: Batch: 0.41044484403095927 Loss_Seg: 13.925107\n",
      "accuracy = 0.933048\n",
      "mean IU  = 0.725961\n",
      "    class # 0 capture rate = 0.932411 \n",
      "    class # 1 capture rate = 0.940535 \n",
      "TRAIN: Batch: 0.4143538425455398 Loss_Total: 13.54026\n",
      "TRAIN: Batch: 0.4143538425455398 Loss_Seg: 10.729903\n",
      "accuracy = 0.951873\n",
      "mean IU  = 0.707378\n",
      "    class # 0 capture rate = 0.953197 \n",
      "    class # 1 capture rate = 0.923948 \n",
      "TRAIN: Batch: 0.4182628410601204 Loss_Total: 11.284304\n",
      "TRAIN: Batch: 0.4182628410601204 Loss_Seg: 11.599276\n",
      "accuracy = 0.938353\n",
      "mean IU  = 0.718939\n",
      "    class # 0 capture rate = 0.937170 \n",
      "    class # 1 capture rate = 0.955241 \n",
      "TRAIN: Batch: 0.42217183957470095 Loss_Total: 11.656901\n",
      "TRAIN: Batch: 0.42217183957470095 Loss_Seg: 9.461278\n",
      "accuracy = 0.955805\n",
      "mean IU  = 0.735215\n",
      "    class # 0 capture rate = 0.956431 \n",
      "    class # 1 capture rate = 0.943930 \n",
      "TRAIN: Batch: 0.42608083808928154 Loss_Total: 15.068875\n",
      "TRAIN: Batch: 0.42608083808928154 Loss_Seg: 9.041491\n",
      "accuracy = 0.937155\n",
      "mean IU  = 0.645535\n",
      "    class # 0 capture rate = 0.935862 \n",
      "    class # 1 capture rate = 0.972014 \n",
      "TRAIN: Batch: 0.4299898366038621 Loss_Total: 13.955915\n",
      "TRAIN: Batch: 0.4299898366038621 Loss_Seg: 12.943782\n",
      "accuracy = 0.941278\n",
      "mean IU  = 0.717107\n",
      "    class # 0 capture rate = 0.942775 \n",
      "    class # 1 capture rate = 0.919024 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.4338988351184426 Loss_Total: 17.050606\n",
      "TRAIN: Batch: 0.4338988351184426 Loss_Seg: 19.220177\n",
      "accuracy = 0.925308\n",
      "mean IU  = 0.688766\n",
      "    class # 0 capture rate = 0.930231 \n",
      "    class # 1 capture rate = 0.862739 \n",
      "TRAIN: Batch: 0.4378078336330232 Loss_Total: 11.802921\n",
      "TRAIN: Batch: 0.4378078336330232 Loss_Seg: 8.581156\n",
      "accuracy = 0.965276\n",
      "mean IU  = 0.787477\n",
      "    class # 0 capture rate = 0.966838 \n",
      "    class # 1 capture rate = 0.939992 \n",
      "TRAIN: Batch: 0.44171683214760377 Loss_Total: 10.884459\n",
      "TRAIN: Batch: 0.44171683214760377 Loss_Seg: 8.479679\n",
      "accuracy = 0.967989\n",
      "mean IU  = 0.809464\n",
      "    class # 0 capture rate = 0.968353 \n",
      "    class # 1 capture rate = 0.962544 \n",
      "TRAIN: Batch: 0.44562583066218436 Loss_Total: 10.440328\n",
      "TRAIN: Batch: 0.44562583066218436 Loss_Seg: 9.660358\n",
      "accuracy = 0.951996\n",
      "mean IU  = 0.746257\n",
      "    class # 0 capture rate = 0.951650 \n",
      "    class # 1 capture rate = 0.957447 \n",
      "TRAIN: Batch: 0.4495348291767649 Loss_Total: 9.632459\n",
      "TRAIN: Batch: 0.4495348291767649 Loss_Seg: 9.837998\n",
      "accuracy = 0.959104\n",
      "mean IU  = 0.752983\n",
      "    class # 0 capture rate = 0.960441 \n",
      "    class # 1 capture rate = 0.935321 \n",
      "TRAIN: Batch: 0.4534438276913455 Loss_Total: 11.72624\n",
      "TRAIN: Batch: 0.4534438276913455 Loss_Seg: 13.733608\n",
      "accuracy = 0.949407\n",
      "mean IU  = 0.743591\n",
      "    class # 0 capture rate = 0.951983 \n",
      "    class # 1 capture rate = 0.912556 \n",
      "TRAIN: Batch: 0.45735282620592604 Loss_Total: 10.979702\n",
      "TRAIN: Batch: 0.45735282620592604 Loss_Seg: 9.123995\n",
      "accuracy = 0.950776\n",
      "mean IU  = 0.722384\n",
      "    class # 0 capture rate = 0.950240 \n",
      "    class # 1 capture rate = 0.960851 \n",
      "TRAIN: Batch: 0.4612618247205066 Loss_Total: 15.3213005\n",
      "TRAIN: Batch: 0.4612618247205066 Loss_Seg: 14.3245325\n",
      "accuracy = 0.932755\n",
      "mean IU  = 0.720901\n",
      "    class # 0 capture rate = 0.933912 \n",
      "    class # 1 capture rate = 0.918979 \n",
      "TRAIN: Batch: 0.4651708232350872 Loss_Total: 9.716412\n",
      "TRAIN: Batch: 0.4651708232350872 Loss_Seg: 10.058927\n",
      "accuracy = 0.944800\n",
      "mean IU  = 0.680121\n",
      "    class # 0 capture rate = 0.944709 \n",
      "    class # 1 capture rate = 0.946877 \n",
      "TRAIN: Batch: 0.4690798217496677 Loss_Total: 10.768568\n",
      "TRAIN: Batch: 0.4690798217496677 Loss_Seg: 9.980665\n",
      "accuracy = 0.955980\n",
      "mean IU  = 0.746234\n",
      "    class # 0 capture rate = 0.957388 \n",
      "    class # 1 capture rate = 0.931889 \n",
      "TRAIN: Batch: 0.4729888202642483 Loss_Total: 13.718001\n",
      "TRAIN: Batch: 0.4729888202642483 Loss_Seg: 11.6684675\n",
      "accuracy = 0.941174\n",
      "mean IU  = 0.752296\n",
      "    class # 0 capture rate = 0.940066 \n",
      "    class # 1 capture rate = 0.953698 \n",
      "TRAIN: Batch: 0.47689781877882886 Loss_Total: 13.65189\n",
      "TRAIN: Batch: 0.47689781877882886 Loss_Seg: 16.976452\n",
      "accuracy = 0.938486\n",
      "mean IU  = 0.734686\n",
      "    class # 0 capture rate = 0.943210 \n",
      "    class # 1 capture rate = 0.884323 \n",
      "TRAIN: Batch: 0.4808068172934094 Loss_Total: 10.543381\n",
      "TRAIN: Batch: 0.4808068172934094 Loss_Seg: 9.860119\n",
      "accuracy = 0.947696\n",
      "mean IU  = 0.726039\n",
      "    class # 0 capture rate = 0.948488 \n",
      "    class # 1 capture rate = 0.934746 \n",
      "TRAIN: Batch: 0.48471581580799 Loss_Total: 11.80266\n",
      "TRAIN: Batch: 0.48471581580799 Loss_Seg: 12.64872\n",
      "accuracy = 0.945254\n",
      "mean IU  = 0.708246\n",
      "    class # 0 capture rate = 0.946790 \n",
      "    class # 1 capture rate = 0.918219 \n",
      "TRAIN: Batch: 0.48862481432257054 Loss_Total: 11.09352\n",
      "TRAIN: Batch: 0.48862481432257054 Loss_Seg: 12.210758\n",
      "accuracy = 0.948340\n",
      "mean IU  = 0.725190\n",
      "    class # 0 capture rate = 0.949652 \n",
      "    class # 1 capture rate = 0.926578 \n",
      "TRAIN: Batch: 0.49253381283715114 Loss_Total: 12.688127\n",
      "TRAIN: Batch: 0.49253381283715114 Loss_Seg: 16.439129\n",
      "accuracy = 0.928594\n",
      "mean IU  = 0.728235\n",
      "    class # 0 capture rate = 0.928149 \n",
      "    class # 1 capture rate = 0.933222 \n",
      "TRAIN: Batch: 0.4964428113517317 Loss_Total: 10.648434\n",
      "TRAIN: Batch: 0.4964428113517317 Loss_Seg: 7.8067436\n",
      "accuracy = 0.961294\n",
      "mean IU  = 0.726452\n",
      "    class # 0 capture rate = 0.962270 \n",
      "    class # 1 capture rate = 0.937956 \n",
      "TRAIN: Batch: 0.5003518098663122 Loss_Total: 10.356347\n",
      "TRAIN: Batch: 0.5003518098663122 Loss_Seg: 9.501742\n",
      "accuracy = 0.959203\n",
      "mean IU  = 0.773547\n",
      "    class # 0 capture rate = 0.960616 \n",
      "    class # 1 capture rate = 0.938084 \n",
      "TRAIN: Batch: 0.5042608083808928 Loss_Total: 8.302615\n",
      "TRAIN: Batch: 0.5042608083808928 Loss_Seg: 8.9305105\n",
      "accuracy = 0.957438\n",
      "mean IU  = 0.746504\n",
      "    class # 0 capture rate = 0.957582 \n",
      "    class # 1 capture rate = 0.954802 \n",
      "TRAIN: Batch: 0.5081698068954734 Loss_Total: 11.41613\n",
      "TRAIN: Batch: 0.5081698068954734 Loss_Seg: 11.727249\n",
      "accuracy = 0.938594\n",
      "mean IU  = 0.701186\n",
      "    class # 0 capture rate = 0.938927 \n",
      "    class # 1 capture rate = 0.933164 \n",
      "TRAIN: Batch: 0.5120788054100539 Loss_Total: 9.03554\n",
      "TRAIN: Batch: 0.5120788054100539 Loss_Seg: 8.079747\n",
      "accuracy = 0.961371\n",
      "mean IU  = 0.788518\n",
      "    class # 0 capture rate = 0.960545 \n",
      "    class # 1 capture rate = 0.973404 \n",
      "TRAIN: Batch: 0.5159878039246345 Loss_Total: 11.724102\n",
      "TRAIN: Batch: 0.5159878039246345 Loss_Seg: 11.188896\n",
      "accuracy = 0.945599\n",
      "mean IU  = 0.723102\n",
      "    class # 0 capture rate = 0.945228 \n",
      "    class # 1 capture rate = 0.951620 \n",
      "TRAIN: Batch: 0.5198968024392151 Loss_Total: 10.038832\n",
      "TRAIN: Batch: 0.5198968024392151 Loss_Seg: 8.672041\n",
      "accuracy = 0.951830\n",
      "mean IU  = 0.721198\n",
      "    class # 0 capture rate = 0.951731 \n",
      "    class # 1 capture rate = 0.953754 \n",
      "TRAIN: Batch: 0.5238058009537956 Loss_Total: 12.72397\n",
      "TRAIN: Batch: 0.5238058009537956 Loss_Seg: 9.445161\n",
      "accuracy = 0.955327\n",
      "mean IU  = 0.741390\n",
      "    class # 0 capture rate = 0.955589 \n",
      "    class # 1 capture rate = 0.950646 \n",
      "TRAIN: Batch: 0.5277147994683762 Loss_Total: 14.051315\n",
      "TRAIN: Batch: 0.5277147994683762 Loss_Seg: 12.381406\n",
      "accuracy = 0.944674\n",
      "mean IU  = 0.736200\n",
      "    class # 0 capture rate = 0.945576 \n",
      "    class # 1 capture rate = 0.932173 \n",
      "TRAIN: Batch: 0.5316237979829568 Loss_Total: 9.338876\n",
      "TRAIN: Batch: 0.5316237979829568 Loss_Seg: 7.9918747\n",
      "accuracy = 0.961419\n",
      "mean IU  = 0.754046\n",
      "    class # 0 capture rate = 0.962530 \n",
      "    class # 1 capture rate = 0.940255 \n",
      "TRAIN: Batch: 0.5355327964975374 Loss_Total: 12.477353\n",
      "TRAIN: Batch: 0.5355327964975374 Loss_Seg: 11.614218\n",
      "accuracy = 0.934808\n",
      "mean IU  = 0.717359\n",
      "    class # 0 capture rate = 0.932538 \n",
      "    class # 1 capture rate = 0.965554 \n",
      "TRAIN: Batch: 0.5394417950121179 Loss_Total: 11.33657\n",
      "TRAIN: Batch: 0.5394417950121179 Loss_Seg: 8.880181\n",
      "accuracy = 0.954341\n",
      "mean IU  = 0.750255\n",
      "    class # 0 capture rate = 0.954197 \n",
      "    class # 1 capture rate = 0.956686 \n",
      "TRAIN: Batch: 0.5433507935266985 Loss_Total: 12.63426\n",
      "TRAIN: Batch: 0.5433507935266985 Loss_Seg: 12.751577\n",
      "accuracy = 0.932117\n",
      "mean IU  = 0.691051\n",
      "    class # 0 capture rate = 0.932084 \n",
      "    class # 1 capture rate = 0.932634 \n",
      "TRAIN: Batch: 0.5472597920412791 Loss_Total: 11.565289\n",
      "TRAIN: Batch: 0.5472597920412791 Loss_Seg: 10.917161\n",
      "accuracy = 0.950878\n",
      "mean IU  = 0.755227\n",
      "    class # 0 capture rate = 0.950484 \n",
      "    class # 1 capture rate = 0.956441 \n",
      "TRAIN: Batch: 0.5511687905558595 Loss_Total: 11.624342\n",
      "TRAIN: Batch: 0.5511687905558595 Loss_Seg: 9.003612\n",
      "accuracy = 0.956492\n",
      "mean IU  = 0.744163\n",
      "    class # 0 capture rate = 0.956913 \n",
      "    class # 1 capture rate = 0.948908 \n",
      "TRAIN: Batch: 0.5550777890704401 Loss_Total: 11.806256\n",
      "TRAIN: Batch: 0.5550777890704401 Loss_Seg: 11.387447\n",
      "accuracy = 0.946159\n",
      "mean IU  = 0.726105\n",
      "    class # 0 capture rate = 0.945909 \n",
      "    class # 1 capture rate = 0.950166 \n",
      "TRAIN: Batch: 0.5589867875850207 Loss_Total: 13.121741\n",
      "TRAIN: Batch: 0.5589867875850207 Loss_Seg: 10.08239\n",
      "accuracy = 0.941248\n",
      "mean IU  = 0.715472\n",
      "    class # 0 capture rate = 0.939656 \n",
      "    class # 1 capture rate = 0.966570 \n",
      "TRAIN: Batch: 0.5628957860996013 Loss_Total: 13.59901\n",
      "TRAIN: Batch: 0.5628957860996013 Loss_Seg: 10.035345\n",
      "accuracy = 0.947140\n",
      "mean IU  = 0.726407\n",
      "    class # 0 capture rate = 0.946278 \n",
      "    class # 1 capture rate = 0.961407 \n",
      "TRAIN: Batch: 0.5668047846141818 Loss_Total: 12.326881\n",
      "TRAIN: Batch: 0.5668047846141818 Loss_Seg: 12.659276\n",
      "accuracy = 0.929714\n",
      "mean IU  = 0.687417\n",
      "    class # 0 capture rate = 0.929071 \n",
      "    class # 1 capture rate = 0.939600 \n",
      "TRAIN: Batch: 0.5707137831287624 Loss_Total: 11.08593\n",
      "TRAIN: Batch: 0.5707137831287624 Loss_Seg: 11.060203\n",
      "accuracy = 0.948360\n",
      "mean IU  = 0.735084\n",
      "    class # 0 capture rate = 0.948527 \n",
      "    class # 1 capture rate = 0.945771 \n",
      "TRAIN: Batch: 0.574622781643343 Loss_Total: 11.093637\n",
      "TRAIN: Batch: 0.574622781643343 Loss_Seg: 11.2990675\n",
      "accuracy = 0.957494\n",
      "mean IU  = 0.747524\n",
      "    class # 0 capture rate = 0.960223 \n",
      "    class # 1 capture rate = 0.910388 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.5785317801579235 Loss_Total: 10.453811\n",
      "TRAIN: Batch: 0.5785317801579235 Loss_Seg: 8.932524\n",
      "accuracy = 0.954952\n",
      "mean IU  = 0.752190\n",
      "    class # 0 capture rate = 0.954945 \n",
      "    class # 1 capture rate = 0.955061 \n",
      "TRAIN: Batch: 0.5824407786725041 Loss_Total: 13.647093\n",
      "TRAIN: Batch: 0.5824407786725041 Loss_Seg: 13.200556\n",
      "accuracy = 0.922867\n",
      "mean IU  = 0.682157\n",
      "    class # 0 capture rate = 0.920407 \n",
      "    class # 1 capture rate = 0.958271 \n",
      "TRAIN: Batch: 0.5863497771870847 Loss_Total: 10.579459\n",
      "TRAIN: Batch: 0.5863497771870847 Loss_Seg: 9.381157\n",
      "accuracy = 0.953114\n",
      "mean IU  = 0.722335\n",
      "    class # 0 capture rate = 0.952646 \n",
      "    class # 1 capture rate = 0.962487 \n",
      "TRAIN: Batch: 0.5902587757016652 Loss_Total: 12.772366\n",
      "TRAIN: Batch: 0.5902587757016652 Loss_Seg: 11.319069\n",
      "accuracy = 0.938359\n",
      "mean IU  = 0.709885\n",
      "    class # 0 capture rate = 0.937645 \n",
      "    class # 1 capture rate = 0.949322 \n",
      "TRAIN: Batch: 0.5941677742162458 Loss_Total: 10.276741\n",
      "TRAIN: Batch: 0.5941677742162458 Loss_Seg: 9.789205\n",
      "accuracy = 0.956561\n",
      "mean IU  = 0.757483\n",
      "    class # 0 capture rate = 0.956853 \n",
      "    class # 1 capture rate = 0.951843 \n",
      "TRAIN: Batch: 0.5980767727308264 Loss_Total: 11.70713\n",
      "TRAIN: Batch: 0.5980767727308264 Loss_Seg: 8.515713\n",
      "accuracy = 0.959492\n",
      "mean IU  = 0.753408\n",
      "    class # 0 capture rate = 0.959482 \n",
      "    class # 1 capture rate = 0.959677 \n",
      "TRAIN: Batch: 0.601985771245407 Loss_Total: 15.808058\n",
      "TRAIN: Batch: 0.601985771245407 Loss_Seg: 15.331435\n",
      "accuracy = 0.916174\n",
      "mean IU  = 0.675915\n",
      "    class # 0 capture rate = 0.914816 \n",
      "    class # 1 capture rate = 0.933953 \n",
      "TRAIN: Batch: 0.6058947697599875 Loss_Total: 12.356899\n",
      "TRAIN: Batch: 0.6058947697599875 Loss_Seg: 12.993974\n",
      "accuracy = 0.935957\n",
      "mean IU  = 0.708463\n",
      "    class # 0 capture rate = 0.936224 \n",
      "    class # 1 capture rate = 0.932101 \n",
      "TRAIN: Batch: 0.6098037682745681 Loss_Total: 11.579483\n",
      "TRAIN: Batch: 0.6098037682745681 Loss_Seg: 12.786995\n",
      "accuracy = 0.942696\n",
      "mean IU  = 0.709665\n",
      "    class # 0 capture rate = 0.944147 \n",
      "    class # 1 capture rate = 0.918930 \n",
      "TRAIN: Batch: 0.6137127667891487 Loss_Total: 13.319386\n",
      "TRAIN: Batch: 0.6137127667891487 Loss_Seg: 13.233194\n",
      "accuracy = 0.930683\n",
      "mean IU  = 0.709576\n",
      "    class # 0 capture rate = 0.929484 \n",
      "    class # 1 capture rate = 0.946288 \n",
      "TRAIN: Batch: 0.6176217653037291 Loss_Total: 12.410034\n",
      "TRAIN: Batch: 0.6176217653037291 Loss_Seg: 11.935845\n",
      "accuracy = 0.939875\n",
      "mean IU  = 0.691713\n",
      "    class # 0 capture rate = 0.942128 \n",
      "    class # 1 capture rate = 0.900317 \n",
      "TRAIN: Batch: 0.6215307638183097 Loss_Total: 12.191255\n",
      "TRAIN: Batch: 0.6215307638183097 Loss_Seg: 11.194901\n",
      "accuracy = 0.954290\n",
      "mean IU  = 0.752174\n",
      "    class # 0 capture rate = 0.955857 \n",
      "    class # 1 capture rate = 0.930064 \n",
      "TRAIN: Batch: 0.6254397623328903 Loss_Total: 17.901947\n",
      "TRAIN: Batch: 0.6254397623328903 Loss_Seg: 17.01059\n",
      "accuracy = 0.924601\n",
      "mean IU  = 0.684065\n",
      "    class # 0 capture rate = 0.927131 \n",
      "    class # 1 capture rate = 0.890387 \n",
      "TRAIN: Batch: 0.6293487608474708 Loss_Total: 8.241538\n",
      "TRAIN: Batch: 0.6293487608474708 Loss_Seg: 8.09585\n",
      "accuracy = 0.964599\n",
      "mean IU  = 0.767868\n",
      "    class # 0 capture rate = 0.965507 \n",
      "    class # 1 capture rate = 0.947394 \n",
      "TRAIN: Batch: 0.6332577593620514 Loss_Total: 9.885809\n",
      "TRAIN: Batch: 0.6332577593620514 Loss_Seg: 9.674249\n",
      "accuracy = 0.943135\n",
      "mean IU  = 0.719483\n",
      "    class # 0 capture rate = 0.942108 \n",
      "    class # 1 capture rate = 0.959504 \n",
      "TRAIN: Batch: 0.637166757876632 Loss_Total: 10.404778\n",
      "TRAIN: Batch: 0.637166757876632 Loss_Seg: 8.436407\n",
      "accuracy = 0.963133\n",
      "mean IU  = 0.800678\n",
      "    class # 0 capture rate = 0.963307 \n",
      "    class # 1 capture rate = 0.960770 \n",
      "TRAIN: Batch: 0.6410757563912126 Loss_Total: 9.839036\n",
      "TRAIN: Batch: 0.6410757563912126 Loss_Seg: 7.341468\n",
      "accuracy = 0.967625\n",
      "mean IU  = 0.797469\n",
      "    class # 0 capture rate = 0.967743 \n",
      "    class # 1 capture rate = 0.965665 \n",
      "TRAIN: Batch: 0.6449847549057931 Loss_Total: 9.872589\n",
      "TRAIN: Batch: 0.6449847549057931 Loss_Seg: 7.9086695\n",
      "accuracy = 0.958776\n",
      "mean IU  = 0.750763\n",
      "    class # 0 capture rate = 0.958902 \n",
      "    class # 1 capture rate = 0.956460 \n",
      "TRAIN: Batch: 0.6488937534203737 Loss_Total: 10.320824\n",
      "TRAIN: Batch: 0.6488937534203737 Loss_Seg: 11.309886\n",
      "accuracy = 0.947141\n",
      "mean IU  = 0.734539\n",
      "    class # 0 capture rate = 0.947026 \n",
      "    class # 1 capture rate = 0.948891 \n",
      "TRAIN: Batch: 0.6528027519349543 Loss_Total: 14.22542\n",
      "TRAIN: Batch: 0.6528027519349543 Loss_Seg: 14.154265\n",
      "accuracy = 0.937944\n",
      "mean IU  = 0.728818\n",
      "    class # 0 capture rate = 0.938680 \n",
      "    class # 1 capture rate = 0.928690 \n",
      "TRAIN: Batch: 0.6567117504495348 Loss_Total: 11.597293\n",
      "TRAIN: Batch: 0.6567117504495348 Loss_Seg: 7.6666846\n",
      "accuracy = 0.964740\n",
      "mean IU  = 0.783289\n",
      "    class # 0 capture rate = 0.964979 \n",
      "    class # 1 capture rate = 0.960716 \n",
      "TRAIN: Batch: 0.6606207489641154 Loss_Total: 14.421766\n",
      "TRAIN: Batch: 0.6606207489641154 Loss_Seg: 12.359081\n",
      "accuracy = 0.939457\n",
      "mean IU  = 0.726799\n",
      "    class # 0 capture rate = 0.939310 \n",
      "    class # 1 capture rate = 0.941426 \n",
      "TRAIN: Batch: 0.664529747478696 Loss_Total: 14.957927\n",
      "TRAIN: Batch: 0.664529747478696 Loss_Seg: 9.857159\n",
      "accuracy = 0.950508\n",
      "mean IU  = 0.742821\n",
      "    class # 0 capture rate = 0.949718 \n",
      "    class # 1 capture rate = 0.962903 \n",
      "TRAIN: Batch: 0.6684387459932766 Loss_Total: 13.307678\n",
      "TRAIN: Batch: 0.6684387459932766 Loss_Seg: 11.816944\n",
      "accuracy = 0.939101\n",
      "mean IU  = 0.720881\n",
      "    class # 0 capture rate = 0.938287 \n",
      "    class # 1 capture rate = 0.950649 \n",
      "TRAIN: Batch: 0.6723477445078571 Loss_Total: 16.905197\n",
      "TRAIN: Batch: 0.6723477445078571 Loss_Seg: 24.599577\n",
      "accuracy = 0.889220\n",
      "mean IU  = 0.679277\n",
      "    class # 0 capture rate = 0.884604 \n",
      "    class # 1 capture rate = 0.926064 \n",
      "TRAIN: Batch: 0.6762567430224377 Loss_Total: 13.495919\n",
      "TRAIN: Batch: 0.6762567430224377 Loss_Seg: 11.830756\n",
      "accuracy = 0.943944\n",
      "mean IU  = 0.716103\n",
      "    class # 0 capture rate = 0.944145 \n",
      "    class # 1 capture rate = 0.940653 \n",
      "TRAIN: Batch: 0.6801657415370183 Loss_Total: 10.76428\n",
      "TRAIN: Batch: 0.6801657415370183 Loss_Seg: 10.006358\n",
      "accuracy = 0.955342\n",
      "mean IU  = 0.764148\n",
      "    class # 0 capture rate = 0.955565 \n",
      "    class # 1 capture rate = 0.952071 \n",
      "TRAIN: Batch: 0.6840747400515987 Loss_Total: 12.290443\n",
      "TRAIN: Batch: 0.6840747400515987 Loss_Seg: 11.1018505\n",
      "accuracy = 0.934297\n",
      "mean IU  = 0.676633\n",
      "    class # 0 capture rate = 0.933544 \n",
      "    class # 1 capture rate = 0.948409 \n",
      "TRAIN: Batch: 0.6879837385661793 Loss_Total: 10.697294\n",
      "TRAIN: Batch: 0.6879837385661793 Loss_Seg: 10.802521\n",
      "accuracy = 0.943005\n",
      "mean IU  = 0.706651\n",
      "    class # 0 capture rate = 0.944791 \n",
      "    class # 1 capture rate = 0.912968 \n",
      "TRAIN: Batch: 0.6918927370807599 Loss_Total: 11.13729\n",
      "TRAIN: Batch: 0.6918927370807599 Loss_Seg: 10.901448\n",
      "accuracy = 0.947332\n",
      "mean IU  = 0.726199\n",
      "    class # 0 capture rate = 0.948023 \n",
      "    class # 1 capture rate = 0.936134 \n",
      "TRAIN: Batch: 0.6958017355953404 Loss_Total: 10.766865\n",
      "TRAIN: Batch: 0.6958017355953404 Loss_Seg: 7.488665\n",
      "accuracy = 0.957420\n",
      "mean IU  = 0.721492\n",
      "    class # 0 capture rate = 0.957402 \n",
      "    class # 1 capture rate = 0.957828 \n",
      "TRAIN: Batch: 0.699710734109921 Loss_Total: 10.564427\n",
      "TRAIN: Batch: 0.699710734109921 Loss_Seg: 8.907659\n",
      "accuracy = 0.961469\n",
      "mean IU  = 0.777934\n",
      "    class # 0 capture rate = 0.962704 \n",
      "    class # 1 capture rate = 0.942292 \n",
      "TRAIN: Batch: 0.7036197326245016 Loss_Total: 8.283497\n",
      "TRAIN: Batch: 0.7036197326245016 Loss_Seg: 7.289885\n",
      "accuracy = 0.961422\n",
      "mean IU  = 0.774173\n",
      "    class # 0 capture rate = 0.961125 \n",
      "    class # 1 capture rate = 0.966311 \n",
      "TRAIN: Batch: 0.7075287311390822 Loss_Total: 10.969129\n",
      "TRAIN: Batch: 0.7075287311390822 Loss_Seg: 9.430826\n",
      "accuracy = 0.945017\n",
      "mean IU  = 0.727824\n",
      "    class # 0 capture rate = 0.943677 \n",
      "    class # 1 capture rate = 0.965937 \n",
      "TRAIN: Batch: 0.7114377296536627 Loss_Total: 13.782094\n",
      "TRAIN: Batch: 0.7114377296536627 Loss_Seg: 13.535012\n",
      "accuracy = 0.943768\n",
      "mean IU  = 0.725444\n",
      "    class # 0 capture rate = 0.945758 \n",
      "    class # 1 capture rate = 0.914732 \n",
      "TRAIN: Batch: 0.7153467281682433 Loss_Total: 13.400761\n",
      "TRAIN: Batch: 0.7153467281682433 Loss_Seg: 13.4815035\n",
      "accuracy = 0.940066\n",
      "mean IU  = 0.704015\n",
      "    class # 0 capture rate = 0.943548 \n",
      "    class # 1 capture rate = 0.885830 \n",
      "TRAIN: Batch: 0.7192557266828239 Loss_Total: 14.21209\n",
      "TRAIN: Batch: 0.7192557266828239 Loss_Seg: 14.96698\n",
      "accuracy = 0.934076\n",
      "mean IU  = 0.708855\n",
      "    class # 0 capture rate = 0.937077 \n",
      "    class # 1 capture rate = 0.894371 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.7231647251974044 Loss_Total: 11.171795\n",
      "TRAIN: Batch: 0.7231647251974044 Loss_Seg: 11.25366\n",
      "accuracy = 0.944255\n",
      "mean IU  = 0.710007\n",
      "    class # 0 capture rate = 0.945113 \n",
      "    class # 1 capture rate = 0.929521 \n",
      "TRAIN: Batch: 0.727073723711985 Loss_Total: 9.951239\n",
      "TRAIN: Batch: 0.727073723711985 Loss_Seg: 10.1734085\n",
      "accuracy = 0.943915\n",
      "mean IU  = 0.703729\n",
      "    class # 0 capture rate = 0.943543 \n",
      "    class # 1 capture rate = 0.950758 \n",
      "TRAIN: Batch: 0.7309827222265656 Loss_Total: 11.130217\n",
      "TRAIN: Batch: 0.7309827222265656 Loss_Seg: 10.069621\n",
      "accuracy = 0.947065\n",
      "mean IU  = 0.710045\n",
      "    class # 0 capture rate = 0.947237 \n",
      "    class # 1 capture rate = 0.943858 \n",
      "TRAIN: Batch: 0.7348917207411462 Loss_Total: 11.444239\n",
      "TRAIN: Batch: 0.7348917207411462 Loss_Seg: 9.958191\n",
      "accuracy = 0.942476\n",
      "mean IU  = 0.695031\n",
      "    class # 0 capture rate = 0.941560 \n",
      "    class # 1 capture rate = 0.960219 \n",
      "TRAIN: Batch: 0.7388007192557267 Loss_Total: 10.883622\n",
      "TRAIN: Batch: 0.7388007192557267 Loss_Seg: 7.170366\n",
      "accuracy = 0.965058\n",
      "mean IU  = 0.781257\n",
      "    class # 0 capture rate = 0.964721 \n",
      "    class # 1 capture rate = 0.970975 \n",
      "TRAIN: Batch: 0.7427097177703073 Loss_Total: 15.038674\n",
      "TRAIN: Batch: 0.7427097177703073 Loss_Seg: 18.060928\n",
      "accuracy = 0.918728\n",
      "mean IU  = 0.697754\n",
      "    class # 0 capture rate = 0.921413 \n",
      "    class # 1 capture rate = 0.890000 \n",
      "TRAIN: Batch: 0.7466187162848879 Loss_Total: 12.913187\n",
      "TRAIN: Batch: 0.7466187162848879 Loss_Seg: 15.940219\n",
      "accuracy = 0.923676\n",
      "mean IU  = 0.671197\n",
      "    class # 0 capture rate = 0.924167 \n",
      "    class # 1 capture rate = 0.916134 \n",
      "TRAIN: Batch: 0.7505277147994683 Loss_Total: 10.918607\n",
      "TRAIN: Batch: 0.7505277147994683 Loss_Seg: 10.513171\n",
      "accuracy = 0.949701\n",
      "mean IU  = 0.745814\n",
      "    class # 0 capture rate = 0.950301 \n",
      "    class # 1 capture rate = 0.940930 \n",
      "TRAIN: Batch: 0.7544367133140489 Loss_Total: 12.433162\n",
      "TRAIN: Batch: 0.7544367133140489 Loss_Seg: 12.009109\n",
      "accuracy = 0.931434\n",
      "mean IU  = 0.663378\n",
      "    class # 0 capture rate = 0.930717 \n",
      "    class # 1 capture rate = 0.945630 \n",
      "TRAIN: Batch: 0.7583457118286295 Loss_Total: 7.75002\n",
      "TRAIN: Batch: 0.7583457118286295 Loss_Seg: 7.111999\n",
      "accuracy = 0.961398\n",
      "mean IU  = 0.759630\n",
      "    class # 0 capture rate = 0.961255 \n",
      "    class # 1 capture rate = 0.964046 \n",
      "TRAIN: Batch: 0.76225471034321 Loss_Total: 10.771553\n",
      "TRAIN: Batch: 0.76225471034321 Loss_Seg: 9.378262\n",
      "accuracy = 0.953704\n",
      "mean IU  = 0.748851\n",
      "    class # 0 capture rate = 0.953382 \n",
      "    class # 1 capture rate = 0.958910 \n",
      "TRAIN: Batch: 0.7661637088577906 Loss_Total: 11.826535\n",
      "TRAIN: Batch: 0.7661637088577906 Loss_Seg: 8.721135\n",
      "accuracy = 0.959786\n",
      "mean IU  = 0.756443\n",
      "    class # 0 capture rate = 0.960178 \n",
      "    class # 1 capture rate = 0.952734 \n",
      "TRAIN: Batch: 0.7700727073723712 Loss_Total: 12.628979\n",
      "TRAIN: Batch: 0.7700727073723712 Loss_Seg: 11.973412\n",
      "accuracy = 0.943264\n",
      "mean IU  = 0.766869\n",
      "    class # 0 capture rate = 0.942334 \n",
      "    class # 1 capture rate = 0.952938 \n",
      "TRAIN: Batch: 0.7739817058869518 Loss_Total: 9.990727\n",
      "TRAIN: Batch: 0.7739817058869518 Loss_Seg: 10.325421\n",
      "accuracy = 0.949268\n",
      "mean IU  = 0.753945\n",
      "    class # 0 capture rate = 0.948085 \n",
      "    class # 1 capture rate = 0.965617 \n",
      "TRAIN: Batch: 0.7778907044015323 Loss_Total: 10.2751045\n",
      "TRAIN: Batch: 0.7778907044015323 Loss_Seg: 7.705639\n",
      "accuracy = 0.955144\n",
      "mean IU  = 0.725367\n",
      "    class # 0 capture rate = 0.954399 \n",
      "    class # 1 capture rate = 0.970667 \n",
      "TRAIN: Batch: 0.7817997029161129 Loss_Total: 13.77656\n",
      "TRAIN: Batch: 0.7817997029161129 Loss_Seg: 15.348911\n",
      "accuracy = 0.930387\n",
      "mean IU  = 0.710296\n",
      "    class # 0 capture rate = 0.932886 \n",
      "    class # 1 capture rate = 0.899968 \n",
      "TRAIN: Batch: 0.7857087014306935 Loss_Total: 11.690802\n",
      "TRAIN: Batch: 0.7857087014306935 Loss_Seg: 7.985111\n",
      "accuracy = 0.961419\n",
      "mean IU  = 0.781803\n",
      "    class # 0 capture rate = 0.961470 \n",
      "    class # 1 capture rate = 0.960636 \n",
      "TRAIN: Batch: 0.789617699945274 Loss_Total: 12.309126\n",
      "TRAIN: Batch: 0.789617699945274 Loss_Seg: 11.503724\n",
      "accuracy = 0.949526\n",
      "mean IU  = 0.733839\n",
      "    class # 0 capture rate = 0.950473 \n",
      "    class # 1 capture rate = 0.934358 \n",
      "TRAIN: Batch: 0.7935266984598546 Loss_Total: 12.298559\n",
      "TRAIN: Batch: 0.7935266984598546 Loss_Seg: 14.727185\n",
      "accuracy = 0.937118\n",
      "mean IU  = 0.710840\n",
      "    class # 0 capture rate = 0.940257 \n",
      "    class # 1 capture rate = 0.893571 \n",
      "TRAIN: Batch: 0.7974356969744352 Loss_Total: 11.639756\n",
      "TRAIN: Batch: 0.7974356969744352 Loss_Seg: 11.930369\n",
      "accuracy = 0.952583\n",
      "mean IU  = 0.780483\n",
      "    class # 0 capture rate = 0.952719 \n",
      "    class # 1 capture rate = 0.950993 \n",
      "TRAIN: Batch: 0.8013446954890158 Loss_Total: 9.900508\n",
      "TRAIN: Batch: 0.8013446954890158 Loss_Seg: 7.880633\n",
      "accuracy = 0.963842\n",
      "mean IU  = 0.769292\n",
      "    class # 0 capture rate = 0.965291 \n",
      "    class # 1 capture rate = 0.937703 \n",
      "TRAIN: Batch: 0.8052536940035963 Loss_Total: 11.162982\n",
      "TRAIN: Batch: 0.8052536940035963 Loss_Seg: 8.898149\n",
      "accuracy = 0.952727\n",
      "mean IU  = 0.743906\n",
      "    class # 0 capture rate = 0.952856 \n",
      "    class # 1 capture rate = 0.950633 \n",
      "TRAIN: Batch: 0.8091626925181769 Loss_Total: 9.693373\n",
      "TRAIN: Batch: 0.8091626925181769 Loss_Seg: 11.796038\n",
      "accuracy = 0.945311\n",
      "mean IU  = 0.743417\n",
      "    class # 0 capture rate = 0.945126 \n",
      "    class # 1 capture rate = 0.947795 \n",
      "TRAIN: Batch: 0.8130716910327574 Loss_Total: 14.273113\n",
      "TRAIN: Batch: 0.8130716910327574 Loss_Seg: 14.031331\n",
      "accuracy = 0.938867\n",
      "mean IU  = 0.714914\n",
      "    class # 0 capture rate = 0.941156 \n",
      "    class # 1 capture rate = 0.906504 \n",
      "TRAIN: Batch: 0.8169806895473379 Loss_Total: 12.364125\n",
      "TRAIN: Batch: 0.8169806895473379 Loss_Seg: 11.303088\n",
      "accuracy = 0.939173\n",
      "mean IU  = 0.711183\n",
      "    class # 0 capture rate = 0.938535 \n",
      "    class # 1 capture rate = 0.949016 \n",
      "TRAIN: Batch: 0.8208896880619185 Loss_Total: 10.093011\n",
      "TRAIN: Batch: 0.8208896880619185 Loss_Seg: 11.099692\n",
      "accuracy = 0.935882\n",
      "mean IU  = 0.692715\n",
      "    class # 0 capture rate = 0.934598 \n",
      "    class # 1 capture rate = 0.957746 \n",
      "TRAIN: Batch: 0.8247986865764991 Loss_Total: 12.272799\n",
      "TRAIN: Batch: 0.8247986865764991 Loss_Seg: 12.112244\n",
      "accuracy = 0.938078\n",
      "mean IU  = 0.709916\n",
      "    class # 0 capture rate = 0.938200 \n",
      "    class # 1 capture rate = 0.936253 \n",
      "TRAIN: Batch: 0.8287076850910796 Loss_Total: 12.969015\n",
      "TRAIN: Batch: 0.8287076850910796 Loss_Seg: 8.555723\n",
      "accuracy = 0.957295\n",
      "mean IU  = 0.742800\n",
      "    class # 0 capture rate = 0.957340 \n",
      "    class # 1 capture rate = 0.956438 \n",
      "TRAIN: Batch: 0.8326166836056602 Loss_Total: 8.261715\n",
      "TRAIN: Batch: 0.8326166836056602 Loss_Seg: 6.150524\n",
      "accuracy = 0.963276\n",
      "mean IU  = 0.758532\n",
      "    class # 0 capture rate = 0.962519 \n",
      "    class # 1 capture rate = 0.978657 \n",
      "TRAIN: Batch: 0.8365256821202408 Loss_Total: 11.919076\n",
      "TRAIN: Batch: 0.8365256821202408 Loss_Seg: 9.827188\n",
      "accuracy = 0.948355\n",
      "mean IU  = 0.728531\n",
      "    class # 0 capture rate = 0.947456 \n",
      "    class # 1 capture rate = 0.963478 \n",
      "TRAIN: Batch: 0.8404346806348214 Loss_Total: 9.367083\n",
      "TRAIN: Batch: 0.8404346806348214 Loss_Seg: 9.877104\n",
      "accuracy = 0.950095\n",
      "mean IU  = 0.750016\n",
      "    class # 0 capture rate = 0.949650 \n",
      "    class # 1 capture rate = 0.956538 \n",
      "TRAIN: Batch: 0.8443436791494019 Loss_Total: 13.401995\n",
      "TRAIN: Batch: 0.8443436791494019 Loss_Seg: 15.685456\n",
      "accuracy = 0.932256\n",
      "mean IU  = 0.718464\n",
      "    class # 0 capture rate = 0.932067 \n",
      "    class # 1 capture rate = 0.934564 \n",
      "TRAIN: Batch: 0.8482526776639825 Loss_Total: 10.792816\n",
      "TRAIN: Batch: 0.8482526776639825 Loss_Seg: 8.891651\n",
      "accuracy = 0.949559\n",
      "mean IU  = 0.671871\n",
      "    class # 0 capture rate = 0.949729 \n",
      "    class # 1 capture rate = 0.944871 \n",
      "TRAIN: Batch: 0.8521616761785631 Loss_Total: 11.655422\n",
      "TRAIN: Batch: 0.8521616761785631 Loss_Seg: 13.529017\n",
      "accuracy = 0.943774\n",
      "mean IU  = 0.688860\n",
      "    class # 0 capture rate = 0.946505 \n",
      "    class # 1 capture rate = 0.890664 \n",
      "TRAIN: Batch: 0.8560706746931436 Loss_Total: 11.966818\n",
      "TRAIN: Batch: 0.8560706746931436 Loss_Seg: 11.789461\n",
      "accuracy = 0.943845\n",
      "mean IU  = 0.708983\n",
      "    class # 0 capture rate = 0.946168 \n",
      "    class # 1 capture rate = 0.905132 \n",
      "TRAIN: Batch: 0.8599796732077242 Loss_Total: 12.431667\n",
      "TRAIN: Batch: 0.8599796732077242 Loss_Seg: 11.016471\n",
      "accuracy = 0.944846\n",
      "mean IU  = 0.717157\n",
      "    class # 0 capture rate = 0.945213 \n",
      "    class # 1 capture rate = 0.938784 \n",
      "TRAIN: Batch: 0.8638886717223048 Loss_Total: 9.83494\n",
      "TRAIN: Batch: 0.8638886717223048 Loss_Seg: 8.529319\n",
      "accuracy = 0.965246\n",
      "mean IU  = 0.780405\n",
      "    class # 0 capture rate = 0.966267 \n",
      "    class # 1 capture rate = 0.947510 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Batch: 0.8677976702368853 Loss_Total: 9.772565\n",
      "TRAIN: Batch: 0.8677976702368853 Loss_Seg: 8.59499\n",
      "accuracy = 0.962034\n",
      "mean IU  = 0.763151\n",
      "    class # 0 capture rate = 0.962552 \n",
      "    class # 1 capture rate = 0.952604 \n",
      "TRAIN: Batch: 0.8717066687514659 Loss_Total: 11.225641\n",
      "TRAIN: Batch: 0.8717066687514659 Loss_Seg: 14.305492\n",
      "accuracy = 0.927352\n",
      "mean IU  = 0.723807\n",
      "    class # 0 capture rate = 0.926428 \n",
      "    class # 1 capture rate = 0.937146 \n",
      "TRAIN: Batch: 0.8756156672660464 Loss_Total: 13.937675\n",
      "TRAIN: Batch: 0.8756156672660464 Loss_Seg: 11.594869\n",
      "accuracy = 0.945452\n",
      "mean IU  = 0.737764\n",
      "    class # 0 capture rate = 0.945662 \n",
      "    class # 1 capture rate = 0.942499 \n",
      "TRAIN: Batch: 0.879524665780627 Loss_Total: 16.07704\n",
      "TRAIN: Batch: 0.879524665780627 Loss_Seg: 18.043686\n",
      "accuracy = 0.939412\n",
      "mean IU  = 0.731444\n",
      "    class # 0 capture rate = 0.946339 \n",
      "    class # 1 capture rate = 0.858558 \n",
      "TRAIN: Batch: 0.8834336642952075 Loss_Total: 8.252621\n",
      "TRAIN: Batch: 0.8834336642952075 Loss_Seg: 9.23921\n",
      "accuracy = 0.954903\n",
      "mean IU  = 0.767003\n",
      "    class # 0 capture rate = 0.955215 \n",
      "    class # 1 capture rate = 0.950499 \n",
      "TRAIN: Batch: 0.8873426628097881 Loss_Total: 10.886532\n",
      "TRAIN: Batch: 0.8873426628097881 Loss_Seg: 10.074407\n",
      "accuracy = 0.950778\n",
      "mean IU  = 0.735796\n",
      "    class # 0 capture rate = 0.950834 \n",
      "    class # 1 capture rate = 0.949850 \n",
      "TRAIN: Batch: 0.8912516613243687 Loss_Total: 10.06192\n",
      "TRAIN: Batch: 0.8912516613243687 Loss_Seg: 8.9931965\n",
      "accuracy = 0.958126\n",
      "mean IU  = 0.761524\n",
      "    class # 0 capture rate = 0.958690 \n",
      "    class # 1 capture rate = 0.948945 \n",
      "TRAIN: Batch: 0.8951606598389492 Loss_Total: 12.678375\n",
      "TRAIN: Batch: 0.8951606598389492 Loss_Seg: 9.148544\n",
      "accuracy = 0.951012\n",
      "mean IU  = 0.745877\n",
      "    class # 0 capture rate = 0.950242 \n",
      "    class # 1 capture rate = 0.962948 \n",
      "TRAIN: Batch: 0.8990696583535298 Loss_Total: 14.216906\n",
      "TRAIN: Batch: 0.8990696583535298 Loss_Seg: 14.066687\n",
      "accuracy = 0.930633\n",
      "mean IU  = 0.710604\n",
      "    class # 0 capture rate = 0.929797 \n",
      "    class # 1 capture rate = 0.941335 \n",
      "TRAIN: Batch: 0.9029786568681104 Loss_Total: 13.956144\n",
      "TRAIN: Batch: 0.9029786568681104 Loss_Seg: 9.200839\n",
      "accuracy = 0.958100\n",
      "mean IU  = 0.771715\n",
      "    class # 0 capture rate = 0.958872 \n",
      "    class # 1 capture rate = 0.946656 \n",
      "TRAIN: Batch: 0.906887655382691 Loss_Total: 18.857197\n",
      "TRAIN: Batch: 0.906887655382691 Loss_Seg: 10.748555\n",
      "accuracy = 0.940647\n",
      "mean IU  = 0.710830\n",
      "    class # 0 capture rate = 0.940081 \n",
      "    class # 1 capture rate = 0.949709 \n",
      "TRAIN: Batch: 0.9107966538972715 Loss_Total: 13.849276\n",
      "TRAIN: Batch: 0.9107966538972715 Loss_Seg: 11.570689\n",
      "accuracy = 0.945772\n",
      "mean IU  = 0.716978\n",
      "    class # 0 capture rate = 0.946728 \n",
      "    class # 1 capture rate = 0.929748 \n",
      "TRAIN: Batch: 0.9147056524118521 Loss_Total: 12.036599\n",
      "TRAIN: Batch: 0.9147056524118521 Loss_Seg: 12.611914\n",
      "accuracy = 0.936148\n",
      "mean IU  = 0.717295\n",
      "    class # 0 capture rate = 0.936236 \n",
      "    class # 1 capture rate = 0.934959 \n",
      "TRAIN: Batch: 0.9186146509264327 Loss_Total: 15.145365\n",
      "TRAIN: Batch: 0.9186146509264327 Loss_Seg: 12.461548\n",
      "accuracy = 0.935142\n",
      "mean IU  = 0.685722\n",
      "    class # 0 capture rate = 0.935055 \n",
      "    class # 1 capture rate = 0.936658 \n",
      "TRAIN: Batch: 0.9225236494410132 Loss_Total: 11.654188\n",
      "TRAIN: Batch: 0.9225236494410132 Loss_Seg: 12.173168\n",
      "accuracy = 0.945627\n",
      "mean IU  = 0.753600\n",
      "    class # 0 capture rate = 0.945233 \n",
      "    class # 1 capture rate = 0.950524 \n",
      "TRAIN: Batch: 0.9264326479555938 Loss_Total: 11.547442\n",
      "TRAIN: Batch: 0.9264326479555938 Loss_Seg: 13.989297\n",
      "accuracy = 0.946011\n",
      "mean IU  = 0.726653\n",
      "    class # 0 capture rate = 0.948805 \n",
      "    class # 1 capture rate = 0.903959 \n",
      "TRAIN: Batch: 0.9303416464701744 Loss_Total: 9.852295\n",
      "TRAIN: Batch: 0.9303416464701744 Loss_Seg: 8.050915\n",
      "accuracy = 0.953727\n",
      "mean IU  = 0.739436\n",
      "    class # 0 capture rate = 0.952333 \n",
      "    class # 1 capture rate = 0.978733 \n",
      "TRAIN: Batch: 0.9342506449847549 Loss_Total: 11.904192\n",
      "TRAIN: Batch: 0.9342506449847549 Loss_Seg: 12.644748\n",
      "accuracy = 0.940702\n",
      "mean IU  = 0.723784\n",
      "    class # 0 capture rate = 0.941495 \n",
      "    class # 1 capture rate = 0.929593 \n",
      "TRAIN: Batch: 0.9381596434993354 Loss_Total: 11.981589\n",
      "TRAIN: Batch: 0.9381596434993354 Loss_Seg: 8.641508\n",
      "accuracy = 0.960471\n",
      "mean IU  = 0.742149\n",
      "    class # 0 capture rate = 0.961163 \n",
      "    class # 1 capture rate = 0.946203 \n",
      "TRAIN: Batch: 0.942068642013916 Loss_Total: 20.597641\n",
      "TRAIN: Batch: 0.942068642013916 Loss_Seg: 21.47917\n",
      "accuracy = 0.907646\n",
      "mean IU  = 0.675034\n",
      "    class # 0 capture rate = 0.910658 \n",
      "    class # 1 capture rate = 0.875743 \n",
      "TRAIN: Batch: 0.9459776405284966 Loss_Total: 10.681785\n",
      "TRAIN: Batch: 0.9459776405284966 Loss_Seg: 9.177277\n",
      "accuracy = 0.963134\n",
      "mean IU  = 0.812631\n",
      "    class # 0 capture rate = 0.963420 \n",
      "    class # 1 capture rate = 0.959667 \n",
      "TRAIN: Batch: 0.9498866390430771 Loss_Total: 11.761212\n",
      "TRAIN: Batch: 0.9498866390430771 Loss_Seg: 10.018468\n",
      "accuracy = 0.945279\n",
      "mean IU  = 0.739794\n",
      "    class # 0 capture rate = 0.944288 \n",
      "    class # 1 capture rate = 0.959206 \n",
      "TRAIN: Batch: 0.9537956375576577 Loss_Total: 12.353821\n",
      "TRAIN: Batch: 0.9537956375576577 Loss_Seg: 9.94943\n",
      "accuracy = 0.946014\n",
      "mean IU  = 0.740542\n",
      "    class # 0 capture rate = 0.944178 \n",
      "    class # 1 capture rate = 0.972505 \n",
      "TRAIN: Batch: 0.9577046360722383 Loss_Total: 14.997662\n",
      "TRAIN: Batch: 0.9577046360722383 Loss_Seg: 15.572806\n",
      "accuracy = 0.922126\n",
      "mean IU  = 0.697315\n",
      "    class # 0 capture rate = 0.920808 \n",
      "    class # 1 capture rate = 0.938121 \n",
      "TRAIN: Batch: 0.9616136345868188 Loss_Total: 10.460145\n",
      "TRAIN: Batch: 0.9616136345868188 Loss_Seg: 11.673226\n",
      "accuracy = 0.951850\n",
      "mean IU  = 0.741966\n",
      "    class # 0 capture rate = 0.952449 \n",
      "    class # 1 capture rate = 0.942252 \n",
      "TRAIN: Batch: 0.9655226331013994 Loss_Total: 12.327219\n",
      "TRAIN: Batch: 0.9655226331013994 Loss_Seg: 10.74974\n",
      "accuracy = 0.955680\n",
      "mean IU  = 0.734933\n",
      "    class # 0 capture rate = 0.957848 \n",
      "    class # 1 capture rate = 0.915879 \n",
      "TRAIN: Batch: 0.96943163161598 Loss_Total: 13.613495\n",
      "TRAIN: Batch: 0.96943163161598 Loss_Seg: 13.995468\n",
      "accuracy = 0.933636\n",
      "mean IU  = 0.723895\n",
      "    class # 0 capture rate = 0.933051 \n",
      "    class # 1 capture rate = 0.940724 \n",
      "TRAIN: Batch: 0.9733406301305606 Loss_Total: 13.148247\n",
      "TRAIN: Batch: 0.9733406301305606 Loss_Seg: 11.665655\n",
      "accuracy = 0.925136\n",
      "mean IU  = 0.657511\n",
      "    class # 0 capture rate = 0.922921 \n",
      "    class # 1 capture rate = 0.966974 \n",
      "TRAIN: Batch: 0.9772496286451411 Loss_Total: 10.809548\n",
      "TRAIN: Batch: 0.9772496286451411 Loss_Seg: 8.447915\n",
      "accuracy = 0.952746\n",
      "mean IU  = 0.716594\n",
      "    class # 0 capture rate = 0.952660 \n",
      "    class # 1 capture rate = 0.954521 \n",
      "TRAIN: Batch: 0.9811586271597217 Loss_Total: 11.912935\n",
      "TRAIN: Batch: 0.9811586271597217 Loss_Seg: 10.161957\n",
      "accuracy = 0.944815\n",
      "mean IU  = 0.703373\n",
      "    class # 0 capture rate = 0.944800 \n",
      "    class # 1 capture rate = 0.945087 \n",
      "TRAIN: Batch: 0.9850676256743023 Loss_Total: 11.497276\n",
      "TRAIN: Batch: 0.9850676256743023 Loss_Seg: 10.551669\n",
      "accuracy = 0.938492\n",
      "mean IU  = 0.695428\n",
      "    class # 0 capture rate = 0.937663 \n",
      "    class # 1 capture rate = 0.953005 \n",
      "TRAIN: Batch: 0.9889766241888828 Loss_Total: 10.266901\n",
      "TRAIN: Batch: 0.9889766241888828 Loss_Seg: 7.259023\n",
      "accuracy = 0.964964\n",
      "mean IU  = 0.776261\n",
      "    class # 0 capture rate = 0.965890 \n",
      "    class # 1 capture rate = 0.948435 \n",
      "TRAIN: Batch: 0.9928856227034634 Loss_Total: 11.210649\n",
      "TRAIN: Batch: 0.9928856227034634 Loss_Seg: 8.976628\n",
      "accuracy = 0.952979\n",
      "mean IU  = 0.719540\n",
      "    class # 0 capture rate = 0.953304 \n",
      "    class # 1 capture rate = 0.946447 \n",
      "TRAIN: Batch: 0.996794621218044 Loss_Total: 10.025221\n",
      "TRAIN: Batch: 0.996794621218044 Loss_Seg: 9.759785\n",
      "accuracy = 0.951874\n",
      "mean IU  = 0.734945\n",
      "    class # 0 capture rate = 0.952532 \n",
      "    class # 1 capture rate = 0.940685 \n",
      "Validating NN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"\n",
      "Process Process-24:\n",
      "Process Process-23:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-76742371c67f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#model = Model(args, experiment, loss_weight=weight, mustRestore=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckptpath_recg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'charList.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_beta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoderType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecoderType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSearch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmustRestore_seg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmustRestore_recg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainSeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidateloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6809828a6d02>\u001b[0m in \u001b[0;36mtrainSeg\u001b[0;34m(self, loader, validateloader, testloader)\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;31m# validate:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidateloader\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mavg_batch_loss_seg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_batch_loss_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidateSeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidateloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mavg_batch_loss_seg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_batch_loss_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidateSeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6809828a6d02>\u001b[0m in \u001b[0;36mvalidateSeg\u001b[0;34m(self, loader, epoch, is_testing)\u001b[0m\n\u001b[1;32m    886\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseqLen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxTextLen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbt_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#self.batchsize_recg, #changed!!!!!!!!!!!!!!!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m              self.is_training:False})  # self.loss,val_loss,\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mtotal_val_loss_seg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval_loss_seg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "checkArgs()\n",
    "if args.train:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Lambda(lambda img: cv2.resize(img, (args.image_w, args.image_h), interpolation=cv2.INTER_CUBIC)),\n",
    "        transforms.Lambda(lambda img: np.expand_dims(img, 3)),\n",
    "        # transforms.Lambda(lambda img: add_artifacts(img,args)),\n",
    "        # transforms.Lambda(lambda img: cv2.transpose(img))\n",
    "    ])\n",
    "    # arprint=ArtPrint(args.data_root, transform=transform_train)\n",
    "    arprint = ArtPrintNoIntsectBinary(args.data_root, transform=transform_train)\n",
    "    concat = arprint\n",
    "    idxTrain = int(len(arprint) * 0.9)\n",
    "    trainset, testset = random_split(concat, [idxTrain, len(concat) - idxTrain])\n",
    "    trainloader = DataLoader(trainset, batch_size=args.batch_size_seg, shuffle=True, drop_last=True, num_workers=4)\n",
    "    testloader = DataLoader(testset, batch_size=args.batch_size_seg, shuffle=False, drop_last=False, num_workers=2)\n",
    "\n",
    "    # weight gen\n",
    "    ##pos_perc = sum(map(lambda x: np.sum(cv2.imread(x[1], cv2.IMREAD_GRAYSCALE)), trainset.dataset.samples)) / (\n",
    "    ##            args.image_h * args.image_w * len(trainset.dataset.samples))\n",
    "    ##neg_perc = 1 - pos_perc\n",
    "    ##weight = np.array([pos_perc, neg_perc])  # just reverse\n",
    "    weight=np.array([0.1,0.9])\n",
    "    print(weight)\n",
    "    #model = Model(args, experiment, loss_weight=weight, mustRestore=False)\n",
    "    model = Model(args, charList=open(join(args.ckptpath_recg, 'charList.txt')).read(), loss_beta=0.5,loss_weight=weight, decoderType=DecoderType.BeamSearch,experiment=experiment,mustRestore_seg=False,mustRestore_recg=True,joint=True)\n",
    "    model.trainSeg(loader=trainloader, validateloader=testloader)\n",
    "\n",
    "else:\n",
    "    pass  # for no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
